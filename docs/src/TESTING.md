# Multi-Agent System Test Document

## 1. Introduction & Overview

### 1.1 Purpose and Scope

This document defines the comprehensive testing strategy for the multi-agent system, covering unit tests, integration tests, and system tests for the source code in `main.py` and `multi_agent_system.py`.

**Primary Objectives:**
- Verify the correctness of individual agent functions and utility components
- Validate agent communication protocols and coordination mechanisms
- Ensure complete system workflows function as intended
- Test error handling and recovery mechanisms across all system levels

**Testing Scope:**
- **Unit Tests**: Individual functions, classes, and components in isolation
- **Integration Tests**: Agent interactions, communication flows, and coordination mechanisms
- **System Tests**: Complete end-to-end workflows and emergent system behaviors

**Exclusions:**
- Performance testing (response times, throughput, scalability)
- User experience testing (UI/UX, usability, accessibility)
- Security testing (vulnerability assessment, penetration testing)
- Load testing and stress testing

### 1.2 Testing Objectives

**Unit Testing Goals:**
- Validate individual agent functions (planner, researcher, expert, critic, finalizer)
- Test utility functions (validation, message composition, state management)
- Verify tool integrations (research tools, expert tools, file operations)
- Ensure proper error handling in isolated components

**Integration Testing Goals:**
- Validate agent communication protocols and message routing
- Test coordination mechanisms between agents
- Verify state synchronization across the multi-agent system
- Ensure proper workflow transitions and agent handoffs

**System Testing Goals:**
- Validate complete end-to-end question answering workflows
- Test emergent behaviors arising from agent interactions
- Verify system-level error handling and recovery
- Ensure consistent state management across the entire system

**Core Testing Principles:**
- Focus on **static behavior** and logic flow, not LLM output quality
- Test **agent interactions** and coordination, not individual agent intelligence
- Validate **system architecture** and workflow design
- Ensure **error handling** and graceful degradation

### 1.3 System Overview

The multi-agent system implements a sophisticated question-answering architecture using five specialized agents coordinated through a central orchestrator:

**Core Agents:**
- **Planner Agent**: Analyzes questions and creates research/expert step plans
- **Researcher Agent**: Executes research steps using various tools (web search, file reading, etc.)
- **Expert Agent**: Processes research results and generates expert answers using specialized tools
- **Critic Agent**: Reviews and validates outputs from planner, researcher, and expert agents
- **Finalizer Agent**: Synthesizes all results into final answers and reasoning traces

**System Architecture:**
- **LangGraph-based workflow**: Uses StateGraph for agent coordination and state management
- **Message-based communication**: Agents communicate through structured AgentMessage protocol
- **Tool integration**: Research tools (web search, file reading) and expert tools (calculator, unit converter)
- **State management**: Centralized GraphState manages system state and agent interactions

**Workflow Process:**
1. **Input Processing**: Question and optional file input initialization
2. **Planning Phase**: Planner creates research and expert step strategies
3. **Research Phase**: Researcher executes planned research steps with critic validation
4. **Expert Phase**: Expert processes research results with critic validation
5. **Finalization**: Finalizer synthesizes complete answer and reasoning trace

**Coordination Mechanisms:**
- **Orchestrator**: Central controller managing workflow transitions and agent routing
- **Critic validation**: Each major phase includes critic review and potential retry logic
- **State synchronization**: Consistent state management across all agents and phases
- **Error handling**: Graceful failure handling with retry limits and fallback mechanisms

### 1.4 Testing Framework and Tools

**Primary Testing Framework:**
- **pytest**: Main testing framework for test execution and organization
- **pytest-mock**: Mocking and patching utilities for external dependencies
- **pytest-cov**: Code coverage reporting and analysis

**Backup Framework:**
- **unittest**: Available for specific test cases requiring unittest features
- **unittest.mock**: Alternative mocking approach when needed

**Testing Utilities:**
- **Mock configurations**: Predefined mock setups for LLMs, external APIs, and file systems
- **Test fixtures**: Reusable test data and setup configurations
- **Assertion helpers**: Custom assertion functions for agent state and message validation

**Project Test Structure:**
```
tests/
â”œâ”€â”€ conftest.py              # Pytest configuration and shared fixtures
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ test_core/           # Core functionality and utility tests
â”‚   â”œâ”€â”€ test_agents/         # Individual agent tests
â”‚   â”œâ”€â”€ test_integration/    # Agent interaction and coordination tests
â”‚   â””â”€â”€ test_evaluation/     # System-level and evaluation tests
â””â”€â”€ fixtures/                # Test data and mock configurations
```

**Test Execution:**
- **Unit tests**: `pytest tests/source/test_core/ tests/source/test_agents/`
- **Integration tests**: `pytest tests/source/test_integration/`
- **System tests**: `pytest tests/source/test_evaluation/`
- **Full test suite**: `pytest --cov=src --cov-report=html`

## 2. Test Environment & Setup

### 2.1 Dependencies and Requirements

**Python Environment:**
- **Python Version**: 3.8 or higher
- **Operating System**: Linux, macOS, or Windows (WSL2 recommended for Windows)
- **Package Manager**: pip or conda

**Core Testing Dependencies:**
```bash
pytest>=7.0.0
pytest-cov>=4.0.0
pytest-mock>=3.10.0
```

**Project Dependencies (from existing requirements):**
```bash
langchain-core>=0.1.0
langchain>=0.1.0
langchain-openai>=0.1.0
langchain-community>=0.1.0
langchain-tavily>=0.1.0
langchain-experimental>=0.1.0
langgraph>=0.1.0
```

**External Tools and Services:**
- **Git**: Version control for test code management
- **Text Editor/IDE**: VS Code, PyCharm, or similar with Python support
- **Terminal**: Command-line interface for test execution

**Requirements File Reference:**
- **Test Requirements**: `tests/fixtures/requirements.txt`
- **Evaluation Requirements**: `requirements_eval.txt`
- **Main Project Dependencies**: Defined in project setup files

### 2.2 Test Data and Fixtures

**Core Test Data Structures:**

**Agent Configuration Data:**
```python
SAMPLE_AGENT_CONFIG = {
    "planner": AgentConfig(
        name="planner",
        provider="openai",
        model="gpt-4o-mini",
        temperature=0.0,
        output_schema={"research_steps": list[str], "expert_steps": list[str]},
        system_prompt="test prompt",
        retry_limit=3
    )
}
```

**Agent Message Data:**
```python
SAMPLE_AGENT_MESSAGE = {
    "timestamp": "2024-01-01T12:00:00",
    "sender": "orchestrator",
    "receiver": "planner",
    "type": "instruction",
    "content": "Develop a plan for testing",
    "step_id": None
}
```

**Graph State Data:**
```python
SAMPLE_GRAPH_STATE = {
    "agent_messages": [],
    "question": "What is the capital of France?",
    "file": None,
    "research_steps": [],
    "expert_steps": [],
    "current_step": "input",
    "next_step": "planner",
    "planner_retry_count": 0,
    "researcher_retry_count": 0,
    "expert_retry_count": 0
}
```

**Fixture Requirements:**

**Agent Configuration Fixtures:**
```python
@pytest.fixture
def sample_agent_configs():
    """Provide sample agent configurations for testing."""
    return SAMPLE_AGENT_CONFIG

@pytest.fixture
def mock_llm_responses():
    """Provide mock LLM responses for different agents."""
    return {
        "planner": {
            "research_steps": ["Research step 1", "Research step 2"],
            "expert_steps": ["Expert step 1"]
        },
        "researcher": {"result": "Research result content"},
        "expert": {
            "expert_answer": "Expert answer",
            "reasoning_trace": "Expert reasoning"
        }
    }
```

**Test Data Organization:**
- **Fixtures**: Defined in `tests/conftest.py` for shared test data
- **Sample Data**: Stored in `tests/fixtures/` directory
- **Mock Data**: Organized by component type (agents, tools, states)
- **Test Cases**: Include specific data for each test scenario

### 2.3 Mocking Strategy

**External Dependencies to Mock:**

**LLM Services:**
```python
@patch('multi_agent_system.ChatOpenAI')
def test_agent_with_mocked_llm(mock_llm):
    """Mock OpenAI LLM for agent testing."""
    mock_llm.return_value.invoke.return_value = {
        "research_steps": ["step1", "step2"],
        "expert_steps": ["expert1"]
    }
    # Test implementation
```

**File System Operations:**
```python
@patch('builtins.open', mock_open(read_data='test content'))
def test_file_operations():
    """Mock file system operations."""
    # Test file reading/writing operations
```

**Network Calls and APIs:**
```python
@patch('requests.get')
def test_network_calls(mock_get):
    """Mock external API calls."""
    mock_get.return_value.json.return_value = {"data": "test"}
    # Test network-dependent operations
```

**Agent Interaction Mocking:**
```python
@patch('multi_agent_system.get_agent_conversation')
def test_agent_communication(mock_conversation):
    """Mock agent communication for isolation testing."""
    mock_conversation.return_value = [SAMPLE_AGENT_MESSAGE]
    # Test agent communication logic
```

**Tool Integration Mocking:**
```python
@patch('multi_agent_system.youtube_transcript_tool')
def test_research_tools(mock_youtube):
    """Mock research tools for testing."""
    mock_youtube.return_value = "Mock transcript content"
    # Test tool integration
```

**Mock Data Configurations:**

**LLM Response Mocks:**
```python
LLM_MOCK_RESPONSES = {
    "planner": {
        "research_steps": ["Research step 1", "Research step 2"],
        "expert_steps": ["Expert step 1"]
    },
    "researcher": {"result": "Research result content"},
    "expert": {
        "expert_answer": "Expert answer",
        "reasoning_trace": "Expert reasoning"
    },
    "critic": {
        "decision": "approve",
        "feedback": "Good work"
    },
    "finalizer": {
        "final_answer": "Final answer content",
        "final_reasoning_trace": "Final reasoning trace"
    }
}
```

**Error Scenario Mocks:**
```python
ERROR_MOCK_RESPONSES = {
    "llm_failure": Exception("LLM API error"),
    "file_not_found": FileNotFoundError("File not found"),
    "network_error": ConnectionError("Network connection failed"),
    "validation_error": ValueError("Invalid response format")
}
```

### 2.4 Test Organization Structure

**Directory Structure:**
```
tests/
â”œâ”€â”€ conftest.py                    # Shared fixtures and configuration
â”œâ”€â”€ source/
â”‚   â”œâ”€â”€ test_core/                # Core functionality tests
â”‚   â”‚   â”œâ”€â”€ test_validation.py    # Validation utility tests
â”‚   â”‚   â”œâ”€â”€ test_communication.py # Agent communication tests
â”‚   â”‚   â”œâ”€â”€ test_tools.py         # Tool integration tests
â”‚   â”‚   â””â”€â”€ test_state.py         # State management tests
â”‚   â”œâ”€â”€ test_agents/              # Individual agent tests
â”‚   â”‚   â”œâ”€â”€ test_planner.py       # Planner agent tests
â”‚   â”‚   â”œâ”€â”€ test_researcher.py    # Researcher agent tests
â”‚   â”‚   â”œâ”€â”€ test_expert.py        # Expert agent tests
â”‚   â”‚   â”œâ”€â”€ test_critic.py        # Critic agent tests
â”‚   â”‚   â””â”€â”€ test_finalizer.py     # Finalizer agent tests
â”‚   â”œâ”€â”€ test_integration/         # Integration tests
â”‚   â”‚   â”œâ”€â”€ test_agent_communication.py
â”‚   â”‚   â”œâ”€â”€ test_coordination.py
â”‚   â”‚   â””â”€â”€ test_workflows.py
â”‚   â””â”€â”€ test_evaluation/          # System-level tests
â”‚       â”œâ”€â”€ test_complete_workflows.py
â”‚       â”œâ”€â”€ test_emergent_behaviors.py
â”‚       â””â”€â”€ test_error_handling.py
â””â”€â”€ fixtures/                     # Test data and mock configurations
    â”œâ”€â”€ agent_configs.py          # Agent configuration data
    â”œâ”€â”€ mock_responses.py         # Mock LLM responses
    â”œâ”€â”€ test_states.py            # Sample graph states
    â””â”€â”€ requirements.txt          # Test dependencies
```

**Naming Conventions:**

**Test Files:**
- `test_<component_name>.py` - Individual component tests
- `test_<functionality>_<scenario>.py` - Specific functionality tests
- `test_<integration_type>.py` - Integration test files

**Test Functions:**
- `test_<function_name>_<scenario>()` - Unit test functions
- `test_<component>_<interaction>()` - Integration test functions
- `test_<workflow>_<behavior>()` - System test functions

**Test Classes:**
- `Test<ComponentName>` - Test class for component-specific tests
- `Test<Functionality>Integration` - Integration test classes
- `Test<Workflow>System` - System test classes

**Test Grouping Strategy:**

**Unit Tests (test_core/, test_agents/):**
- Group by component type (validation, communication, tools, agents)
- Each component gets its own test file
- Focus on isolated functionality testing

**Integration Tests (test_integration/):**
- Group by interaction type (communication, coordination, workflows)
- Test agent interactions and coordination mechanisms
- Focus on component integration testing

**System Tests (test_evaluation/):**
- Group by workflow type (complete workflows, emergent behaviors, error handling)
- Test end-to-end system behavior
- Focus on system-level validation

**Test Execution Order:**
1. **Unit Tests**: Individual components in isolation
2. **Integration Tests**: Component interactions and coordination
3. **System Tests**: Complete workflows and emergent behaviors

## 3. Unit Tests
### 3.1 Validation Utilities
#### 3.1.1 JSON Response Validation

**Function Information:**
- **Function Name**: `enforce_json_response`
- **Location**: `multi_agent_system.py:106`
- **Purpose**: Ensures LLM responses are valid JSON or dictionaries, converting string responses to dictionaries

**Function Signature and Parameters:**
- **Input**: `response: Any` - The response from the LLM (can be dict, string, or other types)
- **Input**: `component: str` - The component name for error reporting
- **Return**: `dict` - The response as a dictionary
- **Side Effects**: None

**Dependencies Analysis:**
- **Dependencies to Mock**: None - This is a pure function with no external dependencies
- **Dependencies to Use Directly**: 
  - `json.loads()` - Built-in Python function for JSON parsing
  - `isinstance()` - Built-in Python function for type checking
  - `str()` - Built-in Python function for string conversion

- **Mock Configuration**: Not applicable - no dependencies to mock

**Test Cases:**

**Happy Path:**
- **Valid Dictionary Input**: Pass a dictionary, expect same dictionary returned
- **Valid JSON String Input**: Pass JSON string, expect parsed dictionary returned

**Edge Cases:**
- **Empty String Input**: Pass empty string, expect ValueError
- **None Input**: Pass None, expect ValueError
- **Non-string, Non-dict Input**: Pass other types (int, list), expect ValueError

**Error Conditions:**
- **Invalid JSON String**: Pass malformed JSON string, expect ValueError
- **Component Name in Error**: Verify error message includes component name

**Mock Configurations:**
```python
# No mocks needed - this is a pure function
# All dependencies are built-in Python functions that can be used directly
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions in the implementation:
import json

def enforce_json_response(response: Any, component: str) -> dict:
    if isinstance(response, dict):  # Direct use of isinstance()
        return response
    try:
        data = json.loads(response)  # Direct use of json.loads()
        return data
    except Exception:
        raise ValueError(f"{component}: Response is not a dictionary or valid JSON string: {response}")
```

**Assertion Specifications:**
- Verify dictionary inputs are returned unchanged
- Verify valid JSON strings are parsed to dictionaries
- Verify ValueError is raised for invalid inputs
- Verify error messages include component name
- Verify no side effects occur (function is pure)

**Code Examples:**
```python
def test_enforce_json_response_valid_dict():
    """Test that dictionary inputs are returned unchanged."""
    response = {"key": "value", "number": 42}
    result = enforce_json_response(response, "test_component")
    assert result == {"key": "value", "number": 42}

def test_enforce_json_response_valid_json_string():
    """Test that valid JSON strings are parsed to dictionaries."""
    response = '{"key": "value", "number": 42}'
    result = enforce_json_response(response, "test_component")
    assert result == {"key": "value", "number": 42}

def test_enforce_json_response_invalid_json_string():
    """Test that invalid JSON strings raise ValueError."""
    response = '{"key": value}'  # Missing quotes around value
    with pytest.raises(ValueError) as exc_info:
        enforce_json_response(response, "test_component")
    assert "test_component" in str(exc_info.value)

def test_enforce_json_response_empty_string():
    """Test that empty strings raise ValueError."""
    with pytest.raises(ValueError) as exc_info:
        enforce_json_response("", "test_component")
    assert "test_component" in str(exc_info.value)

def test_enforce_json_response_none_input():
    """Test that None input raises ValueError."""
    with pytest.raises(ValueError) as exc_info:
        enforce_json_response(None, "test_component")
    assert "test_component" in str(exc_info.value)

def test_enforce_json_response_non_string_non_dict():
    """Test that non-string, non-dict inputs raise ValueError."""
    with pytest.raises(ValueError) as exc_info:
        enforce_json_response(42, "test_component")
    assert "test_component" in str(exc_info.value)

def test_enforce_json_response_complex_dict():
    """Test that complex nested dictionaries are handled correctly."""
    response = {
        "nested": {"key": "value"},
        "list": [1, 2, 3],
        "boolean": True,
        "null": None
    }
    result = enforce_json_response(response, "test_component")
    assert result == response
    assert result["nested"]["key"] == "value"
    assert result["list"] == [1, 2, 3]
    assert result["boolean"] is True
    assert result["null"] is None
```

#### 3.1.2 Schema Validation

**Function Information:**
- **Function Name**: `validate_output_matches_json_schema`
- **Location**: `multi_agent_system.py:131`
- **Purpose**: Checks if LLM response has all expected fields from the output schema

**Function Signature and Parameters:**
- **Input**: `response: Any` - The response from the LLM (assumed to be a dictionary)
- **Input**: `output_schema_keys: list[str]` - List of expected field names
- **Return**: `bool` - True if all expected fields are present, False otherwise
- **Side Effects**: None

**Dependencies Analysis:**
- **Dependencies to Mock**: None - This is a pure function with no external dependencies
- **Dependencies to Use Directly**: 
  - `all()` - Built-in Python function for checking if all elements are True
  - `in` operator - Built-in Python operator for membership testing
  - List iteration - Built-in Python list functionality
- **Mock Configuration**: Not applicable - no dependencies to mock

**Test Cases:**

**Happy Path:**
- **All Fields Present**: Response contains all expected schema keys
- **Extra Fields Present**: Response contains all expected keys plus additional fields

**Edge Cases:**
- **Empty Schema**: Empty list of expected keys
- **Empty Response**: Empty dictionary response
- **None Response**: None input

**Error Conditions:**
- **Missing Required Fields**: Response missing some expected keys
- **No Fields Present**: Response has no expected keys

**Mock Configurations:**
```python
# No mocks needed - this is a pure function
# All dependencies are built-in Python functions that can be used directly
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions in the implementation:
def validate_output_matches_json_schema(response: Any, output_schema_keys: list[str]) -> bool:
    return all(key in response for key in output_schema_keys)  # Direct use of all() and in operator
```

**Assertion Specifications:**
- Verify function returns True when all expected fields are present
- Verify function returns False when any expected field is missing
- Verify function handles edge cases gracefully
- Verify function works with different data types in response values
- Verify no side effects occur (function is pure)

**Code Examples:**
```python
def test_validate_output_matches_json_schema_all_fields_present():
    """Test that function returns True when all expected fields are present."""
    response = {"field1": "value1", "field2": "value2", "field3": "value3"}
    schema_keys = ["field1", "field2", "field3"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is True

def test_validate_output_matches_json_schema_extra_fields():
    """Test that function returns True when extra fields are present."""
    response = {"field1": "value1", "field2": "value2", "extra_field": "extra_value"}
    schema_keys = ["field1", "field2"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is True

def test_validate_output_matches_json_schema_missing_fields():
    """Test that function returns False when required fields are missing."""
    response = {"field1": "value1", "field2": "value2"}
    schema_keys = ["field1", "field2", "field3"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is False

def test_validate_output_matches_json_schema_empty_schema():
    """Test that function returns True with empty schema."""
    response = {"field1": "value1", "field2": "value2"}
    schema_keys = []
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is True

def test_validate_output_matches_json_schema_empty_response():
    """Test that function returns False with empty response."""
    response = {}
    schema_keys = ["field1", "field2"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is False

def test_validate_output_matches_json_schema_none_response():
    """Test that function returns False with None response."""
    response = None
    schema_keys = ["field1", "field2"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is False

def test_validate_output_matches_json_schema_different_value_types():
    """Test that function works with different value types."""
    response = {
        "string_field": "string_value",
        "int_field": 42,
        "float_field": 3.14,
        "bool_field": True,
        "list_field": [1, 2, 3],
        "dict_field": {"nested": "value"},
        "null_field": None
    }
    schema_keys = ["string_field", "int_field", "float_field", "bool_field", "list_field", "dict_field", "null_field"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is True

def test_validate_output_matches_json_schema_partial_missing():
    """Test that function returns False when some fields are missing."""
    response = {"field1": "value1", "field3": "value3"}
    schema_keys = ["field1", "field2", "field3"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is False

def test_validate_output_matches_json_schema_case_sensitive():
    """Test that field matching is case sensitive."""
    response = {"Field1": "value1", "field2": "value2"}
    schema_keys = ["field1", "field2"]
    result = validate_output_matches_json_schema(response, schema_keys)
    assert result is False
```

#### 3.1.3 LLM Response Validation

**Function Information:**
- **Function Name**: `validate_llm_response`
- **Location**: `multi_agent_system.py:156`
- **Purpose**: Validates LLM response format and content, ensuring it matches expected schema and contains required fields

**Function Signature and Parameters:**
- **Input**: `response: Any` - The response from the LLM (can be dict, string, or other types)
- **Input**: `output_schema_keys: list[str]` - List of expected field names from the output schema
- **Input**: `component: str` - The component name for error reporting
- **Return**: `dict` - The validated response as a dictionary
- **Side Effects**: None

**Dependencies Analysis:**
- **Dependencies to Mock**: None - This function uses internal validation functions that are pure
- **Dependencies to Use Directly**: 
  - `enforce_json_response()` - Internal function for JSON validation
  - `validate_output_matches_json_schema()` - Internal function for schema validation
  - `isinstance()` - Built-in Python function for type checking
  - `str()` - Built-in Python function for string conversion
- **Mock Configuration**: Not applicable - no external dependencies to mock

**Test Cases:**

**Happy Path:**
- **Valid Dictionary with All Fields**: Response is dict with all expected schema fields
- **Valid JSON String with All Fields**: Response is JSON string with all expected schema fields
- **Extra Fields Present**: Response contains all required fields plus additional fields

**Edge Cases:**
- **Empty Schema**: Empty list of expected schema keys
- **Empty Response**: Empty dictionary or string response
- **None Response**: None input

**Error Conditions:**
- **Invalid JSON Format**: Malformed JSON string response
- **Missing Required Fields**: Response missing some expected schema fields
- **Non-dict, Non-string Input**: Input that's neither dictionary nor string
- **Component Name in Error**: Verify error messages include component name

**Mock Configurations:**
```python
# No mocks needed - this function uses internal pure functions
# Internal functions can be tested directly or mocked if needed for isolation
```

**Direct Usage Examples:**
```python
# Direct usage of internal functions in the implementation:
def validate_llm_response(response: Any, output_schema_keys: list[str], component: str) -> dict:
    # Direct use of internal validation functions
    json_response = enforce_json_response(response, component)  # Direct use of internal function
    if not validate_output_matches_json_schema(json_response, output_schema_keys):  # Direct use of internal function
        raise ValueError(f"{component}: Response missing required fields: {output_schema_keys}")
    return json_response
```

**Assertion Specifications:**
- Verify function returns validated dictionary when all validations pass
- Verify ValueError is raised for invalid JSON format
- Verify ValueError is raised for missing required fields
- Verify error messages include component name
- Verify function handles edge cases gracefully
- Verify no side effects occur (function is pure)

**Code Examples:**
```python
def test_validate_llm_response_valid_dict_with_all_fields():
    """Test that function returns validated dict when all fields are present."""
    response = {"field1": "value1", "field2": "value2", "field3": "value3"}
    schema_keys = ["field1", "field2", "field3"]
    result = validate_llm_response(response, schema_keys, "test_component")
    assert result == {"field1": "value1", "field2": "value2", "field3": "value3"}

def test_validate_llm_response_valid_json_string():
    """Test that function validates JSON string responses."""
    response = '{"field1": "value1", "field2": "value2"}'
    schema_keys = ["field1", "field2"]
    result = validate_llm_response(response, schema_keys, "test_component")
    assert result == {"field1": "value1", "field2": "value2"}

def test_validate_llm_response_extra_fields():
    """Test that function accepts responses with extra fields."""
    response = {"field1": "value1", "field2": "value2", "extra_field": "extra_value"}
    schema_keys = ["field1", "field2"]
    result = validate_llm_response(response, schema_keys, "test_component")
    assert result == {"field1": "value1", "field2": "value2", "extra_field": "extra_value"}

def test_validate_llm_response_invalid_json_string():
    """Test that function raises ValueError for invalid JSON."""
    response = '{"field1": value1}'  # Missing quotes around value1
    schema_keys = ["field1"]
    with pytest.raises(ValueError) as exc_info:
        validate_llm_response(response, schema_keys, "test_component")
    assert "test_component" in str(exc_info.value)

def test_validate_llm_response_missing_required_fields():
    """Test that function raises ValueError for missing required fields."""
    response = {"field1": "value1", "field2": "value2"}
    schema_keys = ["field1", "field2", "field3"]
    with pytest.raises(ValueError) as exc_info:
        validate_llm_response(response, schema_keys, "test_component")
    assert "test_component" in str(exc_info.value)
    assert "missing required fields" in str(exc_info.value)

def test_validate_llm_response_empty_schema():
    """Test that function works with empty schema."""
    response = {"field1": "value1", "field2": "value2"}
    schema_keys = []
    result = validate_llm_response(response, schema_keys, "test_component")
    assert result == {"field1": "value1", "field2": "value2"}

def test_validate_llm_response_empty_dict():
    """Test that function works with empty dictionary."""
    response = {}
    schema_keys = []
    result = validate_llm_response(response, schema_keys, "test_component")
    assert result == {}

def test_validate_llm_response_none_input():
    """Test that function raises ValueError for None input."""
    response = None
    schema_keys = ["field1"]
    with pytest.raises(ValueError) as exc_info:
        validate_llm_response(response, schema_keys, "test_component")
    assert "test_component" in str(exc_info.value)

def test_validate_llm_response_non_dict_non_string():
    """Test that function raises ValueError for non-dict, non-string input."""
    response = 42
    schema_keys = ["field1"]
    with pytest.raises(ValueError) as exc_info:
        validate_llm_response(response, schema_keys, "test_component")
    assert "test_component" in str(exc_info.value)

def test_validate_llm_response_complex_nested_structure():
    """Test that function handles complex nested data structures."""
    response = {
        "simple_field": "value",
        "nested_field": {"key": "value"},
        "list_field": [1, 2, 3],
        "mixed_field": {"nested": [{"item": "value"}]}
    }
    schema_keys = ["simple_field", "nested_field", "list_field", "mixed_field"]
    result = validate_llm_response(response, schema_keys, "test_component")
    assert result == response
    assert result["nested_field"]["key"] == "value"
    assert result["list_field"] == [1, 2, 3]
    assert result["mixed_field"]["nested"][0]["item"] == "value"

def test_validate_llm_response_component_name_in_error():
    """Test that error messages include the component name."""
    response = {"field1": "value1"}
    schema_keys = ["field1", "field2"]
    with pytest.raises(ValueError) as exc_info:
        validate_llm_response(response, schema_keys, "planner_agent")
    error_message = str(exc_info.value)
    assert "planner_agent" in error_message
    assert "missing required fields" in error_message

def test_validate_llm_response_empty_string():
    """Test that function raises ValueError for empty string."""
    response = ""
    schema_keys = ["field1"]
    with pytest.raises(ValueError) as exc_info:
        validate_llm_response(response, schema_keys, "test_component")
    assert "test_component" in str(exc_info.value)
```
### 3.2 Agent Communication Functions
#### 3.2.1 Message Composition

**Function Information:**
- **Function Name**: `compose_agent_message`
- **Location**: `multi_agent_system.py:181`
- **Purpose**: Creates structured AgentMessage objects for inter-agent communication with proper timestamp and metadata

**Function Signature and Parameters:**
- **Input**: `sender: str` - The name of the sending agent
- **Input**: `receiver: str` - The name of the receiving agent
- **Input**: `message_type: str` - The type of message (instruction, result, feedback, etc.)
- **Input**: `content: str` - The message content
- **Input**: `step_id: Optional[str]` - Optional step identifier for workflow tracking
- **Return**: `AgentMessage` - Structured message object with timestamp and metadata
- **Side Effects**: None

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `datetime.datetime.now()` - External time dependency that should be mocked for deterministic testing
- **Dependencies to Use Directly**: 
  - `AgentMessage` - Internal TypedDict class definition
  - `Optional` - Built-in Python typing module
  - `str` - Built-in Python type
- **Mock Configuration**: Mock `datetime.datetime.now()` to return fixed timestamps for consistent testing

**Test Cases:**

**Happy Path:**
- **Complete Message**: All parameters provided with valid values
- **Message with Step ID**: Message includes step_id for workflow tracking
- **Different Message Types**: Various message types (instruction, result, feedback)

**Edge Cases:**
- **Empty Content**: Empty string content
- **None Step ID**: Step ID explicitly set to None
- **Long Content**: Very long message content
- **Special Characters**: Content with special characters and Unicode

**Error Conditions:**
- **Missing Required Parameters**: Missing sender, receiver, message_type, or content
- **Invalid Message Type**: Unrecognized message type
- **Invalid Agent Names**: Empty or invalid agent names

**Mock Configurations:**
```python
@patch('multi_agent_system.datetime')
def test_message_composition_with_mocked_time(mock_datetime):
    """Mock datetime for consistent timestamp testing."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of internal types and built-in functions in the implementation:
from typing import Optional
from multi_agent_system import AgentMessage

def compose_agent_message(
    sender: str, 
    receiver: str, 
    message_type: str, 
    content: str, 
    step_id: Optional[str] = None
) -> AgentMessage:
    # Direct use of AgentMessage TypedDict
    # Direct use of datetime.datetime.now() (to be mocked in tests)
    return AgentMessage(
        timestamp=datetime.now().isoformat(),  # This should be mocked
        sender=sender,
        receiver=receiver,
        type=message_type,
        content=content,
        step_id=step_id
    )
```

**Assertion Specifications:**
- Verify message object has correct structure and field types
- Verify timestamp is properly formatted ISO string
- Verify all input parameters are correctly assigned
- Verify optional step_id is handled properly
- Verify message type validation (if implemented)
- Verify no side effects occur (function is pure)

**Code Examples:**
```python
from datetime import datetime
from unittest.mock import patch
import pytest
from multi_agent_system import compose_agent_message, AgentMessage

@patch('multi_agent_system.datetime')
def test_compose_agent_message_complete(mock_datetime):
    """Test that function creates complete message with all fields."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    result = compose_agent_message(
        sender="planner",
        receiver="researcher",
        message_type="instruction",
        content="Research the topic",
        step_id="step_1"
    )
    
    assert isinstance(result, dict)  # AgentMessage is TypedDict
    assert result["timestamp"] == "2024-01-01T12:00:00"
    assert result["sender"] == "planner"
    assert result["receiver"] == "researcher"
    assert result["type"] == "instruction"
    assert result["content"] == "Research the topic"
    assert result["step_id"] == "step_1"

@patch('multi_agent_system.datetime')
def test_compose_agent_message_without_step_id(mock_datetime):
    """Test that function works without step_id."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    result = compose_agent_message(
        sender="researcher",
        receiver="expert",
        message_type="result",
        content="Research completed"
    )
    
    assert result["timestamp"] == "2024-01-01T12:00:00"
    assert result["sender"] == "researcher"
    assert result["receiver"] == "expert"
    assert result["type"] == "result"
    assert result["content"] == "Research completed"
    assert result["step_id"] is None

@patch('multi_agent_system.datetime')
def test_compose_agent_message_different_types(mock_datetime):
    """Test that function handles different message types."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    message_types = ["instruction", "result", "feedback", "error", "status"]
    
    for msg_type in message_types:
        result = compose_agent_message(
            sender="agent1",
            receiver="agent2",
            message_type=msg_type,
            content=f"Message of type {msg_type}"
        )
        assert result["type"] == msg_type
        assert result["content"] == f"Message of type {msg_type}"

@patch('multi_agent_system.datetime')
def test_compose_agent_message_empty_content(mock_datetime):
    """Test that function handles empty content."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    result = compose_agent_message(
        sender="agent1",
        receiver="agent2",
        message_type="status",
        content=""
    )
    
    assert result["content"] == ""
    assert result["timestamp"] == "2024-01-01T12:00:00"

@patch('multi_agent_system.datetime')
def test_compose_agent_message_long_content(mock_datetime):
    """Test that function handles long content."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    long_content = "A" * 1000  # 1000 character content
    
    result = compose_agent_message(
        sender="agent1",
        receiver="agent2",
        message_type="result",
        content=long_content
    )
    
    assert result["content"] == long_content
    assert len(result["content"]) == 1000

@patch('multi_agent_system.datetime')
def test_compose_agent_message_special_characters(mock_datetime):
    """Test that function handles special characters and Unicode."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    special_content = "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~"
    unicode_content = "Unicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€"
    
    result1 = compose_agent_message(
        sender="agent1",
        receiver="agent2",
        message_type="test",
        content=special_content
    )
    
    result2 = compose_agent_message(
        sender="agent1",
        receiver="agent2",
        message_type="test",
        content=unicode_content
    )
    
    assert result1["content"] == special_content
    assert result2["content"] == unicode_content

@patch('multi_agent_system.datetime')
def test_compose_agent_message_none_step_id(mock_datetime):
    """Test that function handles explicit None step_id."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    result = compose_agent_message(
        sender="agent1",
        receiver="agent2",
        message_type="test",
        content="Test message",
        step_id=None
    )
    
    assert result["step_id"] is None

@patch('multi_agent_system.datetime')
def test_compose_agent_message_timestamp_format(mock_datetime):
    """Test that timestamp is properly formatted."""
    test_times = [
        datetime(2024, 1, 1, 12, 0, 0),
        datetime(2024, 12, 31, 23, 59, 59),
        datetime(2024, 6, 15, 6, 30, 45)
    ]
    
    for test_time in test_times:
        mock_datetime.datetime.now.return_value = test_time
        
        result = compose_agent_message(
            sender="agent1",
            receiver="agent2",
            message_type="test",
            content="Test message"
        )
        
        expected_timestamp = test_time.isoformat()
        assert result["timestamp"] == expected_timestamp

@patch('multi_agent_system.datetime')
def test_compose_agent_message_structure_validation(mock_datetime):
    """Test that returned object has correct AgentMessage structure."""
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    
    result = compose_agent_message(
        sender="agent1",
        receiver="agent2",
        message_type="test",
        content="Test message",
        step_id="step_1"
    )
    
    # Verify all required fields are present
    required_fields = ["timestamp", "sender", "receiver", "type", "content", "step_id"]
    for field in required_fields:
        assert field in result
    
    # Verify field types
    assert isinstance(result["timestamp"], str)
    assert isinstance(result["sender"], str)
    assert isinstance(result["receiver"], str)
    assert isinstance(result["type"], str)
    assert isinstance(result["content"], str)
    assert isinstance(result["step_id"], str) or result["step_id"] is None
```
#### 3.2.2 Message Sending

**Function Information:**
- **Function Name**: `send_message`
- **Location**: `multi_agent_system.py:206`
- **Purpose**: Adds a message to the agent_messages list in the graph state for inter-agent communication

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing agent_messages list
- **Input**: `message: AgentMessage` - The message to send and store in the state
- **Return**: `GraphState` - The updated state with the message added to agent_messages
- **Side Effects**: Modifies the agent_messages list in the state

**Dependencies Analysis:**
- **Dependencies to Mock**: None - This function only modifies internal state
- **Dependencies to Use Directly**: 
  - `list.append()` - Built-in Python list method
  - `GraphState` - Internal TypedDict class definition
  - `AgentMessage` - Internal TypedDict class definition
- **Mock Configuration**: Not applicable - no external dependencies to mock

**Test Cases:**

**Happy Path:**
- **Single Message**: Send one message and verify it's added to the list
- **Multiple Messages**: Send multiple messages and verify they're all stored
- **Message with All Fields**: Send message with complete AgentMessage structure

**Edge Cases:**
- **Empty Message List**: Send message when agent_messages is empty
- **Large Message List**: Send message when agent_messages already contains many messages
- **Message with None Step ID**: Send message with step_id set to None

**Error Conditions:**
- **Invalid Message Structure**: Send message missing required fields
- **Invalid State**: Pass state without agent_messages field
- **None Message**: Pass None as message parameter

**Mock Configurations:**
```python
# No mocks needed - this function only modifies internal state
# All dependencies are built-in Python functions and internal types
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and internal types in the implementation:
def send_message(state: GraphState, message: AgentMessage) -> GraphState:
    state["agent_messages"].append(message)  # Direct use of list.append()
    return state
```

**Assertion Specifications:**
- Verify message is added to the end of agent_messages list
- Verify state is returned with updated agent_messages
- Verify original message object is preserved in the list
- Verify list length increases by exactly one
- Verify no other state fields are modified

**Code Examples:**
```python
import pytest
from multi_agent_system import send_message, AgentMessage, GraphState

def test_send_message_single_message():
    """Test that function adds a single message to the state."""
    # Setup initial state
    state = GraphState(
        agent_messages=[],
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="planner",
        receiver="orchestrator",
        type="instruction",
        content="Test message content",
        step_id=1
    )
    
    # Execute function
    result = send_message(state, message)
    
    # Verify results
    assert len(result["agent_messages"]) == 1
    assert result["agent_messages"][0] == message
    assert result["agent_messages"][0]["sender"] == "planner"
    assert result["agent_messages"][0]["receiver"] == "orchestrator"
    assert result["agent_messages"][0]["type"] == "instruction"
    assert result["agent_messages"][0]["content"] == "Test message content"
    assert result["agent_messages"][0]["step_id"] == 1

def test_send_message_multiple_messages():
    """Test that function adds multiple messages to the state."""
    # Setup initial state with existing messages
    existing_message = AgentMessage(
        timestamp="2024-01-01T11:00:00",
        sender="orchestrator",
        receiver="planner",
        type="instruction",
        content="Initial message",
        step_id=None
    )
    
    state = GraphState(
        agent_messages=[existing_message],
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    new_message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="planner",
        receiver="orchestrator",
        type="response",
        content="Response message",
        step_id=1
    )
    
    # Execute function
    result = send_message(state, new_message)
    
    # Verify results
    assert len(result["agent_messages"]) == 2
    assert result["agent_messages"][0] == existing_message
    assert result["agent_messages"][1] == new_message
    assert result["agent_messages"][1]["sender"] == "planner"
    assert result["agent_messages"][1]["type"] == "response"

def test_send_message_empty_message_list():
    """Test that function works with empty message list."""
    state = GraphState(
        agent_messages=[],
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="researcher",
        receiver="expert",
        type="result",
        content="Research result",
        step_id=None
    )
    
    result = send_message(state, message)
    
    assert len(result["agent_messages"]) == 1
    assert result["agent_messages"][0] == message

def test_send_message_none_step_id():
    """Test that function handles message with None step_id."""
    state = GraphState(
        agent_messages=[],
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="expert",
        receiver="finalizer",
        type="answer",
        content="Expert answer",
        step_id=None
    )
    
    result = send_message(state, message)
    
    assert len(result["agent_messages"]) == 1
    assert result["agent_messages"][0]["step_id"] is None

def test_send_message_preserves_other_state_fields():
    """Test that function only modifies agent_messages and preserves other fields."""
    state = GraphState(
        agent_messages=[],
        question="Original question",
        file="test_file.txt",
        research_steps=["step1", "step2"],
        expert_steps=["expert1"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="planner",
        next_step="researcher",
        planner_retry_count=1,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="planner",
        receiver="orchestrator",
        type="instruction",
        content="Test message",
        step_id=1
    )
    
    result = send_message(state, message)
    
    # Verify agent_messages was modified
    assert len(result["agent_messages"]) == 1
    
    # Verify other fields remain unchanged
    assert result["question"] == "Original question"
    assert result["file"] == "test_file.txt"
    assert result["research_steps"] == ["step1", "step2"]
    assert result["expert_steps"] == ["expert1"]
    assert result["current_step"] == "planner"
    assert result["next_step"] == "researcher"
    assert result["planner_retry_count"] == 1

def test_send_message_large_message_list():
    """Test that function works with large existing message list."""
    # Create state with many existing messages
    existing_messages = []
    for i in range(100):
        message = AgentMessage(
            timestamp=f"2024-01-01T{i:02d}:00:00",
            sender="agent1",
            receiver="agent2",
            type="test",
            content=f"Message {i}",
            step_id=i
        )
        existing_messages.append(message)
    
    state = GraphState(
        agent_messages=existing_messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    new_message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="new_agent",
        receiver="target_agent",
        type="new_message",
        content="New message content",
        step_id=101
    )
    
    result = send_message(state, new_message)
    
    assert len(result["agent_messages"]) == 101
    assert result["agent_messages"][-1] == new_message
    assert result["agent_messages"][0] == existing_messages[0]

def test_send_message_message_immutability():
    """Test that the original message object is not modified."""
    state = GraphState(
        agent_messages=[],
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    original_message = AgentMessage(
        timestamp="2024-01-01T12:00:00",
        sender="planner",
        receiver="orchestrator",
        type="instruction",
        content="Original content",
        step_id=1
    )
    
    # Store original message for comparison
    original_message_copy = {
        "timestamp": original_message["timestamp"],
        "sender": original_message["sender"],
        "receiver": original_message["receiver"],
        "type": original_message["type"],
        "content": original_message["content"],
        "step_id": original_message["step_id"]
    }
    
    result = send_message(state, original_message)
    
    # Verify message in state matches original
    stored_message = result["agent_messages"][0]
    assert stored_message["timestamp"] == original_message_copy["timestamp"]
    assert stored_message["sender"] == original_message_copy["sender"]
    assert stored_message["receiver"] == original_message_copy["receiver"]
    assert stored_message["type"] == original_message_copy["type"]
    assert stored_message["content"] == original_message_copy["content"]
    assert stored_message["step_id"] == original_message_copy["step_id"]
```

#### 3.2.3 Agent Conversation Retrieval

**Function Information:**
- **Function Name**: `get_agent_conversation`
- **Location**: `multi_agent_system.py:213`
- **Purpose**: Retrieves filtered conversation messages between the orchestrator and a specific agent from the graph state

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing agent_messages list
- **Input**: `agent_name: str` - The name of the agent to filter conversations for
- **Input**: `types: Optional[List[str]]` - Optional list of message types to filter by
- **Input**: `step_id: Optional[int]` - Optional step identifier to filter messages by
- **Return**: `List[AgentMessage]` - Filtered list of messages between orchestrator and the agent
- **Side Effects**: None - This function only reads state, does not modify it

**Dependencies Analysis:**
- **Dependencies to Mock**: None - This function only performs list filtering operations
- **Dependencies to Use Directly**: 
  - List comprehension - Built-in Python list filtering
  - Dictionary access - Built-in Python dict key access
  - `in` operator - Built-in Python membership testing
  - `Optional` - Built-in Python typing module
  - `List` - Built-in Python typing module
- **Mock Configuration**: Not applicable - no external dependencies to mock

**Test Cases:**

**Happy Path:**
- **Basic Conversation Retrieval**: Get all messages between orchestrator and agent
- **Type Filtering**: Filter messages by specific message types
- **Step ID Filtering**: Filter messages by specific step ID
- **Combined Filtering**: Filter by both types and step ID

**Edge Cases:**
- **Empty Message List**: No messages in state
- **No Matching Messages**: No messages between orchestrator and specified agent
- **None Parameters**: Pass None for types or step_id parameters
- **Empty Type List**: Pass empty list for types parameter
- **Multiple Message Types**: Messages with various types

**Error Conditions:**
- **Invalid Agent Name**: Agent name that doesn't exist in messages
- **Invalid State**: State without agent_messages field
- **Invalid Message Structure**: Messages missing required fields

**Mock Configurations:**
```python
# No mocks needed - this function only performs list filtering operations
# All dependencies are built-in Python functions and operations
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and operations in the implementation:
def get_agent_conversation(state: GraphState, agent_name: str, types: Optional[List[str]] = None, step_id: Optional[int] = None) -> List[AgentMessage]:
    # Direct use of list comprehension and dictionary access
    inbox = [
        m for m in state["agent_messages"]  # Direct dict access
        if (m["sender"] == agent_name and m["receiver"] == "orchestrator")  # Direct dict access and comparison
        or (m["sender"] == "orchestrator" and m["receiver"] == agent_name)  # Direct dict access and comparison
    ]
    if step_id is not None:  # Direct None comparison
        inbox = [m for m in inbox if m["step_id"] == step_id]  # Direct dict access and comparison
    if types:  # Direct truthiness check
        inbox = [m for m in inbox if m["type"] in types]  # Direct dict access and in operator
    return inbox
```

**Assertion Specifications:**
- Verify returned list contains only messages between orchestrator and specified agent
- Verify type filtering works correctly when types parameter is provided
- Verify step_id filtering works correctly when step_id parameter is provided
- Verify combined filtering works when both parameters are provided
- Verify empty list is returned when no matching messages exist
- Verify function handles None parameters gracefully
- Verify function doesn't modify the original state

**Code Examples:**
```python
import pytest
from multi_agent_system import get_agent_conversation, AgentMessage, GraphState

def test_get_agent_conversation_basic():
    """Test basic conversation retrieval between orchestrator and agent."""
    # Setup test messages
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Plan this task",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Task planned",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="researcher",
            receiver="expert",
            type="result",
            content="Research complete",
            step_id=2
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    # Execute function
    result = get_agent_conversation(state, "planner")
    
    # Verify results
    assert len(result) == 2
    assert result[0]["sender"] == "orchestrator"
    assert result[0]["receiver"] == "planner"
    assert result[1]["sender"] == "planner"
    assert result[1]["receiver"] == "orchestrator"
    # Verify researcher message is not included
    assert not any(msg["sender"] == "researcher" for msg in result)

def test_get_agent_conversation_type_filtering():
    """Test filtering messages by type."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Plan this task",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Task planned",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="planner",
            type="feedback",
            content="Good plan",
            step_id=1
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    # Test filtering by single type
    result = get_agent_conversation(state, "planner", types=["instruction"])
    assert len(result) == 1
    assert result[0]["type"] == "instruction"
    
    # Test filtering by multiple types
    result = get_agent_conversation(state, "planner", types=["instruction", "response"])
    assert len(result) == 2
    assert all(msg["type"] in ["instruction", "response"] for msg in result)

def test_get_agent_conversation_step_id_filtering():
    """Test filtering messages by step_id."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="researcher",
            type="instruction",
            content="Research step 1",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="researcher",
            receiver="orchestrator",
            type="response",
            content="Step 1 complete",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="researcher",
            type="instruction",
            content="Research step 2",
            step_id=2
        ),
        AgentMessage(
            timestamp="2024-01-01T12:03:00",
            sender="researcher",
            receiver="orchestrator",
            type="response",
            content="Step 2 complete",
            step_id=2
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    # Test filtering by step_id
    result = get_agent_conversation(state, "researcher", step_id=1)
    assert len(result) == 2
    assert all(msg["step_id"] == 1 for msg in result)
    
    result = get_agent_conversation(state, "researcher", step_id=2)
    assert len(result) == 2
    assert all(msg["step_id"] == 2 for msg in result)

def test_get_agent_conversation_combined_filtering():
    """Test filtering by both types and step_id."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="expert",
            type="instruction",
            content="Expert instruction",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="expert",
            receiver="orchestrator",
            type="response",
            content="Expert response",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="expert",
            type="feedback",
            content="Expert feedback",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:03:00",
            sender="orchestrator",
            receiver="expert",
            type="instruction",
            content="Another instruction",
            step_id=2
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    # Test combined filtering
    result = get_agent_conversation(state, "expert", types=["instruction"], step_id=1)
    assert len(result) == 1
    assert result[0]["type"] == "instruction"
    assert result[0]["step_id"] == 1

def test_get_agent_conversation_empty_message_list():
    """Test function behavior with empty message list."""
    state = GraphState(
        agent_messages=[],
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    result = get_agent_conversation(state, "planner")
    assert len(result) == 0

def test_get_agent_conversation_no_matching_messages():
    """Test function behavior when no messages match the agent."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="researcher",
            type="instruction",
            content="Research task",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="researcher",
            receiver="orchestrator",
            type="response",
            content="Research complete",
            step_id=1
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    result = get_agent_conversation(state, "planner")
    assert len(result) == 0

def test_get_agent_conversation_none_parameters():
    """Test function behavior with None parameters."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Test instruction",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Test response",
            step_id=None
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    # Test with None types
    result = get_agent_conversation(state, "planner", types=None)
    assert len(result) == 2
    
    # Test with None step_id
    result = get_agent_conversation(state, "planner", step_id=None)
    assert len(result) == 2
    
    # Test with both None
    result = get_agent_conversation(state, "planner", types=None, step_id=None)
    assert len(result) == 2

def test_get_agent_conversation_empty_type_list():
    """Test function behavior with empty type list."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Test instruction",
            step_id=1
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    result = get_agent_conversation(state, "planner", types=[])
    assert len(result) == 0

def test_get_agent_conversation_state_immutability():
    """Test that function doesn't modify the original state."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Test instruction",
            step_id=1
        )
    ]
    
    state = GraphState(
        agent_messages=messages.copy(),  # Create a copy to test immutability
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    original_messages_count = len(state["agent_messages"])
    
    # Execute function
    result = get_agent_conversation(state, "planner")
    
    # Verify state wasn't modified
    assert len(state["agent_messages"]) == original_messages_count
    assert state["agent_messages"] == messages

def test_get_agent_conversation_multiple_message_types():
    """Test filtering with multiple message types."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="finalizer",
            type="instruction",
            content="Finalize answer",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="finalizer",
            receiver="orchestrator",
            type="response",
            content="Answer finalized",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="finalizer",
            type="feedback",
            content="Good work",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:03:00",
            sender="finalizer",
            receiver="orchestrator",
            type="status",
            content="Processing complete",
            step_id=1
        )
    ]
    
    state = GraphState(
        agent_messages=messages,
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=3,
        expert_retry_limit=3
    )
    
    # Test filtering by multiple types
    result = get_agent_conversation(state, "finalizer", types=["instruction", "response", "feedback"])
    assert len(result) == 3
    assert all(msg["type"] in ["instruction", "response", "feedback"] for msg in result)
    
    # Verify status message is excluded
    assert not any(msg["type"] == "status" for msg in result)
```

#### 3.2.4 Agent Message to LangChain Conversion

**Function Information:**
- **Function Name**: `convert_agent_messages_to_langchain`
- **Location**: `multi_agent_system.py:238`
- **Purpose**: Converts a list of AgentMessage objects to LangChain BaseMessage objects for use in LLM interactions

**Function Signature and Parameters:**
- **Input**: `messages: List[AgentMessage]` - List of AgentMessage objects to convert
- **Return**: `List[BaseMessage]` - List of LangChain BaseMessage objects (HumanMessage or AIMessage)
- **Side Effects**: None - This function only performs data transformation

**Dependencies Analysis:**
- **Dependencies to Mock**: None - This function only uses LangChain message constructors
- **Dependencies to Use Directly**: 
  - `HumanMessage` - LangChain message class for human messages
  - `AIMessage` - LangChain message class for AI messages
  - `List` - Built-in Python typing module
  - List iteration - Built-in Python list functionality
  - Dictionary access - Built-in Python dict key access
- **Mock Configuration**: Not applicable - no external dependencies to mock

**Test Cases:**

**Happy Path:**
- **Single Orchestrator Message**: Convert one message from orchestrator to HumanMessage
- **Single Agent Message**: Convert one message from agent to AIMessage
- **Mixed Messages**: Convert multiple messages with different senders
- **Multiple Messages**: Convert list with multiple messages

**Edge Cases:**
- **Empty Message List**: Convert empty list of messages
- **Single Message**: Convert list with only one message
- **Large Message List**: Convert list with many messages
- **Messages with Empty Content**: Handle messages with empty content strings

**Error Conditions:**
- **Invalid Message Structure**: Messages missing required fields
- **None Input**: Pass None as messages parameter
- **Invalid Sender**: Messages with unexpected sender values

**Mock Configurations:**
```python
# No mocks needed - this function only uses LangChain message constructors
# LangChain message classes can be used directly in tests
```

**Direct Usage Examples:**
```python
# Direct usage of LangChain message classes in the implementation:
from langchain_core.messages import HumanMessage, AIMessage

def convert_agent_messages_to_langchain(messages: List[AgentMessage]) -> List[BaseMessage]:
    converted_messages = []
    for m in messages:  # Direct list iteration
        if m["sender"] == "orchestrator":  # Direct dict access and comparison
            message = HumanMessage(content=m["content"])  # Direct LangChain constructor
        else:
            message = AIMessage(content=m["content"])  # Direct LangChain constructor
        converted_messages.append(message)  # Direct list append
    return converted_messages
```

**Assertion Specifications:**
- Verify orchestrator messages are converted to HumanMessage objects
- Verify agent messages are converted to AIMessage objects
- Verify message content is preserved correctly
- Verify returned list has same length as input list
- Verify message order is maintained
- Verify function handles empty content gracefully

**Code Examples:**
```python
import pytest
from langchain_core.messages import HumanMessage, AIMessage
from multi_agent_system import convert_agent_messages_to_langchain, AgentMessage

def test_convert_agent_messages_single_orchestrator():
    """Test converting single orchestrator message to HumanMessage."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Plan this task",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 1
    assert isinstance(result[0], HumanMessage)
    assert result[0].content == "Plan this task"

def test_convert_agent_messages_single_agent():
    """Test converting single agent message to AIMessage."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Task planned successfully",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 1
    assert isinstance(result[0], AIMessage)
    assert result[0].content == "Task planned successfully"

def test_convert_agent_messages_mixed_senders():
    """Test converting messages with different senders."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="researcher",
            type="instruction",
            content="Research this topic",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="researcher",
            receiver="orchestrator",
            type="response",
            content="Research completed",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="expert",
            type="instruction",
            content="Analyze the results",
            step_id=2
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 3
    assert isinstance(result[0], HumanMessage)
    assert isinstance(result[1], AIMessage)
    assert isinstance(result[2], HumanMessage)
    assert result[0].content == "Research this topic"
    assert result[1].content == "Research completed"
    assert result[2].content == "Analyze the results"

def test_convert_agent_messages_empty_list():
    """Test converting empty message list."""
    messages = []
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 0
    assert isinstance(result, list)

def test_convert_agent_messages_empty_content():
    """Test converting messages with empty content."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 2
    assert isinstance(result[0], HumanMessage)
    assert isinstance(result[1], AIMessage)
    assert result[0].content == ""
    assert result[1].content == ""

def test_convert_agent_messages_large_list():
    """Test converting large list of messages."""
    messages = []
    for i in range(100):
        sender = "orchestrator" if i % 2 == 0 else "planner"
        message = AgentMessage(
            timestamp=f"2024-01-01T{i:02d}:00:00",
            sender=sender,
            receiver="planner" if sender == "orchestrator" else "orchestrator",
            type="test",
            content=f"Message {i}",
            step_id=i
        )
        messages.append(message)
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 100
    for i, msg in enumerate(result):
        if i % 2 == 0:
            assert isinstance(msg, HumanMessage)
        else:
            assert isinstance(msg, AIMessage)
        assert msg.content == f"Message {i}"

def test_convert_agent_messages_preserves_order():
    """Test that message order is preserved during conversion."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="First message",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Second message",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="planner",
            type="feedback",
            content="Third message",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 3
    assert result[0].content == "First message"
    assert result[1].content == "Second message"
    assert result[2].content == "Third message"
    assert isinstance(result[0], HumanMessage)
    assert isinstance(result[1], AIMessage)
    assert isinstance(result[2], HumanMessage)

def test_convert_agent_messages_different_agents():
    """Test converting messages from different agents."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="researcher",
            receiver="orchestrator",
            type="result",
            content="Research result",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="expert",
            receiver="orchestrator",
            type="answer",
            content="Expert answer",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="critic",
            receiver="orchestrator",
            type="feedback",
            content="Critic feedback",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 3
    assert all(isinstance(msg, AIMessage) for msg in result)
    assert result[0].content == "Research result"
    assert result[1].content == "Expert answer"
    assert result[2].content == "Critic feedback"

def test_convert_agent_messages_special_characters():
    """Test converting messages with special characters and Unicode."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Unicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 2
    assert isinstance(result[0], HumanMessage)
    assert isinstance(result[1], AIMessage)
    assert result[0].content == "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~"
    assert result[1].content == "Unicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€"

def test_convert_agent_messages_long_content():
    """Test converting messages with long content."""
    long_content = "A" * 1000  # 1000 character content
    
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content=long_content,
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 1
    assert isinstance(result[0], HumanMessage)
    assert result[0].content == long_content
    assert len(result[0].content) == 1000

def test_convert_agent_messages_none_step_id():
    """Test converting messages with None step_id."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="Test message",
            step_id=None
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Test response",
            step_id=None
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 2
    assert isinstance(result[0], HumanMessage)
    assert isinstance(result[1], AIMessage)
    assert result[0].content == "Test message"
    assert result[1].content == "Test response"

def test_convert_agent_messages_all_orchestrator():
    """Test converting list with only orchestrator messages."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="orchestrator",
            receiver="planner",
            type="instruction",
            content="First instruction",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="orchestrator",
            receiver="researcher",
            type="instruction",
            content="Second instruction",
            step_id=2
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="orchestrator",
            receiver="expert",
            type="instruction",
            content="Third instruction",
            step_id=3
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 3
    assert all(isinstance(msg, HumanMessage) for msg in result)
    assert result[0].content == "First instruction"
    assert result[1].content == "Second instruction"
    assert result[2].content == "Third instruction"

def test_convert_agent_messages_all_agents():
    """Test converting list with only agent messages (no orchestrator)."""
    messages = [
        AgentMessage(
            timestamp="2024-01-01T12:00:00",
            sender="planner",
            receiver="orchestrator",
            type="response",
            content="Planner response",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:01:00",
            sender="researcher",
            receiver="orchestrator",
            type="result",
            content="Researcher result",
            step_id=1
        ),
        AgentMessage(
            timestamp="2024-01-01T12:02:00",
            sender="expert",
            receiver="orchestrator",
            type="answer",
            content="Expert answer",
            step_id=1
        )
    ]
    
    result = convert_agent_messages_to_langchain(messages)
    
    assert len(result) == 3
    assert all(isinstance(msg, AIMessage) for msg in result)
    assert result[0].content == "Planner response"
    assert result[1].content == "Researcher result"
    assert result[2].content == "Expert answer"
```
### 3.3 Research Tools
#### 3.3.1 YouTube Video Transcribing Tool

**Function Information:**
- **Function Name**: `youtube_transcript_tool`
- **Location**: `multi_agent_system.py:260`
- **Purpose**: Extracts transcript content and metadata from YouTube video URLs using LangChain's YoutubeLoader

**Function Signature and Parameters:**
- **Input**: `url: str` - YouTube video URL to extract transcript from
- **Return**: `str` - Combined video metadata and transcript content
- **Side Effects**: None - This function only performs data extraction

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `YoutubeLoader.from_youtube_url()` - External YouTube API dependency
  - `YoutubeLoader.load()` - External YouTube API dependency
  - `logger.info()` - Logging dependency for consistent testing
- **Dependencies to Use Directly**: 
  - `str` - Built-in Python string operations
  - `f-string` formatting - Built-in Python string formatting
  - `dict.get()` - Built-in Python dictionary method
  - `Document` - LangChain document class (for type checking)
- **Mock Configuration**: Mock YoutubeLoader to return predefined document objects with metadata and content

**Test Cases:**

**Happy Path:**
- **Valid YouTube URL**: Extract transcript from valid YouTube video URL
- **Complete Metadata**: Video with title, author, and duration information
- **Long Transcript**: Video with substantial transcript content

**Edge Cases:**
- **Empty Transcript**: Video with no transcript available
- **Missing Metadata**: Video with incomplete metadata fields
- **Short Transcript**: Video with minimal transcript content
- **Special Characters**: Transcript containing special characters and Unicode

**Error Conditions:**
- **Invalid URL**: Malformed or non-YouTube URLs
- **Network Errors**: YouTube API connection failures
- **Video Not Found**: Non-existent video URLs
- **Private Video**: Access to private or restricted videos

**Mock Configurations:**
```python
@patch('multi_agent_system.YoutubeLoader')
def test_youtube_transcript_with_mocked_loader(mock_loader):
    """Mock YoutubeLoader for consistent testing."""
    # Mock document with complete metadata
    mock_document = Mock()
    mock_document.page_content = "Sample transcript content"
    mock_document.metadata = {
        "title": "Sample Video Title",
        "author": "Sample Channel",
        "length": 120
    }
    
    mock_loader.from_youtube_url.return_value.load.return_value = [mock_document]
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions in the implementation:
def youtube_transcript_tool(url: str) -> str:
    logger.info(f"YouTube transcript tool processing URL: {url}")  # Direct f-string formatting
    
    # Load the YouTube video transcript (to be mocked)
    loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)  # External dependency
    documents = loader.load()  # External dependency
    
    if not documents:  # Direct list truthiness check
        logger.info("No transcript found for YouTube video")  # Direct logging
        return "No transcript found for this YouTube video."  # Direct string return
    
    # Extract transcript content
    transcript = documents[0].page_content  # Direct dict access
    
    # Add video metadata if available
    metadata = documents[0].metadata  # Direct dict access
    video_info = f"Video Title: {metadata.get('title', 'Unknown')}\n"  # Direct f-string and dict.get()
    video_info += f"Channel: {metadata.get('author', 'Unknown')}\n"  # Direct f-string and dict.get()
    video_info += f"Duration: {metadata.get('length', 'Unknown')} seconds\n\n"  # Direct f-string and dict.get()
    
    logger.info("YouTube transcript tool completed successfully")  # Direct logging
    return video_info + transcript  # Direct string concatenation
```

**Assertion Specifications:**
- Verify function returns combined metadata and transcript content
- Verify metadata fields are properly formatted with fallback values
- Verify transcript content is preserved correctly
- Verify function handles missing transcripts gracefully
- Verify logging calls are made with correct messages
- Verify function handles various metadata combinations

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import youtube_transcript_tool

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_valid_url(mock_logger, mock_loader):
    """Test extracting transcript from valid YouTube URL."""
    # Setup mock document with complete metadata
    mock_document = Mock()
    mock_document.page_content = "This is a sample transcript from the video."
    mock_document.metadata = {
        "title": "Sample Video Title",
        "author": "Sample Channel Name",
        "length": 180
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=sample123")
    
    # Verify results
    expected_output = (
        "Video Title: Sample Video Title\n"
        "Channel: Sample Channel Name\n"
        "Duration: 180 seconds\n\n"
        "This is a sample transcript from the video."
    )
    assert result == expected_output
    
    # Verify logging calls
    mock_logger.info.assert_any_call("YouTube transcript tool processing URL: https://www.youtube.com/watch?v=sample123")
    mock_logger.info.assert_any_call("YouTube transcript tool completed successfully")
    
    # Verify loader was called correctly
    mock_loader.from_youtube_url.assert_called_once_with("https://www.youtube.com/watch?v=sample123", add_video_info=True)

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_no_transcript(mock_logger, mock_loader):
    """Test handling of videos with no transcript."""
    # Configure mock loader to return empty documents
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=no_transcript")
    
    # Verify results
    assert result == "No transcript found for this YouTube video."
    
    # Verify logging calls
    mock_logger.info.assert_any_call("YouTube transcript tool processing URL: https://www.youtube.com/watch?v=no_transcript")
    mock_logger.info.assert_any_call("No transcript found for YouTube video")

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_missing_metadata(mock_logger, mock_loader):
    """Test handling of videos with missing metadata fields."""
    # Setup mock document with incomplete metadata
    mock_document = Mock()
    mock_document.page_content = "Transcript content without metadata."
    mock_document.metadata = {
        "title": "Known Title",
        # Missing author and length
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=missing_metadata")
    
    # Verify results
    expected_output = (
        "Video Title: Known Title\n"
        "Channel: Unknown\n"
        "Duration: Unknown seconds\n\n"
        "Transcript content without metadata."
    )
    assert result == expected_output

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_long_transcript(mock_logger, mock_loader):
    """Test handling of videos with long transcript content."""
    # Create long transcript content
    long_transcript = "This is a very long transcript. " * 100  # 100 repetitions
    
    mock_document = Mock()
    mock_document.page_content = long_transcript
    mock_document.metadata = {
        "title": "Long Video",
        "author": "Long Channel",
        "length": 3600
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=long_video")
    
    # Verify results
    expected_output = (
        "Video Title: Long Video\n"
        "Channel: Long Channel\n"
        "Duration: 3600 seconds\n\n"
        + long_transcript
    )
    assert result == expected_output
    assert len(result) > 1000  # Verify it's actually long

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_special_characters(mock_logger, mock_loader):
    """Test handling of transcripts with special characters and Unicode."""
    special_transcript = "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~\nUnicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€"
    
    mock_document = Mock()
    mock_document.page_content = special_transcript
    mock_document.metadata = {
        "title": "Special Video",
        "author": "Special Channel",
        "length": 120
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=special_chars")
    
    # Verify results
    expected_output = (
        "Video Title: Special Video\n"
        "Channel: Special Channel\n"
        "Duration: 120 seconds\n\n"
        + special_transcript
    )
    assert result == expected_output

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_empty_metadata(mock_logger, mock_loader):
    """Test handling of videos with completely empty metadata."""
    mock_document = Mock()
    mock_document.page_content = "Transcript with no metadata."
    mock_document.metadata = {}  # Empty metadata
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=empty_metadata")
    
    # Verify results
    expected_output = (
        "Video Title: Unknown\n"
        "Channel: Unknown\n"
        "Duration: Unknown seconds\n\n"
        "Transcript with no metadata."
    )
    assert result == expected_output

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_network_error(mock_logger, mock_loader):
    """Test handling of network errors during transcript loading."""
    # Configure mock loader to raise an exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = Exception("Network error")
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        youtube_transcript_tool("https://www.youtube.com/watch?v=network_error")
    
    assert "Network error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("YouTube transcript tool processing URL: https://www.youtube.com/watch?v=network_error")

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_multiple_documents(mock_logger, mock_loader):
    """Test handling when loader returns multiple documents (edge case)."""
    # Setup multiple mock documents
    mock_document1 = Mock()
    mock_document1.page_content = "First transcript part"
    mock_document1.metadata = {"title": "Video", "author": "Channel", "length": 60}
    
    mock_document2 = Mock()
    mock_document2.page_content = "Second transcript part"
    mock_document2.metadata = {"title": "Video", "author": "Channel", "length": 60}
    
    # Configure mock loader to return multiple documents
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document1, mock_document2]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=multiple_docs")
    
    # Verify results (should use first document)
    expected_output = (
        "Video Title: Video\n"
        "Channel: Channel\n"
        "Duration: 60 seconds\n\n"
        "First transcript part"
    )
    assert result == expected_output

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_none_metadata_values(mock_logger, mock_loader):
    """Test handling of None values in metadata."""
    mock_document = Mock()
    mock_document.page_content = "Transcript with None metadata."
    mock_document.metadata = {
        "title": None,
        "author": None,
        "length": None
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=none_metadata")
    
    # Verify results (should use fallback values)
    expected_output = (
        "Video Title: Unknown\n"
        "Channel: Unknown\n"
        "Duration: Unknown seconds\n\n"
        "Transcript with None metadata."
    )
    assert result == expected_output

@patch('multi_agent_system.YoutubeLoader')
@patch('multi_agent_system.logger')
def test_youtube_transcript_tool_very_short_transcript(mock_logger, mock_loader):
    """Test handling of videos with very short transcript content."""
    mock_document = Mock()
    mock_document.page_content = "Hi."  # Very short transcript
    mock_document.metadata = {
        "title": "Short Video",
        "author": "Short Channel",
        "length": 10
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.from_youtube_url.return_value = mock_loader_instance
    
    # Execute function
    result = youtube_transcript_tool("https://www.youtube.com/watch?v=short_video")
    
    # Verify results
    expected_output = (
        "Video Title: Short Video\n"
        "Channel: Short Channel\n"
        "Duration: 10 seconds\n\n"
        "Hi."
    )
    assert result == expected_output
```
#### 3.3.2 Unstructured Excel Tool

**Function Information:**
- **Function Name**: `unstructured_excel_tool`
- **Location**: `multi_agent_system.py:295`
- **Purpose**: Loads an Excel file using UnstructuredExcelLoader and returns the content as a list of Document objects

**Function Signature and Parameters:**
- **Input**: `file_path: str` - Path to the Excel file to be loaded
- **Return**: `list[Document]` - List of Document objects containing the Excel file content
- **Side Effects**: None - This function only performs file reading operations

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `UnstructuredExcelLoader` - External LangChain loader class for Excel files
  - `UnstructuredExcelLoader.load()` - External method that performs actual file reading
- **Dependencies to Use Directly**: 
  - `@tool` decorator - LangChain decorator for tool registration
  - `list` - Built-in Python list type annotation
  - `Document` - LangChain Document class (for type checking)
  - `str` - Built-in Python string type
- **Mock Configuration**: Mock UnstructuredExcelLoader to return predefined Document objects with metadata and content

**Test Cases:**

**Happy Path:**
- **Valid Excel File**: Load Excel file with standard content (text, numbers, formulas)
- **Multiple Sheets**: Excel file containing multiple worksheets
- **Complex Formatting**: Excel file with formatting, headers, and structured data
- **Large Excel File**: Excel file with substantial content and multiple rows/columns

**Edge Cases:**
- **Empty Excel File**: Excel file with no content or empty sheets
- **Single Cell Content**: Excel file with minimal content (single cell)
- **Mixed Data Types**: Excel file containing various data types (text, numbers, dates, formulas)
- **Special Characters**: Excel file with special characters, Unicode, and formatting

**Error Conditions:**
- **File Not Found**: Non-existent file path
- **Invalid File Format**: File that's not a valid Excel format
- **Corrupted File**: Damaged or corrupted Excel file
- **Permission Errors**: File with restricted access permissions
- **Empty File Path**: Empty or None file path

**Mock Configurations:**
```python
@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_with_mocked_loader(mock_loader):
    """Mock UnstructuredExcelLoader for consistent testing."""
    # Mock document with Excel content
    mock_document = Mock()
    mock_document.page_content = "Sample Excel content"
    mock_document.metadata = {
        "source": "test_file.xlsx",
        "file_path": "/path/to/test_file.xlsx"
    }
    
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and LangChain classes in the implementation:
from langchain_community.document_loaders import UnstructuredExcelLoader
from langchain_core.documents import Document

@tool
def unstructured_excel_tool(file_path: str) -> list[Document]:  # Direct use of @tool decorator and type annotations
    """
    Load an Excel file and return the content.
    """
    loader = UnstructuredExcelLoader(file_path)  # Direct instantiation of external class
    return loader.load()  # Direct method call on external object
```

**Assertion Specifications:**
- Verify function returns list of Document objects
- Verify Document objects contain expected content and metadata
- Verify loader is instantiated with correct file path
- Verify load() method is called on the loader instance
- Verify function handles various Excel content types correctly
- Verify error conditions are properly propagated
- Verify function maintains proper return type structure

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from langchain_core.documents import Document
from multi_agent_system import unstructured_excel_tool

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_valid_file(mock_loader):
    """Test loading a valid Excel file."""
    # Setup mock document with Excel content
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Sample Excel content with data"
    mock_document.metadata = {
        "source": "test_file.xlsx",
        "file_path": "/path/to/test_file.xlsx",
        "file_type": "xlsx"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/test_file.xlsx")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 1
    assert isinstance(result[0], Mock)  # Mock object with Document spec
    assert result[0].page_content == "Sample Excel content with data"
    assert result[0].metadata["source"] == "test_file.xlsx"
    
    # Verify loader was called correctly
    mock_loader.assert_called_once_with("/path/to/test_file.xlsx")
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_multiple_sheets(mock_loader):
    """Test loading Excel file with multiple sheets."""
    # Setup mock documents for multiple sheets
    mock_document1 = Mock(spec=Document)
    mock_document1.page_content = "Sheet 1 content"
    mock_document1.metadata = {"sheet_name": "Sheet1", "source": "multi_sheet.xlsx"}
    
    mock_document2 = Mock(spec=Document)
    mock_document2.page_content = "Sheet 2 content"
    mock_document2.metadata = {"sheet_name": "Sheet2", "source": "multi_sheet.xlsx"}
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document1, mock_document2]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/multi_sheet.xlsx")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 2
    assert result[0].page_content == "Sheet 1 content"
    assert result[1].page_content == "Sheet 2 content"
    assert result[0].metadata["sheet_name"] == "Sheet1"
    assert result[1].metadata["sheet_name"] == "Sheet2"

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_empty_file(mock_loader):
    """Test loading Excel file with no content."""
    # Configure mock loader to return empty list
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/empty_file.xlsx")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 0

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_complex_content(mock_loader):
    """Test loading Excel file with complex content and formatting."""
    complex_content = """
    Product Name    Price    Quantity    Total
    Widget A        $10.50   5           $52.50
    Widget B        $25.00   2           $50.00
    Widget C        $15.75   3           $47.25
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = complex_content
    mock_document.metadata = {
        "source": "inventory.xlsx",
        "file_path": "/path/to/inventory.xlsx",
        "has_formulas": True,
        "has_formatting": True
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/inventory.xlsx")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == complex_content
    assert result[0].metadata["has_formulas"] is True
    assert result[0].metadata["has_formatting"] is True

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_special_characters(mock_loader):
    """Test loading Excel file with special characters and Unicode."""
    special_content = "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~\nUnicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€\nFormulas: =SUM(A1:A10)"
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = special_content
    mock_document.metadata = {
        "source": "special_chars.xlsx",
        "file_path": "/path/to/special_chars.xlsx"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/special_chars.xlsx")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == special_content
    assert "ä½ å¥½ä¸–ç•Œ" in result[0].page_content
    assert "=SUM(A1:A10)" in result[0].page_content

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_file_not_found(mock_loader):
    """Test handling of non-existent file."""
    # Configure mock loader to raise FileNotFoundError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = FileNotFoundError("File not found")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError) as exc_info:
        unstructured_excel_tool("/path/to/nonexistent.xlsx")
    
    assert "File not found" in str(exc_info.value)
    
    # Verify loader was called
    mock_loader.assert_called_once_with("/path/to/nonexistent.xlsx")

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_invalid_format(mock_loader):
    """Test handling of invalid Excel file format."""
    # Configure mock loader to raise format-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = ValueError("Invalid Excel format")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        unstructured_excel_tool("/path/to/invalid.xlsx")
    
    assert "Invalid Excel format" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_permission_error(mock_loader):
    """Test handling of permission errors."""
    # Configure mock loader to raise PermissionError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = PermissionError("Access denied")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(PermissionError) as exc_info:
        unstructured_excel_tool("/path/to/restricted.xlsx")
    
    assert "Access denied" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_corrupted_file(mock_loader):
    """Test handling of corrupted Excel file."""
    # Configure mock loader to raise corruption-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = Exception("File is corrupted")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        unstructured_excel_tool("/path/to/corrupted.xlsx")
    
    assert "File is corrupted" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_large_file(mock_loader):
    """Test loading large Excel file with substantial content."""
    # Create large content
    large_content = "Large Excel content. " * 1000  # 1000 repetitions
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = large_content
    mock_document.metadata = {
        "source": "large_file.xlsx",
        "file_path": "/path/to/large_file.xlsx",
        "size": "large"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/large_file.xlsx")
    
    # Verify results
    assert len(result) == 1
    assert len(result[0].page_content) > 1000  # Verify it's actually large
    assert result[0].metadata["size"] == "large"

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_single_cell(mock_loader):
    """Test loading Excel file with minimal content (single cell)."""
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Single cell value"
    mock_document.metadata = {
        "source": "single_cell.xlsx",
        "file_path": "/path/to/single_cell.xlsx",
        "cell_count": 1
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/single_cell.xlsx")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == "Single cell value"
    assert result[0].metadata["cell_count"] == 1

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_mixed_data_types(mock_loader):
    """Test loading Excel file with various data types."""
    mixed_content = """
    Text Data    Number Data    Date Data        Formula Data
    Hello        42             2024-01-01       =A1+B1
    World        3.14           2024-12-31       =SUM(A1:A10)
    Test         -100           2024-06-15       =AVERAGE(B1:B10)
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = mixed_content
    mock_document.metadata = {
        "source": "mixed_data.xlsx",
        "file_path": "/path/to/mixed_data.xlsx",
        "has_text": True,
        "has_numbers": True,
        "has_dates": True,
        "has_formulas": True
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/mixed_data.xlsx")
    
    # Verify results
    assert len(result) == 1
    content = result[0].page_content
    assert "Hello" in content  # Text data
    assert "42" in content     # Number data
    assert "2024-01-01" in content  # Date data
    assert "=A1+B1" in content  # Formula data
    assert result[0].metadata["has_text"] is True
    assert result[0].metadata["has_formulas"] is True

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_loader_instantiation(mock_loader):
    """Test that UnstructuredExcelLoader is instantiated correctly."""
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    unstructured_excel_tool("/path/to/test.xlsx")
    
    # Verify loader was instantiated with correct file path
    mock_loader.assert_called_once_with("/path/to/test.xlsx")
    
    # Verify load method was called
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_unstructured_excel_tool_return_type_structure(mock_loader):
    """Test that function returns correct type structure."""
    # Setup mock document
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Test content"
    mock_document.metadata = {"source": "test.xlsx"}
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_excel_tool("/path/to/test.xlsx")
    
    # Verify return type structure
    assert isinstance(result, list)
    assert all(isinstance(doc, Mock) for doc in result)  # Mock objects with Document spec
    assert all(hasattr(doc, 'page_content') for doc in result)
    assert all(hasattr(doc, 'metadata') for doc in result)
```
#### 3.3.3 Unstructured PowerPoint Tool

**Function Information:**
- **Function Name**: `unstructured_powerpoint_tool`
- **Location**: `multi_agent_system.py:304`
- **Purpose**: Loads a PowerPoint file using UnstructuredPowerPointLoader and returns the content as a list of Document objects

**Function Signature and Parameters:**
- **Input**: `file_path: str` - Path to the PowerPoint file to be loaded
- **Return**: `list[Document]` - List of Document objects containing the PowerPoint file content
- **Side Effects**: None - This function only performs file reading operations

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `UnstructuredPowerPointLoader` - External LangChain loader class for PowerPoint files
  - `UnstructuredPowerPointLoader.load()` - External method that performs actual file reading
- **Dependencies to Use Directly**: 
  - `@tool` decorator - LangChain decorator for tool registration
  - `list` - Built-in Python list type annotation
  - `Document` - LangChain Document class (for type checking)
  - `str` - Built-in Python string type
- **Mock Configuration**: Mock UnstructuredPowerPointLoader to return predefined Document objects with metadata and content

**Test Cases:**

**Happy Path:**
- **Valid PowerPoint File**: Load PowerPoint file with standard content (text, images, slides)
- **Multiple Slides**: PowerPoint file containing multiple slides with various content
- **Complex Formatting**: PowerPoint file with formatting, layouts, and multimedia elements
- **Large PowerPoint File**: PowerPoint file with substantial content and many slides

**Edge Cases:**
- **Empty PowerPoint File**: PowerPoint file with no content or empty slides
- **Single Slide Content**: PowerPoint file with minimal content (single slide)
- **Mixed Content Types**: PowerPoint file containing various content types (text, images, charts, tables)
- **Special Characters**: PowerPoint file with special characters, Unicode, and formatting

**Error Conditions:**
- **File Not Found**: Non-existent file path
- **Invalid File Format**: File that's not a valid PowerPoint format
- **Corrupted File**: Damaged or corrupted PowerPoint file
- **Permission Errors**: File with restricted access permissions
- **Empty File Path**: Empty or None file path

**Mock Configurations:**
```python
@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_with_mocked_loader(mock_loader):
    """Mock UnstructuredPowerPointLoader for consistent testing."""
    # Mock document with PowerPoint content
    mock_document = Mock()
    mock_document.page_content = "Sample PowerPoint content"
    mock_document.metadata = {
        "source": "test_file.pptx",
        "file_path": "/path/to/test_file.pptx"
    }
    
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and LangChain classes in the implementation:
from langchain_community.document_loaders import UnstructuredPowerPointLoader
from langchain_core.documents import Document

@tool
def unstructured_powerpoint_tool(file_path: str) -> list[Document]:  # Direct use of @tool decorator and type annotations
    """
    Load a PowerPoint file and return the content.
    """
    loader = UnstructuredPowerPointLoader(file_path)  # Direct instantiation of external class
    return loader.load()  # Direct method call on external object
```

**Assertion Specifications:**
- Verify function returns list of Document objects
- Verify Document objects contain expected content and metadata
- Verify loader is instantiated with correct file path
- Verify load() method is called on the loader instance
- Verify function handles various PowerPoint content types correctly
- Verify error conditions are properly propagated
- Verify function maintains proper return type structure

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from langchain_core.documents import Document
from multi_agent_system import unstructured_powerpoint_tool

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_valid_file(mock_loader):
    """Test loading a valid PowerPoint file."""
    # Setup mock document with PowerPoint content
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Sample PowerPoint content with slides"
    mock_document.metadata = {
        "source": "test_file.pptx",
        "file_path": "/path/to/test_file.pptx",
        "file_type": "pptx"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/test_file.pptx")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 1
    assert isinstance(result[0], Mock)  # Mock object with Document spec
    assert result[0].page_content == "Sample PowerPoint content with slides"
    assert result[0].metadata["source"] == "test_file.pptx"
    
    # Verify loader was called correctly
    mock_loader.assert_called_once_with("/path/to/test_file.pptx")
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_multiple_slides(mock_loader):
    """Test loading PowerPoint file with multiple slides."""
    # Setup mock documents for multiple slides
    mock_document1 = Mock(spec=Document)
    mock_document1.page_content = "Slide 1: Introduction"
    mock_document1.metadata = {"slide_number": 1, "source": "multi_slide.pptx"}
    
    mock_document2 = Mock(spec=Document)
    mock_document2.page_content = "Slide 2: Main Content"
    mock_document2.metadata = {"slide_number": 2, "source": "multi_slide.pptx"}
    
    mock_document3 = Mock(spec=Document)
    mock_document3.page_content = "Slide 3: Conclusion"
    mock_document3.metadata = {"slide_number": 3, "source": "multi_slide.pptx"}
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document1, mock_document2, mock_document3]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/multi_slide.pptx")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 3
    assert result[0].page_content == "Slide 1: Introduction"
    assert result[1].page_content == "Slide 2: Main Content"
    assert result[2].page_content == "Slide 3: Conclusion"
    assert result[0].metadata["slide_number"] == 1
    assert result[1].metadata["slide_number"] == 2
    assert result[2].metadata["slide_number"] == 3

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_empty_file(mock_loader):
    """Test loading PowerPoint file with no content."""
    # Configure mock loader to return empty list
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/empty_file.pptx")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 0

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_complex_content(mock_loader):
    """Test loading PowerPoint file with complex content and formatting."""
    complex_content = """
    Title: Business Presentation
    
    Slide 1: Executive Summary
    - Revenue increased by 25%
    - Market share expanded to 15%
    - Customer satisfaction at 95%
    
    Slide 2: Financial Overview
    - Q1: $1.2M revenue
    - Q2: $1.5M revenue
    - Q3: $1.8M revenue
    - Q4: $2.1M revenue (projected)
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = complex_content
    mock_document.metadata = {
        "source": "business_presentation.pptx",
        "file_path": "/path/to/business_presentation.pptx",
        "has_images": True,
        "has_charts": True,
        "slide_count": 2
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/business_presentation.pptx")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == complex_content
    assert result[0].metadata["has_images"] is True
    assert result[0].metadata["has_charts"] is True
    assert result[0].metadata["slide_count"] == 2

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_special_characters(mock_loader):
    """Test loading PowerPoint file with special characters and Unicode."""
    special_content = "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~\nUnicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€\nBullet points: â€¢ First item â€¢ Second item"
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = special_content
    mock_document.metadata = {
        "source": "special_chars.pptx",
        "file_path": "/path/to/special_chars.pptx"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/special_chars.pptx")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == special_content
    assert "ä½ å¥½ä¸–ç•Œ" in result[0].page_content
    assert "â€¢ First item" in result[0].page_content

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_file_not_found(mock_loader):
    """Test handling of non-existent file."""
    # Configure mock loader to raise FileNotFoundError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = FileNotFoundError("File not found")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError) as exc_info:
        unstructured_powerpoint_tool("/path/to/nonexistent.pptx")
    
    assert "File not found" in str(exc_info.value)
    
    # Verify loader was called
    mock_loader.assert_called_once_with("/path/to/nonexistent.pptx")

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_invalid_format(mock_loader):
    """Test handling of invalid PowerPoint file format."""
    # Configure mock loader to raise format-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = ValueError("Invalid PowerPoint format")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        unstructured_powerpoint_tool("/path/to/invalid.pptx")
    
    assert "Invalid PowerPoint format" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_permission_error(mock_loader):
    """Test handling of permission errors."""
    # Configure mock loader to raise PermissionError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = PermissionError("Access denied")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(PermissionError) as exc_info:
        unstructured_powerpoint_tool("/path/to/restricted.pptx")
    
    assert "Access denied" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_corrupted_file(mock_loader):
    """Test handling of corrupted PowerPoint file."""
    # Configure mock loader to raise corruption-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = Exception("File is corrupted")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        unstructured_powerpoint_tool("/path/to/corrupted.pptx")
    
    assert "File is corrupted" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_large_file(mock_loader):
    """Test loading large PowerPoint file with substantial content."""
    # Create large content
    large_content = "Large PowerPoint content. " * 1000  # 1000 repetitions
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = large_content
    mock_document.metadata = {
        "source": "large_file.pptx",
        "file_path": "/path/to/large_file.pptx",
        "size": "large",
        "slide_count": 50
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/large_file.pptx")
    
    # Verify results
    assert len(result) == 1
    assert len(result[0].page_content) > 1000  # Verify it's actually large
    assert result[0].metadata["size"] == "large"
    assert result[0].metadata["slide_count"] == 50

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_single_slide(mock_loader):
    """Test loading PowerPoint file with minimal content (single slide)."""
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Single slide content"
    mock_document.metadata = {
        "source": "single_slide.pptx",
        "file_path": "/path/to/single_slide.pptx",
        "slide_count": 1
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/single_slide.pptx")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == "Single slide content"
    assert result[0].metadata["slide_count"] == 1

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_mixed_content_types(mock_loader):
    """Test loading PowerPoint file with various content types."""
    mixed_content = """
    Slide 1: Text Content
    - Bullet point 1
    - Bullet point 2
    - Bullet point 3
    
    Slide 2: Table Content
    | Column 1 | Column 2 | Column 3 |
    |----------|----------|----------|
    | Data 1   | Data 2   | Data 3   |
    | Data 4   | Data 5   | Data 6   |
    
    Slide 3: Chart Content
    [Chart: Revenue Growth 2020-2024]
    - 2020: $1M
    - 2021: $1.5M
    - 2022: $2M
    - 2023: $2.5M
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = mixed_content
    mock_document.metadata = {
        "source": "mixed_content.pptx",
        "file_path": "/path/to/mixed_content.pptx",
        "has_text": True,
        "has_tables": True,
        "has_charts": True,
        "has_images": False
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/mixed_content.pptx")
    
    # Verify results
    assert len(result) == 1
    content = result[0].page_content
    assert "Bullet point 1" in content  # Text content
    assert "| Column 1 |" in content    # Table content
    assert "[Chart: Revenue Growth" in content  # Chart content
    assert result[0].metadata["has_text"] is True
    assert result[0].metadata["has_tables"] is True
    assert result[0].metadata["has_charts"] is True

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_different_formats(mock_loader):
    """Test loading PowerPoint files in different formats."""
    formats = [".pptx", ".ppt"]
    
    for file_format in formats:
        file_path = f"/path/to/presentation{file_format}"
        
        mock_document = Mock(spec=Document)
        mock_document.page_content = f"Content from {file_format} file"
        mock_document.metadata = {
            "source": f"presentation{file_format}",
            "file_path": file_path,
            "format": file_format
        }
        
        # Configure mock loader
        mock_loader_instance = Mock()
        mock_loader_instance.load.return_value = [mock_document]
        mock_loader.return_value = mock_loader_instance
        
        # Execute function
        result = unstructured_powerpoint_tool(file_path)
        
        # Verify results
        assert len(result) == 1
        assert result[0].page_content == f"Content from {file_format} file"
        assert result[0].metadata["format"] == file_format
        
        # Verify loader was called with correct file path
        mock_loader.assert_called_with(file_path)

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_loader_instantiation(mock_loader):
    """Test that UnstructuredPowerPointLoader is instantiated correctly."""
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    unstructured_powerpoint_tool("/path/to/test.pptx")
    
    # Verify loader was instantiated with correct file path
    mock_loader.assert_called_once_with("/path/to/test.pptx")
    
    # Verify load method was called
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_return_type_structure(mock_loader):
    """Test that function returns correct type structure."""
    # Setup mock document
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Test content"
    mock_document.metadata = {"source": "test.pptx"}
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/test.pptx")
    
    # Verify return type structure
    assert isinstance(result, list)
    assert all(isinstance(doc, Mock) for doc in result)  # Mock objects with Document spec
    assert all(hasattr(doc, 'page_content') for doc in result)
    assert all(hasattr(doc, 'metadata') for doc in result)

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_unstructured_powerpoint_tool_presentation_structure(mock_loader):
    """Test loading PowerPoint file with typical presentation structure."""
    presentation_content = """
    Title Slide: Welcome to Our Company
    
    Agenda Slide:
    1. Company Overview
    2. Market Analysis
    3. Financial Performance
    4. Future Plans
    5. Q&A Session
    
    Content Slide 1: Company Overview
    - Founded in 2010
    - 500+ employees
    - Global presence in 20 countries
    
    Content Slide 2: Market Analysis
    - Market size: $10B
    - Growth rate: 15% annually
    - Competitive landscape
    
    Closing Slide: Thank You
    Questions and Answers
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = presentation_content
    mock_document.metadata = {
        "source": "company_presentation.pptx",
        "file_path": "/path/to/company_presentation.pptx",
        "slide_count": 5,
        "has_title_slide": True,
        "has_agenda": True,
        "has_content_slides": True,
        "has_closing_slide": True
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_powerpoint_tool("/path/to/company_presentation.pptx")
    
    # Verify results
    assert len(result) == 1
    content = result[0].page_content
    assert "Title Slide: Welcome to Our Company" in content
    assert "Agenda Slide:" in content
    assert "1. Company Overview" in content
    assert "Content Slide 1: Company Overview" in content
    assert "Closing Slide: Thank You" in content
    assert result[0].metadata["slide_count"] == 5
    assert result[0].metadata["has_title_slide"] is True
```
#### 3.3.4 Unstructured PDF Tool

**Function Information:**
- **Function Name**: `unstructured_pdf_tool`
- **Location**: `multi_agent_system.py:312`
- **Purpose**: Loads a PDF file using UnstructuredPDFLoader and returns the content as a list of Document objects

**Function Signature and Parameters:**
- **Input**: `file_path: str` - Path to the PDF file to be loaded
- **Return**: `list[Document]` - List of Document objects containing the PDF file content
- **Side Effects**: None - This function only performs file reading operations

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `UnstructuredPDFLoader` - External LangChain loader class for PDF files
  - `UnstructuredPDFLoader.load()` - External method that performs actual file reading
- **Dependencies to Use Directly**: 
  - `@tool` decorator - LangChain decorator for tool registration
  - `list` - Built-in Python list type annotation
  - `Document` - LangChain Document class (for type checking)
  - `str` - Built-in Python string type
- **Mock Configuration**: Mock UnstructuredPDFLoader to return predefined Document objects with metadata and content

**Test Cases:**

**Happy Path:**
- **Valid PDF File**: Load PDF file with standard content (text, images, tables)
- **Multiple Pages**: PDF file containing multiple pages with various content
- **Complex Formatting**: PDF file with formatting, layouts, and multimedia elements
- **Large PDF File**: PDF file with substantial content and many pages

**Edge Cases:**
- **Empty PDF File**: PDF file with no content or empty pages
- **Single Page Content**: PDF file with minimal content (single page)
- **Mixed Content Types**: PDF file containing various content types (text, images, tables, forms)
- **Special Characters**: PDF file with special characters, Unicode, and formatting

**Error Conditions:**
- **File Not Found**: Non-existent file path
- **Invalid File Format**: File that's not a valid PDF format
- **Corrupted File**: Damaged or corrupted PDF file
- **Permission Errors**: File with restricted access permissions
- **Empty File Path**: Empty or None file path

**Mock Configurations:**
```python
@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_with_mocked_loader(mock_loader):
    """Mock UnstructuredPDFLoader for consistent testing."""
    # Mock document with PDF content
    mock_document = Mock()
    mock_document.page_content = "Sample PDF content"
    mock_document.metadata = {
        "source": "test_file.pdf",
        "file_path": "/path/to/test_file.pdf"
    }
    
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and LangChain classes in the implementation:
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_core.documents import Document

@tool
def unstructured_pdf_tool(file_path: str) -> list[Document]:  # Direct use of @tool decorator and type annotations
    """
    Load a PDF file and return the content.
    """
    loader = UnstructuredPDFLoader(file_path)  # Direct instantiation of external class
    return loader.load()  # Direct method call on external object
```

**Assertion Specifications:**
- Verify function returns list of Document objects
- Verify Document objects contain expected content and metadata
- Verify loader is instantiated with correct file path
- Verify load() method is called on the loader instance
- Verify function handles various PDF content types correctly
- Verify error conditions are properly propagated
- Verify function maintains proper return type structure

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from langchain_core.documents import Document
from multi_agent_system import unstructured_pdf_tool

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_valid_file(mock_loader):
    """Test loading a valid PDF file."""
    # Setup mock document with PDF content
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Sample PDF content with text"
    mock_document.metadata = {
        "source": "test_file.pdf",
        "file_path": "/path/to/test_file.pdf",
        "file_type": "pdf"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/test_file.pdf")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 1
    assert isinstance(result[0], Mock)  # Mock object with Document spec
    assert result[0].page_content == "Sample PDF content with text"
    assert result[0].metadata["source"] == "test_file.pdf"
    
    # Verify loader was called correctly
    mock_loader.assert_called_once_with("/path/to/test_file.pdf")
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_multiple_pages(mock_loader):
    """Test loading PDF file with multiple pages."""
    # Setup mock documents for multiple pages
    mock_document1 = Mock(spec=Document)
    mock_document1.page_content = "Page 1: Introduction and Overview"
    mock_document1.metadata = {"page_number": 1, "source": "multi_page.pdf"}
    
    mock_document2 = Mock(spec=Document)
    mock_document2.page_content = "Page 2: Main Content and Analysis"
    mock_document2.metadata = {"page_number": 2, "source": "multi_page.pdf"}
    
    mock_document3 = Mock(spec=Document)
    mock_document3.page_content = "Page 3: Conclusions and References"
    mock_document3.metadata = {"page_number": 3, "source": "multi_page.pdf"}
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document1, mock_document2, mock_document3]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/multi_page.pdf")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 3
    assert result[0].page_content == "Page 1: Introduction and Overview"
    assert result[1].page_content == "Page 2: Main Content and Analysis"
    assert result[2].page_content == "Page 3: Conclusions and References"
    assert result[0].metadata["page_number"] == 1
    assert result[1].metadata["page_number"] == 2
    assert result[2].metadata["page_number"] == 3

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_empty_file(mock_loader):
    """Test loading PDF file with no content."""
    # Configure mock loader to return empty list
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/empty_file.pdf")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 0

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_complex_content(mock_loader):
    """Test loading PDF file with complex content and formatting."""
    complex_content = """
    Research Paper: Machine Learning Applications
    
    Abstract
    This paper presents a comprehensive analysis of machine learning applications
    in various domains including healthcare, finance, and transportation.
    
    Introduction
    Machine learning has revolutionized the way we approach complex problems.
    Recent advances in deep learning have enabled breakthroughs in computer vision,
    natural language processing, and robotics.
    
    Methodology
    We conducted a systematic review of 150 peer-reviewed papers published between
    2020 and 2024. Our analysis focused on three key areas:
    1. Healthcare applications
    2. Financial services
    3. Autonomous systems
    
    Results
    The analysis revealed significant improvements in accuracy and efficiency
    across all domains. Healthcare applications showed 25% improvement in
    diagnostic accuracy, while financial services demonstrated 40% reduction
    in fraud detection time.
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = complex_content
    mock_document.metadata = {
        "source": "research_paper.pdf",
        "file_path": "/path/to/research_paper.pdf",
        "has_tables": True,
        "has_images": True,
        "page_count": 15
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/research_paper.pdf")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == complex_content
    assert result[0].metadata["has_tables"] is True
    assert result[0].metadata["has_images"] is True
    assert result[0].metadata["page_count"] == 15

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_special_characters(mock_loader):
    """Test loading PDF file with special characters and Unicode."""
    special_content = "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~\nUnicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€\nMathematical symbols: Î± Î² Î³ Î´ Îµ âˆ‘ âˆ âˆ« âˆž"
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = special_content
    mock_document.metadata = {
        "source": "special_chars.pdf",
        "file_path": "/path/to/special_chars.pdf"
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/special_chars.pdf")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == special_content
    assert "ä½ å¥½ä¸–ç•Œ" in result[0].page_content
    assert "Î± Î² Î³" in result[0].page_content
    assert "âˆ‘ âˆ âˆ«" in result[0].page_content

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_file_not_found(mock_loader):
    """Test handling of non-existent file."""
    # Configure mock loader to raise FileNotFoundError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = FileNotFoundError("File not found")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError) as exc_info:
        unstructured_pdf_tool("/path/to/nonexistent.pdf")
    
    assert "File not found" in str(exc_info.value)
    
    # Verify loader was called
    mock_loader.assert_called_once_with("/path/to/nonexistent.pdf")

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_invalid_format(mock_loader):
    """Test handling of invalid PDF file format."""
    # Configure mock loader to raise format-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = ValueError("Invalid PDF format")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        unstructured_pdf_tool("/path/to/invalid.pdf")
    
    assert "Invalid PDF format" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_permission_error(mock_loader):
    """Test handling of permission errors."""
    # Configure mock loader to raise PermissionError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = PermissionError("Access denied")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(PermissionError) as exc_info:
        unstructured_pdf_tool("/path/to/restricted.pdf")
    
    assert "Access denied" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_corrupted_file(mock_loader):
    """Test handling of corrupted PDF file."""
    # Configure mock loader to raise corruption-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = Exception("File is corrupted")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        unstructured_pdf_tool("/path/to/corrupted.pdf")
    
    assert "File is corrupted" in str(exc_info.value)

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_large_file(mock_loader):
    """Test loading large PDF file with substantial content."""
    # Create large content
    large_content = "Large PDF content. " * 1000  # 1000 repetitions
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = large_content
    mock_document.metadata = {
        "source": "large_file.pdf",
        "file_path": "/path/to/large_file.pdf",
        "size": "large",
        "page_count": 100
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/large_file.pdf")
    
    # Verify results
    assert len(result) == 1
    assert len(result[0].page_content) > 1000  # Verify it's actually large
    assert result[0].metadata["size"] == "large"
    assert result[0].metadata["page_count"] == 100

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_single_page(mock_loader):
    """Test loading PDF file with minimal content (single page)."""
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Single page content"
    mock_document.metadata = {
        "source": "single_page.pdf",
        "file_path": "/path/to/single_page.pdf",
        "page_count": 1
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/single_page.pdf")
    
    # Verify results
    assert len(result) == 1
    assert result[0].page_content == "Single page content"
    assert result[0].metadata["page_count"] == 1

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_mixed_content_types(mock_loader):
    """Test loading PDF file with various content types."""
    mixed_content = """
    Page 1: Text Content
    This is a sample PDF document containing various types of content.
    
    Page 2: Table Content
    | Column 1 | Column 2 | Column 3 |
    |----------|----------|----------|
    | Data 1   | Data 2   | Data 3   |
    | Data 4   | Data 5   | Data 6   |
    
    Page 3: Form Content
    [Form Field: Name] ________________
    [Form Field: Email] _______________
    [Form Field: Phone] _______________
    
    Page 4: Image Content
    [Image: Company Logo]
    [Image: Chart showing quarterly results]
    [Image: Product diagram]
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = mixed_content
    mock_document.metadata = {
        "source": "mixed_content.pdf",
        "file_path": "/path/to/mixed_content.pdf",
        "has_text": True,
        "has_tables": True,
        "has_forms": True,
        "has_images": True
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/mixed_content.pdf")
    
    # Verify results
    assert len(result) == 1
    content = result[0].page_content
    assert "This is a sample PDF document" in content  # Text content
    assert "| Column 1 |" in content    # Table content
    assert "[Form Field: Name]" in content  # Form content
    assert "[Image: Company Logo]" in content  # Image content
    assert result[0].metadata["has_text"] is True
    assert result[0].metadata["has_tables"] is True
    assert result[0].metadata["has_forms"] is True
    assert result[0].metadata["has_images"] is True

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_academic_paper(mock_loader):
    """Test loading PDF file with academic paper structure."""
    academic_content = """
    Title: Advanced Machine Learning Techniques for Natural Language Processing
    
    Authors: John Doe, Jane Smith, Bob Johnson
    
    Abstract
    This paper presents novel approaches to natural language processing using
    transformer-based architectures. We demonstrate significant improvements
    in text classification, sentiment analysis, and machine translation tasks.
    
    Keywords: machine learning, NLP, transformers, deep learning
    
    1. Introduction
    Natural language processing has evolved significantly with the introduction
    of transformer architectures. The attention mechanism has revolutionized
    how we approach sequence-to-sequence tasks.
    
    2. Related Work
    Previous work in this area includes [1] which introduced the concept of
    attention mechanisms, and [2] which applied transformers to NLP tasks.
    
    3. Methodology
    Our approach combines multiple transformer layers with custom attention
    mechanisms. We train on a dataset of 1M sentences across 10 languages.
    
    4. Results
    Experimental results show 15% improvement in accuracy compared to
    baseline models. Our approach achieves state-of-the-art performance
    on three benchmark datasets.
    
    5. Conclusion
    We have demonstrated the effectiveness of our approach and identified
    areas for future research.
    
    References
    [1] Vaswani, A., et al. "Attention is all you need." NIPS 2017.
    [2] Devlin, J., et al. "BERT: Pre-training of Deep Bidirectional Transformers."
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = academic_content
    mock_document.metadata = {
        "source": "academic_paper.pdf",
        "file_path": "/path/to/academic_paper.pdf",
        "page_count": 8,
        "has_abstract": True,
        "has_references": True,
        "has_equations": False
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/academic_paper.pdf")
    
    # Verify results
    assert len(result) == 1
    content = result[0].page_content
    assert "Title: Advanced Machine Learning Techniques" in content
    assert "Abstract" in content
    assert "1. Introduction" in content
    assert "References" in content
    assert "[1] Vaswani, A., et al." in content
    assert result[0].metadata["has_abstract"] is True
    assert result[0].metadata["has_references"] is True
    assert result[0].metadata["page_count"] == 8

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_scanned_document(mock_loader):
    """Test loading PDF file that contains scanned document content."""
    scanned_content = """
    [OCR Text from Scanned Document]
    
    INVOICE
    
    Invoice Number: INV-2024-001
    Date: January 15, 2024
    Due Date: February 15, 2024
    
    Bill To:
    ABC Company
    123 Business Street
    City, State 12345
    
    Items:
    | Description           | Quantity | Unit Price | Total |
    |----------------------|----------|------------|-------|
    | Software License     | 5        | $500.00    | $2,500.00 |
    | Technical Support    | 1        | $1,000.00  | $1,000.00 |
    | Training Services    | 2        | $750.00    | $1,500.00 |
    
    Subtotal: $5,000.00
    Tax (8.5%): $425.00
    Total: $5,425.00
    
    Payment Terms: Net 30
    """
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = scanned_content
    mock_document.metadata = {
        "source": "scanned_invoice.pdf",
        "file_path": "/path/to/scanned_invoice.pdf",
        "is_scanned": True,
        "ocr_confidence": 0.95,
        "page_count": 1
    }
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/scanned_invoice.pdf")
    
    # Verify results
    assert len(result) == 1
    content = result[0].page_content
    assert "INVOICE" in content
    assert "Invoice Number: INV-2024-001" in content
    assert "ABC Company" in content
    assert "Total: $5,425.00" in content
    assert result[0].metadata["is_scanned"] is True
    assert result[0].metadata["ocr_confidence"] == 0.95

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_loader_instantiation(mock_loader):
    """Test that UnstructuredPDFLoader is instantiated correctly."""
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    unstructured_pdf_tool("/path/to/test.pdf")
    
    # Verify loader was instantiated with correct file path
    mock_loader.assert_called_once_with("/path/to/test.pdf")
    
    # Verify load method was called
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_return_type_structure(mock_loader):
    """Test that function returns correct type structure."""
    # Setup mock document
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Test content"
    mock_document.metadata = {"source": "test.pdf"}
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = unstructured_pdf_tool("/path/to/test.pdf")
    
    # Verify return type structure
    assert isinstance(result, list)
    assert all(isinstance(doc, Mock) for doc in result)  # Mock objects with Document spec
    assert all(hasattr(doc, 'page_content') for doc in result)
    assert all(hasattr(doc, 'metadata') for doc in result)

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_unstructured_pdf_tool_encrypted_pdf(mock_loader):
    """Test handling of encrypted PDF file."""
    # Configure mock loader to raise encryption-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = Exception("PDF is encrypted")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        unstructured_pdf_tool("/path/to/encrypted.pdf")
    
    assert "PDF is encrypted" in str(exc_info.value)
```
#### 3.3.5 Text File Tool

**Function Information:**
- **Function Name**: `text_file_tool`
- **Location**: `multi_agent_system.py:321`
- **Purpose**: Loads a text file using TextLoader and returns the content as a string, handling empty files by returning an empty string

**Function Signature and Parameters:**
- **Input**: `file_path: str` - Path to the text file to be loaded
- **Return**: `str` - String containing the text file content, or empty string if file is empty or has no content
- **Side Effects**: None - This function only performs file reading operations

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `TextLoader` - External LangChain loader class for text files
  - `TextLoader.load()` - External method that performs actual file reading
- **Dependencies to Use Directly**: 
  - `@tool` decorator - LangChain decorator for tool registration
  - `str` - Built-in Python string type annotation and operations
  - `list` - Built-in Python list operations (truthiness check, indexing)
  - `Document` - LangChain Document class (for accessing page_content)
- **Mock Configuration**: Mock TextLoader to return predefined Document objects with content, or empty list for empty files

**Test Cases:**

**Happy Path:**
- **Valid Text File**: Load text file with standard content
- **Large Text File**: Text file with substantial content
- **Multi-line Content**: Text file with multiple lines and paragraphs
- **Formatted Text**: Text file with various formatting and structure

**Edge Cases:**
- **Empty Text File**: Text file with no content
- **Single Line Content**: Text file with minimal content (single line)
- **Whitespace Only**: Text file containing only whitespace characters
- **Special Characters**: Text file with special characters, Unicode, and formatting

**Error Conditions:**
- **File Not Found**: Non-existent file path
- **Invalid File Format**: File that's not a valid text format
- **Corrupted File**: Damaged or corrupted text file
- **Permission Errors**: File with restricted access permissions
- **Empty File Path**: Empty or None file path

**Mock Configurations:**
```python
@patch('multi_agent_system.TextLoader')
def test_text_file_tool_with_mocked_loader(mock_loader):
    """Mock TextLoader for consistent testing."""
    # Mock document with text content
    mock_document = Mock()
    mock_document.page_content = "Sample text content"
    
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and LangChain classes in the implementation:
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document

@tool
def text_file_tool(file_path: str) -> str:  # Direct use of @tool decorator and type annotations
    """
    Load a text file and return the content.
    """
    loader = TextLoader(file_path)  # Direct instantiation of external class
    documents = loader.load()  # Direct method call on external object
    if documents:  # Direct list truthiness check
        return documents[0].page_content  # Direct list indexing and attribute access
    else:
        return ""  # Direct string return
```

**Assertion Specifications:**
- Verify function returns string content when file has content
- Verify function returns empty string when file is empty or has no content
- Verify loader is instantiated with correct file path
- Verify load() method is called on the loader instance
- Verify function handles various text content types correctly
- Verify error conditions are properly propagated
- Verify function maintains proper return type structure

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from langchain_core.documents import Document
from multi_agent_system import text_file_tool

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_valid_file(mock_loader):
    """Test loading a valid text file."""
    # Setup mock document with text content
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Sample text content with multiple lines\nThis is the second line\nAnd a third line"
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/test_file.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == "Sample text content with multiple lines\nThis is the second line\nAnd a third line"
    
    # Verify loader was called correctly
    mock_loader.assert_called_once_with("/path/to/test_file.txt")
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_empty_file(mock_loader):
    """Test loading empty text file."""
    # Configure mock loader to return empty list
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/empty_file.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == ""

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_single_line(mock_loader):
    """Test loading text file with single line content."""
    # Setup mock document with single line content
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Single line of text"
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/single_line.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == "Single line of text"

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_large_content(mock_loader):
    """Test loading text file with large content."""
    # Create large content
    large_content = "Large text content. " * 1000  # 1000 repetitions
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = large_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/large_file.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == large_content
    assert len(result) > 1000  # Verify it's actually large

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_multi_line_content(mock_loader):
    """Test loading text file with multi-line content."""
    multi_line_content = """This is the first line of the text file.
This is the second line with some content.
This is the third line with special characters: !@#$%^&*()
This is the fourth line with numbers: 12345
This is the fifth and final line."""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = multi_line_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/multi_line.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == multi_line_content
    assert result.count('\n') == 4  # Verify it has 4 newlines (5 lines total)

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_special_characters(mock_loader):
    """Test loading text file with special characters and Unicode."""
    special_content = "Special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?`~\nUnicode: ä½ å¥½ä¸–ç•Œ ðŸŒ Ã©mojis ðŸš€\nMathematical symbols: Î± Î² Î³ Î´ Îµ âˆ‘ âˆ âˆ« âˆž"
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = special_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/special_chars.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == special_content
    assert "ä½ å¥½ä¸–ç•Œ" in result
    assert "Î± Î² Î³" in result
    assert "âˆ‘ âˆ âˆ«" in result

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_whitespace_only(mock_loader):
    """Test loading text file with only whitespace characters."""
    whitespace_content = "   \n\t\n   \n"
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = whitespace_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/whitespace.txt")
    
    # Verify results
    assert isinstance(result, str)
    assert result == whitespace_content
    assert result.isspace()  # Verify it contains only whitespace

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_file_not_found(mock_loader):
    """Test handling of non-existent file."""
    # Configure mock loader to raise FileNotFoundError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = FileNotFoundError("File not found")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError) as exc_info:
        text_file_tool("/path/to/nonexistent.txt")
    
    assert "File not found" in str(exc_info.value)
    
    # Verify loader was called
    mock_loader.assert_called_once_with("/path/to/nonexistent.txt")

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_permission_error(mock_loader):
    """Test handling of permission errors."""
    # Configure mock loader to raise PermissionError
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = PermissionError("Access denied")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(PermissionError) as exc_info:
        text_file_tool("/path/to/restricted.txt")
    
    assert "Access denied" in str(exc_info.value)

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_corrupted_file(mock_loader):
    """Test handling of corrupted text file."""
    # Configure mock loader to raise corruption-related exception
    mock_loader_instance = Mock()
    mock_loader_instance.load.side_effect = Exception("File is corrupted")
    mock_loader.return_value = mock_loader_instance
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        text_file_tool("/path/to/corrupted.txt")
    
    assert "File is corrupted" in str(exc_info.value)

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_configuration_file(mock_loader):
    """Test loading text file with configuration content."""
    config_content = """# Configuration file
[Database]
host=localhost
port=5432
database=myapp
username=admin
password=secret123

[Server]
host=0.0.0.0
port=8080
debug=true
log_level=INFO

[Features]
enable_cache=true
cache_size=1000
timeout=30
"""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = config_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/config.ini")
    
    # Verify results
    assert isinstance(result, str)
    assert result == config_content
    assert "[Database]" in result
    assert "host=localhost" in result
    assert "[Server]" in result
    assert "port=8080" in result

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_log_file(mock_loader):
    """Test loading text file with log content."""
    log_content = """2024-01-15 10:30:15 INFO Application started
2024-01-15 10:30:16 DEBUG Loading configuration
2024-01-15 10:30:17 INFO Database connection established
2024-01-15 10:30:18 WARN High memory usage detected
2024-01-15 10:30:19 ERROR Failed to process request: timeout
2024-01-15 10:30:20 INFO Request processed successfully
2024-01-15 10:30:21 DEBUG Cleaning up resources"""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = log_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/application.log")
    
    # Verify results
    assert isinstance(result, str)
    assert result == log_content
    assert "INFO Application started" in result
    assert "ERROR Failed to process request" in result
    assert result.count("2024-01-15") == 7  # Verify all lines have the same date

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_code_file(mock_loader):
    """Test loading text file with code content."""
    code_content = """def fibonacci(n):
    \"\"\"Calculate the nth Fibonacci number.\"\"\"
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

def main():
    # Test the function
    for i in range(10):
        print(f"fibonacci({i}) = {fibonacci(i)}")

if __name__ == "__main__":
    main()
"""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = code_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/fibonacci.py")
    
    # Verify results
    assert isinstance(result, str)
    assert result == code_content
    assert "def fibonacci(n):" in result
    assert "return fibonacci(n-1) + fibonacci(n-2)" in result
    assert "if __name__ == \"__main__\":" in result

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_json_file(mock_loader):
    """Test loading text file with JSON content."""
    json_content = """{
    "name": "John Doe",
    "age": 30,
    "email": "john.doe@example.com",
    "address": {
        "street": "123 Main St",
        "city": "Anytown",
        "state": "CA",
        "zip": "12345"
    },
    "phone_numbers": [
        "+1-555-123-4567",
        "+1-555-987-6543"
    ],
    "active": true
}"""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = json_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/data.json")
    
    # Verify results
    assert isinstance(result, str)
    assert result == json_content
    assert "\"name\": \"John Doe\"" in result
    assert "\"age\": 30" in result
    assert "\"active\": true" in result

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_csv_file(mock_loader):
    """Test loading text file with CSV content."""
    csv_content = """Name,Age,Email,Department
John Doe,30,john.doe@example.com,Engineering
Jane Smith,25,jane.smith@example.com,Marketing
Bob Johnson,35,bob.johnson@example.com,Sales
Alice Brown,28,alice.brown@example.com,HR
Charlie Wilson,32,charlie.wilson@example.com,Finance"""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = csv_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/employees.csv")
    
    # Verify results
    assert isinstance(result, str)
    assert result == csv_content
    assert "Name,Age,Email,Department" in result
    assert "John Doe,30,john.doe@example.com,Engineering" in result
    assert result.count('\n') == 5  # Verify it has 5 newlines (6 lines total)

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_markdown_file(mock_loader):
    """Test loading text file with Markdown content."""
    markdown_content = """# Project Documentation"""
    
    mock_document = Mock(spec=Document)
    mock_document.page_content = markdown_content
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/README.md")
    
    # Verify results
    assert isinstance(result, str)
    assert result == markdown_content
    assert "# Project Documentation" in result
    assert "## Installation" in result
    assert "```bash" in result
    assert "**Feature 1**" in result

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_loader_instantiation(mock_loader):
    """Test that TextLoader is instantiated correctly."""
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = []
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    text_file_tool("/path/to/test.txt")
    
    # Verify loader was instantiated with correct file path
    mock_loader.assert_called_once_with("/path/to/test.txt")
    
    # Verify load method was called
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_return_type_structure(mock_loader):
    """Test that function returns correct type structure."""
    # Setup mock document
    mock_document = Mock(spec=Document)
    mock_document.page_content = "Test content"
    
    # Configure mock loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/test.txt")
    
    # Verify return type structure
    assert isinstance(result, str)
    assert result == "Test content"

@patch('multi_agent_system.TextLoader')
def test_text_file_tool_multiple_documents(mock_loader):
    """Test handling when loader returns multiple documents (edge case)."""
    # Setup multiple mock documents
    mock_document1 = Mock(spec=Document)
    mock_document1.page_content = "First document content"
    
    mock_document2 = Mock(spec=Document)
    mock_document2.page_content = "Second document content"
    
    # Configure mock loader to return multiple documents
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [mock_document1, mock_document2]
    mock_loader.return_value = mock_loader_instance
    
    # Execute function
    result = text_file_tool("/path/to/multiple_docs.txt")
    
    # Verify results (should use first document only)
    assert isinstance(result, str)
    assert result == "First document content"
    assert result != "Second document content"
```
#### 3.3.6 Get Browser MCP Tools

**Function Information:**
- **Function Name**: `get_browser_mcp_tools`
- **Location**: `multi_agent_system.py:334`
- **Purpose**: Establishes an async connection to a browser MCP server, initializes the session, and retrieves the list of available MCP tools

**Function Signature and Parameters:**
- **Input**: `mcp_url: str` - The URL of the browser MCP server to connect to
- **Return**: `list` - List of MCP tools available from the server
- **Side Effects**: Establishes network connections and may modify server state during session initialization

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `streamablehttp_client` - External async HTTP client for establishing connections
  - `ClientSession` - External MCP client session class
  - `ClientSession.initialize()` - External async method for session initialization
  - `load_mcp_tools()` - External async function for loading MCP tools from session
- **Dependencies to Use Directly**: 
  - `async def` - Built-in Python async function definition
  - `async with` - Built-in Python async context manager
  - `await` - Built-in Python async/await syntax
  - `str` - Built-in Python string type annotation
  - `list` - Built-in Python list type annotation
- **Mock Configuration**: Mock all external dependencies to simulate successful and failed MCP server interactions

**Test Cases:**

**Happy Path:**
- **Valid MCP Server**: Connect to valid MCP server and retrieve tools
- **Multiple Tools**: Server returns multiple MCP tools
- **Empty Tools List**: Server returns empty list of tools
- **Different URLs**: Connect to different MCP server URLs

**Edge Cases:**
- **Invalid URL Format**: Malformed or invalid MCP server URLs
- **Connection Timeout**: Server takes too long to respond
- **Empty Response**: Server returns empty or null response
- **Large Tool List**: Server returns large number of tools

**Error Conditions:**
- **Network Errors**: Connection failures, DNS resolution errors
- **Server Errors**: MCP server returns error responses
- **Authentication Errors**: Server requires authentication
- **Invalid Server**: Server doesn't support MCP protocol
- **Connection Refused**: Server is not running or not accessible

**Mock Configurations:**
```python
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_with_mocked_dependencies(mock_client, mock_session_class, mock_load_tools):
    """Mock all external dependencies for consistent testing."""
    # Mock streamablehttp_client context manager
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Mock load_mcp_tools
    mock_load_tools.return_value = [{"name": "test_tool", "description": "Test MCP tool"}]
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in Python async features in the implementation:
async def get_browser_mcp_tools(mcp_url: str) -> list:  # Direct use of async def and type annotations
    """Get the browser MCP tools from the given URL."""
    async with streamablehttp_client(mcp_url) as (read, write, _):  # Direct use of async with context manager
        async with ClientSession(read, write) as session:  # Direct use of nested async with
            await session.initialize()  # Direct use of await
            mcp_tools = await load_mcp_tools(session)  # Direct use of await
            return mcp_tools  # Direct return of list
```

**Assertion Specifications:**
- Verify function returns list of MCP tools when successful
- Verify all async context managers are properly entered and exited
- Verify session.initialize() is called exactly once
- Verify load_mcp_tools() is called with correct session parameter
- Verify function handles various error conditions gracefully
- Verify function maintains proper async behavior
- Verify function returns expected data types

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, AsyncMock, patch
from multi_agent_system import get_browser_mcp_tools

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_valid_server(mock_client, mock_session_class, mock_load_tools):
    """Test successful connection to valid MCP server."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Setup mock load_mcp_tools
    expected_tools = [
        {"name": "navigate", "description": "Navigate to a URL"},
        {"name": "click", "description": "Click on an element"},
        {"name": "type", "description": "Type text into an input field"}
    ]
    mock_load_tools.return_value = expected_tools
    
    # Execute function
    result = await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 3
    assert result == expected_tools
    
    # Verify context managers were used correctly
    mock_client.assert_called_once_with("http://localhost:3000/mcp")
    mock_session_class.assert_called_once_with(mock_read, mock_write)
    
    # Verify async methods were called
    mock_session.initialize.assert_called_once()
    mock_load_tools.assert_called_once_with(mock_session)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_empty_tools(mock_client, mock_session_class, mock_load_tools):
    """Test connection to MCP server that returns empty tools list."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Setup mock load_mcp_tools to return empty list
    mock_load_tools.return_value = []
    
    # Execute function
    result = await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 0
    assert result == []

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_large_tool_list(mock_client, mock_session_class, mock_load_tools):
    """Test connection to MCP server that returns large number of tools."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Setup mock load_mcp_tools to return large list
    large_tools_list = [{"name": f"tool_{i}", "description": f"Tool {i}"} for i in range(100)]
    mock_load_tools.return_value = large_tools_list
    
    # Execute function
    result = await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 100
    assert result == large_tools_list

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_different_urls(mock_client, mock_session_class, mock_load_tools):
    """Test connection to different MCP server URLs."""
    urls = [
        "http://localhost:3000/mcp",
        "http://127.0.0.1:8080/mcp",
        "http://mcp-server.example.com:5000/mcp",
        "https://secure-mcp.example.com/mcp"
    ]
    
    for url in urls:
        # Setup mock streamablehttp_client
        mock_read = Mock()
        mock_write = Mock()
        mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
        mock_client.return_value.__aexit__.return_value = None
        
        # Setup mock ClientSession
        mock_session = Mock()
        mock_session.initialize = AsyncMock()
        mock_session_class.return_value.__aenter__.return_value = mock_session
        mock_session_class.return_value.__aexit__.return_value = None
        
        # Setup mock load_mcp_tools
        mock_load_tools.return_value = [{"name": "test_tool", "description": "Test tool"}]
        
        # Execute function
        result = await get_browser_mcp_tools(url)
        
        # Verify results
        assert isinstance(result, list)
        assert len(result) == 1
        
        # Verify correct URL was used
        mock_client.assert_called_with(url)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_connection_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of connection errors."""
    # Setup mock streamablehttp_client to raise connection error
    mock_client.return_value.__aenter__.side_effect = ConnectionError("Connection refused")
    
    # Execute function and expect exception
    with pytest.raises(ConnectionError) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Connection refused" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_session_initialization_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of session initialization errors."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock(side_effect=Exception("Session initialization failed"))
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Session initialization failed" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_load_tools_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of load_mcp_tools errors."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Setup mock load_mcp_tools to raise error
    mock_load_tools.side_effect = Exception("Failed to load MCP tools")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Failed to load MCP tools" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_timeout_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of timeout errors."""
    # Setup mock streamablehttp_client to raise timeout error
    mock_client.return_value.__aenter__.side_effect = TimeoutError("Connection timeout")
    
    # Execute function and expect exception
    with pytest.raises(TimeoutError) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Connection timeout" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_authentication_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of authentication errors."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock(side_effect=Exception("Authentication required"))
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Authentication required" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_invalid_protocol(mock_client, mock_session_class, mock_load_tools):
    """Test handling of invalid MCP protocol."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock(side_effect=Exception("Invalid MCP protocol version"))
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Invalid MCP protocol version" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_context_manager_cleanup(mock_client, mock_session_class, mock_load_tools):
    """Test that context managers are properly cleaned up even on errors."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock(side_effect=Exception("Test error"))
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Execute function and expect exception
    with pytest.raises(Exception):
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    # Verify context managers were properly exited
    mock_client.return_value.__aexit__.assert_called_once()
    mock_session_class.return_value.__aexit__.assert_called_once()

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_complex_tool_structure(mock_client, mock_session_class, mock_load_tools):
    """Test handling of complex MCP tool structures."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Setup mock load_mcp_tools with complex tool structure
    complex_tools = [
        {
            "name": "navigate",
            "description": "Navigate to a URL",
            "parameters": {
                "url": {"type": "string", "description": "The URL to navigate to"}
            },
            "returns": {"type": "boolean", "description": "Success status"}
        },
        {
            "name": "click",
            "description": "Click on an element",
            "parameters": {
                "selector": {"type": "string", "description": "CSS selector"},
                "timeout": {"type": "number", "description": "Timeout in milliseconds"}
            },
            "returns": {"type": "boolean", "description": "Click success"}
        }
    ]
    mock_load_tools.return_value = complex_tools
    
    # Execute function
    result = await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    # Verify results
    assert isinstance(result, list)
    assert len(result) == 2
    assert result == complex_tools
    assert result[0]["name"] == "navigate"
    assert result[1]["name"] == "click"
    assert "parameters" in result[0]
    assert "returns" in result[1]

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_return_type_validation(mock_client, mock_session_class, mock_load_tools):
    """Test that function returns correct type structure."""
    # Setup mock streamablehttp_client
    mock_read = Mock()
    mock_write = Mock()
    mock_client.return_value.__aenter__.return_value = (mock_read, mock_write, None)
    mock_client.return_value.__aexit__.return_value = None
    
    # Setup mock ClientSession
    mock_session = Mock()
    mock_session.initialize = AsyncMock()
    mock_session_class.return_value.__aenter__.return_value = mock_session
    mock_session_class.return_value.__aexit__.return_value = None
    
    # Setup mock load_mcp_tools
    mock_load_tools.return_value = [{"name": "test_tool"}]
    
    # Execute function
    result = await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    # Verify return type structure
    assert isinstance(result, list)
    assert all(isinstance(tool, dict) for tool in result)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_dns_resolution_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of DNS resolution errors."""
    # Setup mock streamablehttp_client to raise DNS error
    mock_client.return_value.__aenter__.side_effect = Exception("DNS resolution failed")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("http://nonexistent-server.com/mcp")
    
    assert "DNS resolution failed" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_server_unavailable(mock_client, mock_session_class, mock_load_tools):
    """Test handling of server unavailable errors."""
    # Setup mock streamablehttp_client to raise server unavailable error
    mock_client.return_value.__aenter__.side_effect = Exception("Server unavailable")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("http://localhost:3000/mcp")
    
    assert "Server unavailable" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_ssl_error(mock_client, mock_session_class, mock_load_tools):
    """Test handling of SSL/TLS errors."""
    # Setup mock streamablehttp_client to raise SSL error
    mock_client.return_value.__aenter__.side_effect = Exception("SSL certificate verification failed")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        await get_browser_mcp_tools("https://localhost:3000/mcp")
    
    assert "SSL certificate verification failed" in str(exc_info.value)

@pytest.mark.asyncio
@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_get_browser_mcp_tools_invalid_url_format(mock_client, mock_session_class, mock_load_tools):
    """Test handling of invalid URL format."""
    # Setup mock streamablehttp_client to raise URL format error
    mock_client.return_value.__aenter__.side_effect = ValueError("Invalid URL format")
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        await get_browser_mcp_tools("invalid-url-format")
    
    assert "Invalid URL format" in str(exc_info.value)
```
### 3.4 Expert Tools
#### 3.4.1 Unit Converter

**Function Information:**
- **Function Name**: `unit_converter`
- **Location**: `multi_agent_system.py:415`
- **Purpose**: Converts quantities between different units using the pint library and returns the result as a string

**Function Signature and Parameters:**
- **Input**: `quantity: str` - A string representing the quantity to convert (e.g., '10 meters', '5 kg', '32 fahrenheit')
- **Input**: `to_unit: str` - The target unit to convert to (e.g., 'ft', 'lbs', 'celsius')
- **Return**: `str` - The converted value as a string
- **Side Effects**: Logs conversion operations using the logger

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `pint.UnitRegistry()` - External pint library for unit conversions
  - `logger.info()` - Logging dependency for consistent testing
- **Dependencies to Use Directly**: 
  - `@tool` decorator - LangChain decorator for tool registration
  - `str` - Built-in Python string type annotation and operations
  - `f-string` formatting - Built-in Python string formatting
- **Mock Configuration**: Mock pint.UnitRegistry to return predefined conversion results and mock logger for consistent testing

**Test Cases:**

**Happy Path:**
- **Length Conversions**: Convert between different length units (meters, feet, inches, etc.)
- **Weight Conversions**: Convert between different weight units (kg, lbs, grams, etc.)
- **Temperature Conversions**: Convert between different temperature units (celsius, fahrenheit, kelvin)
- **Volume Conversions**: Convert between different volume units (liters, gallons, cubic meters, etc.)

**Edge Cases:**
- **Zero Values**: Convert zero quantities
- **Large Numbers**: Convert very large quantities
- **Small Numbers**: Convert very small quantities (scientific notation)
- **Decimal Values**: Convert quantities with decimal places
- **Negative Values**: Convert negative quantities

**Error Conditions:**
- **Invalid Quantity Format**: Malformed quantity strings
- **Incompatible Units**: Units that cannot be converted between
- **Unknown Units**: Units not recognized by pint
- **Empty Inputs**: Empty or None inputs
- **Invalid Unit Syntax**: Malformed unit specifications

**Mock Configurations:**
```python
@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_with_mocked_dependencies(mock_logger, mock_pint):
    """Mock pint and logger for consistent testing."""
    # Mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 32.8  # Mock conversion result
    mock_ureg.return_value = mock_quantity
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and external libraries in the implementation:
@tool
def unit_converter(quantity: str, to_unit: str) -> str:  # Direct use of @tool decorator and type annotations
    """
    Convert a quantity to a different unit.
    Args:
        quantity: A string like '10 meters', '5 kg', '32 fahrenheit'
        to_unit: The unit to convert to, e.g. 'ft', 'lbs', 'celsius'
    Returns:
        The converted value as a string.
    """
    logger.info(f"Unit converter converting {quantity} to {to_unit}")  # Direct f-string formatting and logging
    ureg = pint.UnitRegistry()  # Direct instantiation of external library
    q = ureg(quantity)  # Direct method call on external object
    result = q.to(to_unit)  # Direct method call on external object
    logger.info(f"Unit converter result: {result}")  # Direct f-string formatting and logging
    return str(result)  # Direct string conversion
```

**Assertion Specifications:**
- Verify function returns string representation of converted value
- Verify pint.UnitRegistry is instantiated correctly
- Verify quantity is parsed correctly by pint
- Verify conversion is performed with correct target unit
- Verify logging calls are made with correct messages
- Verify function handles various unit types correctly
- Verify error conditions are properly propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import unit_converter

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_length_conversion(mock_logger, mock_pint):
    """Test length unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 32.80839895013123  # 10 meters in feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("10 meters", "ft")
    
    # Verify results
    assert result == "32.80839895013123"
    
    # Verify pint was used correctly
    mock_pint.UnitRegistry.assert_called_once()
    mock_ureg.assert_called_once_with("10 meters")
    mock_quantity.to.assert_called_once_with("ft")
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Unit converter converting 10 meters to ft")
    mock_logger.info.assert_any_call("Unit converter result: 32.80839895013123")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_weight_conversion(mock_logger, mock_pint):
    """Test weight unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 11.023113109243878  # 5 kg in lbs
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("5 kg", "lbs")
    
    # Verify results
    assert result == "11.023113109243878"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("5 kg")
    mock_quantity.to.assert_called_once_with("lbs")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_temperature_conversion(mock_logger, mock_pint):
    """Test temperature unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 0.0  # 32 fahrenheit in celsius
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("32 fahrenheit", "celsius")
    
    # Verify results
    assert result == "0.0"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("32 fahrenheit")
    mock_quantity.to.assert_called_once_with("celsius")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_volume_conversion(mock_logger, mock_pint):
    """Test volume unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 0.2641720523581484  # 1 liter in gallons
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 liter", "gallon")
    
    # Verify results
    assert result == "0.2641720523581484"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("1 liter")
    mock_quantity.to.assert_called_once_with("gallon")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_zero_value(mock_logger, mock_pint):
    """Test conversion of zero values."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 0.0  # 0 meters in feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("0 meters", "ft")
    
    # Verify results
    assert result == "0.0"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("0 meters")
    mock_quantity.to.assert_called_once_with("ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_large_number(mock_logger, mock_pint):
    """Test conversion of large numbers."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 328083.9895013123  # 100000 meters in feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("100000 meters", "ft")
    
    # Verify results
    assert result == "328083.9895013123"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("100000 meters")
    mock_quantity.to.assert_called_once_with("ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_small_number(mock_logger, mock_pint):
    """Test conversion of very small numbers."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 3.280839895013123e-06  # 0.001 millimeters in feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("0.001 millimeters", "ft")
    
    # Verify results
    assert result == "3.280839895013123e-06"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("0.001 millimeters")
    mock_quantity.to.assert_called_once_with("ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_decimal_value(mock_logger, mock_pint):
    """Test conversion of decimal values."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 3.280839895013123  # 1.0 meters in feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1.0 meters", "ft")
    
    # Verify results
    assert result == "3.280839895013123"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("1.0 meters")
    mock_quantity.to.assert_called_once_with("ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_negative_value(mock_logger, mock_pint):
    """Test conversion of negative values."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = -3.280839895013123  # -1 meters in feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("-1 meters", "ft")
    
    # Verify results
    assert result == "-3.280839895013123"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("-1 meters")
    mock_quantity.to.assert_called_once_with("ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_invalid_quantity_format(mock_logger, mock_pint):
    """Test handling of invalid quantity format."""
    # Setup mock UnitRegistry to raise error
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    mock_ureg.side_effect = ValueError("Invalid quantity format")
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        unit_converter("invalid quantity", "ft")
    
    assert "Invalid quantity format" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Unit converter converting invalid quantity to ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_incompatible_units(mock_logger, mock_pint):
    """Test handling of incompatible units."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object to raise error on conversion
    mock_quantity = Mock()
    mock_quantity.to.side_effect = Exception("Cannot convert from meters to kilograms")
    mock_ureg.return_value = mock_quantity
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        unit_converter("10 meters", "kg")
    
    assert "Cannot convert from meters to kilograms" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Unit converter converting 10 meters to kg")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_unknown_unit(mock_logger, mock_pint):
    """Test handling of unknown units."""
    # Setup mock UnitRegistry to raise error for unknown unit
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    mock_ureg.side_effect = Exception("Unknown unit: invalid_unit")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        unit_converter("10 invalid_unit", "ft")
    
    assert "Unknown unit: invalid_unit" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Unit converter converting 10 invalid_unit to ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_empty_inputs(mock_logger, mock_pint):
    """Test handling of empty inputs."""
    # Setup mock UnitRegistry to raise error for empty input
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    mock_ureg.side_effect = ValueError("Empty quantity")
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        unit_converter("", "ft")
    
    assert "Empty quantity" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Unit converter converting  to ft")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_area_conversion(mock_logger, mock_pint):
    """Test area unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 10.763910416709722  # 1 square meter in square feet
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 square meter", "square feet")
    
    # Verify results
    assert result == "10.763910416709722"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("1 square meter")
    mock_quantity.to.assert_called_once_with("square feet")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_speed_conversion(mock_logger, mock_pint):
    """Test speed unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 2.2369362920544025  # 1 m/s in mph
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 m/s", "mph")
    
    # Verify results
    assert result == "2.2369362920544025"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("1 m/s")
    mock_quantity.to.assert_called_once_with("mph")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_pressure_conversion(mock_logger, mock_pint):
    """Test pressure unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 14.503773773375  # 1 bar in psi
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 bar", "psi")
    
    # Verify results
    assert result == "14.503773773375"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("1 bar")
    mock_quantity.to.assert_called_once_with("psi")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_energy_conversion(mock_logger, mock_pint):
    """Test energy unit conversion."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 0.0002777777777777778  # 1 joule in watt hours
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 joule", "watt hour")
    
    # Verify results
    assert result == "0.0002777777777777778"
    
    # Verify pint was used correctly
    mock_ureg.assert_called_once_with("1 joule")
    mock_quantity.to.assert_called_once_with("watt hour")

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_return_type_validation(mock_logger, mock_pint):
    """Test that function returns correct type structure."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 3.280839895013123
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 meters", "ft")
    
    # Verify return type structure
    assert isinstance(result, str)
    assert result == "3.280839895013123"

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_logging_verification(mock_logger, mock_pint):
    """Test that logging is called correctly."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 3.280839895013123
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    result = unit_converter("1 meters", "ft")
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Unit converter converting 1 meters to ft")
    mock_logger.info.assert_any_call("Unit converter result: 3.280839895013123")
    assert mock_logger.info.call_count == 2

@patch('multi_agent_system.pint')
@patch('multi_agent_system.logger')
def test_unit_converter_pint_instantiation(mock_logger, mock_pint):
    """Test that pint.UnitRegistry is instantiated correctly."""
    # Setup mock UnitRegistry
    mock_ureg = Mock()
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Setup mock quantity object
    mock_quantity = Mock()
    mock_quantity.to.return_value = 3.280839895013123
    mock_ureg.return_value = mock_quantity
    
    # Execute function
    unit_converter("1 meters", "ft")
    
    # Verify pint.UnitRegistry was called
    mock_pint.UnitRegistry.assert_called_once()
```
#### 3.4.2 Calculator

**Function Information:**
- **Function Name**: `calculator`
- **Location**: `multi_agent_system.py:433`
- **Purpose**: Evaluates basic mathematical expressions using Python's eval() function with restricted access to math module functions and returns the result as a string

**Function Signature and Parameters:**
- **Input**: `expression: str` - A string containing a mathematical expression to evaluate
- **Return**: `str` - The result of the mathematical expression as a string
- **Side Effects**: Logs expression evaluation and results using the logger

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `math` module - Standard library module for mathematical functions
- **Dependencies to Use Directly**: 
  - `@tool` decorator - LangChain decorator for tool registration
  - `str` - Built-in Python string type annotation and operations
  - `f-string` formatting - Built-in Python string formatting
  - `eval()` - Built-in Python function for expression evaluation
  - `dict` comprehension - Built-in Python dictionary operations
  - `import` statement - Built-in Python import mechanism
- **Mock Configuration**: Mock logger for consistent testing, optionally mock math module for specific test scenarios

**Test Cases:**

**Happy Path:**
- **Basic Arithmetic**: Addition, subtraction, multiplication, division
- **Exponentiation**: Power operations with ** operator
- **Parentheses**: Expressions with proper parentheses grouping
- **Math Functions**: Trigonometric, logarithmic, and other math module functions
- **Complex Expressions**: Combinations of multiple operations

**Edge Cases:**
- **Zero Values**: Expressions involving zero
- **Large Numbers**: Very large numerical values
- **Small Numbers**: Very small decimal values
- **Negative Numbers**: Expressions with negative values
- **Floating Point**: Decimal arithmetic and precision
- **Scientific Notation**: Numbers in scientific notation format

**Error Conditions:**
- **Invalid Syntax**: Malformed mathematical expressions
- **Division by Zero**: Mathematical division by zero errors
- **Undefined Operations**: Invalid mathematical operations
- **Empty Inputs**: Empty or None expressions
- **Security Violations**: Attempts to access restricted functions
- **Overflow**: Mathematical overflow conditions

**Mock Configurations:**
```python
@patch('multi_agent_system.logger')
def test_calculator_with_mocked_logger(mock_logger):
    """Mock logger for consistent testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Test implementation
    result = calculator("2 + 2")
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 2 + 2")
    mock_logger.info.assert_any_call("Calculator result: 4")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and standard library in the implementation:
@tool
def calculator(expression: str) -> str:  # Direct use of @tool decorator and type annotations
    """Evaluate a basic math expression. Supports +, -, *, /, **, and parentheses.

    Args:
        expression (str): The math expression to evaluate

    Returns:
        str: The result of the expression
    """
    logger.info(f"Calculator evaluating expression: {expression}")  # Direct f-string formatting and logging
    import math  # Direct import of standard library module
    allowed_names = {
        k: v for k, v in math.__dict__.items() if not k.startswith("__")  # Direct dict comprehension and string operations
    }
    result = eval(expression, {"__builtins__": None}, allowed_names)  # Direct eval() function call
    logger.info(f"Calculator result: {result}")  # Direct f-string formatting and logging
    return str(result)  # Direct string conversion
```

**Assertion Specifications:**
- Verify function returns string representation of calculated result
- Verify mathematical expressions are evaluated correctly
- Verify logging calls are made with correct messages
- Verify math module functions are accessible in allowed_names
- Verify restricted access prevents security violations
- Verify error conditions are properly handled and propagated
- Verify floating point precision is maintained appropriately

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import calculator

@patch('multi_agent_system.logger')
def test_calculator_basic_addition(mock_logger):
    """Test basic addition operation."""
    # Execute function
    result = calculator("2 + 3")
    
    # Verify results
    assert result == "5"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 2 + 3")
    mock_logger.info.assert_any_call("Calculator result: 5")

@patch('multi_agent_system.logger')
def test_calculator_basic_subtraction(mock_logger):
    """Test basic subtraction operation."""
    # Execute function
    result = calculator("10 - 4")
    
    # Verify results
    assert result == "6"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 10 - 4")
    mock_logger.info.assert_any_call("Calculator result: 6")

@patch('multi_agent_system.logger')
def test_calculator_basic_multiplication(mock_logger):
    """Test basic multiplication operation."""
    # Execute function
    result = calculator("6 * 7")
    
    # Verify results
    assert result == "42"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 6 * 7")
    mock_logger.info.assert_any_call("Calculator result: 42")

@patch('multi_agent_system.logger')
def test_calculator_basic_division(mock_logger):
    """Test basic division operation."""
    # Execute function
    result = calculator("15 / 3")
    
    # Verify results
    assert result == "5.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 15 / 3")
    mock_logger.info.assert_any_call("Calculator result: 5.0")

@patch('multi_agent_system.logger')
def test_calculator_exponentiation(mock_logger):
    """Test exponentiation operation."""
    # Execute function
    result = calculator("2 ** 8")
    
    # Verify results
    assert result == "256"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 2 ** 8")
    mock_logger.info.assert_any_call("Calculator result: 256")

@patch('multi_agent_system.logger')
def test_calculator_parentheses(mock_logger):
    """Test expressions with parentheses."""
    # Execute function
    result = calculator("(3 + 4) * 2")
    
    # Verify results
    assert result == "14"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: (3 + 4) * 2")
    mock_logger.info.assert_any_call("Calculator result: 14")

@patch('multi_agent_system.logger')
def test_calculator_complex_expression(mock_logger):
    """Test complex mathematical expression."""
    # Execute function
    result = calculator("(10 + 5) * 2 - 8 / 4")
    
    # Verify results
    assert result == "28.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: (10 + 5) * 2 - 8 / 4")
    mock_logger.info.assert_any_call("Calculator result: 28.0")

@patch('multi_agent_system.logger')
def test_calculator_math_functions_sin(mock_logger):
    """Test trigonometric function (sin)."""
    # Execute function
    result = calculator("sin(3.14159)")
    
    # Verify results (sin(Ï€) â‰ˆ 0)
    assert abs(float(result) - 0.0) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: sin(3.14159)")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_math_functions_cos(mock_logger):
    """Test trigonometric function (cos)."""
    # Execute function
    result = calculator("cos(0)")
    
    # Verify results (cos(0) = 1)
    assert result == "1.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: cos(0)")
    mock_logger.info.assert_any_call("Calculator result: 1.0")

@patch('multi_agent_system.logger')
def test_calculator_math_functions_sqrt(mock_logger):
    """Test square root function."""
    # Execute function
    result = calculator("sqrt(16)")
    
    # Verify results
    assert result == "4.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: sqrt(16)")
    mock_logger.info.assert_any_call("Calculator result: 4.0")

@patch('multi_agent_system.logger')
def test_calculator_math_functions_log(mock_logger):
    """Test logarithmic function."""
    # Execute function
    result = calculator("log(100, 10)")
    
    # Verify results (log base 10 of 100 = 2)
    assert result == "2.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: log(100, 10)")
    mock_logger.info.assert_any_call("Calculator result: 2.0")

@patch('multi_agent_system.logger')
def test_calculator_math_functions_pi(mock_logger):
    """Test mathematical constant (pi)."""
    # Execute function
    result = calculator("pi")
    
    # Verify results (Ï€ â‰ˆ 3.14159)
    assert abs(float(result) - 3.14159) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: pi")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_math_functions_e(mock_logger):
    """Test mathematical constant (e)."""
    # Execute function
    result = calculator("e")
    
    # Verify results (e â‰ˆ 2.71828)
    assert abs(float(result) - 2.71828) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: e")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_zero_value(mock_logger):
    """Test expressions involving zero."""
    # Execute function
    result = calculator("0 + 5")
    
    # Verify results
    assert result == "5"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 0 + 5")
    mock_logger.info.assert_any_call("Calculator result: 5")

@patch('multi_agent_system.logger')
def test_calculator_large_number(mock_logger):
    """Test expressions with large numbers."""
    # Execute function
    result = calculator("1000000 * 1000000")
    
    # Verify results
    assert result == "1000000000000"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 1000000 * 1000000")
    mock_logger.info.assert_any_call("Calculator result: 1000000000000")

@patch('multi_agent_system.logger')
def test_calculator_small_decimal(mock_logger):
    """Test expressions with small decimal values."""
    # Execute function
    result = calculator("0.001 + 0.002")
    
    # Verify results
    assert result == "0.003"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 0.001 + 0.002")
    mock_logger.info.assert_any_call("Calculator result: 0.003")

@patch('multi_agent_system.logger')
def test_calculator_negative_number(mock_logger):
    """Test expressions with negative numbers."""
    # Execute function
    result = calculator("-5 + 10")
    
    # Verify results
    assert result == "5"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: -5 + 10")
    mock_logger.info.assert_any_call("Calculator result: 5")

@patch('multi_agent_system.logger')
def test_calculator_floating_point_precision(mock_logger):
    """Test floating point precision."""
    # Execute function
    result = calculator("0.1 + 0.2")
    
    # Verify results (floating point precision issue)
    assert abs(float(result) - 0.3) < 0.0001
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 0.1 + 0.2")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_scientific_notation(mock_logger):
    """Test scientific notation."""
    # Execute function
    result = calculator("1e6 + 2e6")
    
    # Verify results
    assert result == "3000000.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 1e6 + 2e6")
    mock_logger.info.assert_any_call("Calculator result: 3000000.0")

@patch('multi_agent_system.logger')
def test_calculator_invalid_syntax(mock_logger):
    """Test handling of invalid syntax."""
    # Execute function and expect exception
    with pytest.raises(SyntaxError) as exc_info:
        calculator("2 + + 3")
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Calculator evaluating expression: 2 + + 3")

@patch('multi_agent_system.logger')
def test_calculator_division_by_zero(mock_logger):
    """Test handling of division by zero."""
    # Execute function and expect exception
    with pytest.raises(ZeroDivisionError) as exc_info:
        calculator("10 / 0")
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Calculator evaluating expression: 10 / 0")

@patch('multi_agent_system.logger')
def test_calculator_undefined_operation(mock_logger):
    """Test handling of undefined mathematical operations."""
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        calculator("sqrt(-1)")
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Calculator evaluating expression: sqrt(-1)")

@patch('multi_agent_system.logger')
def test_calculator_empty_input(mock_logger):
    """Test handling of empty input."""
    # Execute function and expect exception
    with pytest.raises(SyntaxError) as exc_info:
        calculator("")
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Calculator evaluating expression: ")

@patch('multi_agent_system.logger')
def test_calculator_security_violation_builtins(mock_logger):
    """Test that builtins are properly restricted."""
    # Execute function and expect exception
    with pytest.raises(NameError) as exc_info:
        calculator("open('file.txt')")
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Calculator evaluating expression: open('file.txt')")

@patch('multi_agent_system.logger')
def test_calculator_security_violation_import(mock_logger):
    """Test that import statements are properly restricted."""
    # Execute function and expect exception
    with pytest.raises(NameError) as exc_info:
        calculator("__import__('os')")
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Calculator evaluating expression: __import__('os')")

@patch('multi_agent_system.logger')
def test_calculator_math_module_access(mock_logger):
    """Test that math module functions are accessible."""
    # Execute function
    result = calculator("factorial(5)")
    
    # Verify results
    assert result == "120"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: factorial(5)")
    mock_logger.info.assert_any_call("Calculator result: 120")

@patch('multi_agent_system.logger')
def test_calculator_math_module_constant(mock_logger):
    """Test that math module constants are accessible."""
    # Execute function
    result = calculator("tau")
    
    # Verify results (Ï„ = 2Ï€ â‰ˆ 6.28318)
    assert abs(float(result) - 6.28318) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: tau")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_math_module_floor(mock_logger):
    """Test floor function from math module."""
    # Execute function
    result = calculator("floor(3.7)")
    
    # Verify results
    assert result == "3"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: floor(3.7)")
    mock_logger.info.assert_any_call("Calculator result: 3")

@patch('multi_agent_system.logger')
def test_calculator_math_module_ceil(mock_logger):
    """Test ceiling function from math module."""
    # Execute function
    result = calculator("ceil(3.2)")
    
    # Verify results
    assert result == "4"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: ceil(3.2)")
    mock_logger.info.assert_any_call("Calculator result: 4")

@patch('multi_agent_system.logger')
def test_calculator_math_module_abs(mock_logger):
    """Test absolute value function from math module."""
    # Execute function
    result = calculator("abs(-15)")
    
    # Verify results
    assert result == "15"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: abs(-15)")
    mock_logger.info.assert_any_call("Calculator result: 15")

@patch('multi_agent_system.logger')
def test_calculator_math_module_pow(mock_logger):
    """Test power function from math module."""
    # Execute function
    result = calculator("pow(2, 10)")
    
    # Verify results
    assert result == "1024.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: pow(2, 10)")
    mock_logger.info.assert_any_call("Calculator result: 1024.0")

@patch('multi_agent_system.logger')
def test_calculator_math_module_exp(mock_logger):
    """Test exponential function from math module."""
    # Execute function
    result = calculator("exp(1)")
    
    # Verify results (e^1 â‰ˆ 2.71828)
    assert abs(float(result) - 2.71828) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: exp(1)")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_math_module_degrees(mock_logger):
    """Test degrees conversion function from math module."""
    # Execute function
    result = calculator("degrees(pi)")
    
    # Verify results (Ï€ radians = 180 degrees)
    assert result == "180.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: degrees(pi)")
    mock_logger.info.assert_any_call("Calculator result: 180.0")

@patch('multi_agent_system.logger')
def test_calculator_math_module_radians(mock_logger):
    """Test radians conversion function from math module."""
    # Execute function
    result = calculator("radians(180)")
    
    # Verify results (180 degrees = Ï€ radians)
    assert abs(float(result) - 3.14159) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: radians(180)")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_return_type_validation(mock_logger):
    """Test that function returns correct type structure."""
    # Execute function
    result = calculator("2 + 2")
    
    # Verify return type structure
    assert isinstance(result, str)
    assert result == "4"

@patch('multi_agent_system.logger')
def test_calculator_logging_verification(mock_logger):
    """Test that logging is called correctly."""
    # Execute function
    result = calculator("3 * 4")
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 3 * 4")
    mock_logger.info.assert_any_call("Calculator result: 12")
    assert mock_logger.info.call_count == 2

@patch('multi_agent_system.logger')
def test_calculator_allowed_names_verification(mock_logger):
    """Test that allowed_names contains expected math functions."""
    # Execute function to trigger import and allowed_names creation
    result = calculator("pi")
    
    # Verify that math functions are accessible
    assert abs(float(result) - 3.14159) < 0.01
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: pi")
    mock_logger.info.assert_any_call(f"Calculator result: {result}")

@patch('multi_agent_system.logger')
def test_calculator_math_module_import_verification(mock_logger):
    """Test that math module is imported correctly."""
    # Execute function
    result = calculator("sqrt(25)")
    
    # Verify results
    assert result == "5.0"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: sqrt(25)")
    mock_logger.info.assert_any_call("Calculator result: 5.0")

@patch('multi_agent_system.logger')
def test_calculator_eval_function_verification(mock_logger):
    """Test that eval function is used correctly."""
    # Execute function
    result = calculator("2 ** 3 + 1")
    
    # Verify results
    assert result == "9"
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Calculator evaluating expression: 2 ** 3 + 1")
    mock_logger.info.assert_any_call("Calculator result: 9")
```
### 3.5 Agent Node Factories
#### 3.5.1 Researcher LLM Node Factory

**Function Information:**
- **Function Name**: `create_researcher_llm_node`
- **Location**: `multi_agent_system.py:372`
- **Purpose**: Creates a researcher LLM node function that processes ResearcherState and returns updated state with LLM responses and validation

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompt and output schema for the researcher agent
- **Input**: `llm_researcher: ChatOpenAI` - LangChain ChatOpenAI instance for LLM interactions
- **Return**: `Callable[[ResearcherState], ResearcherState]` - A function that takes and returns ResearcherState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `llm_researcher.invoke()` - LangChain LLM invocation method
  - `validate_output_matches_json_schema()` - Internal validation function
  - `SystemMessage` - LangChain message class
  - `AIMessage` - LangChain message class
  - `json.dumps()` - JSON serialization
- **Dependencies to Use Directly**: 
  - `Callable` - Built-in Python type annotation
  - `ResearcherState` - Internal TypedDict type
  - `AgentConfig` - Internal configuration class
  - `ChatOpenAI` - LangChain class (type annotation only)
  - Function definition and closure creation - Built-in Python features
- **Mock Configuration**: Mock LLM responses, validation function, and message classes for controlled testing

**Test Cases:**

**Happy Path:**
- **Valid JSON Response**: LLM returns properly formatted JSON that matches schema
- **Valid Non-JSON Response**: LLM returns non-JSON response that doesn't match schema
- **Multiple Messages**: State contains multiple messages for context
- **Empty Messages**: State with empty message list
- **Complex State**: State with various combinations of fields

**Edge Cases:**
- **Large Response**: Very large LLM responses
- **Empty Response**: Empty or minimal LLM responses
- **Special Characters**: Responses with special characters and Unicode
- **Nested JSON**: Complex nested JSON structures in responses
- **Missing Fields**: JSON responses missing expected fields

**Error Conditions:**
- **LLM Invocation Failure**: Network errors, API failures, timeout errors
- **Invalid JSON**: Malformed JSON responses from LLM
- **Schema Mismatch**: JSON that doesn't match expected schema
- **Missing Configuration**: Invalid or missing AgentConfig
- **Invalid State**: Corrupted or invalid ResearcherState
- **Validation Function Failure**: Errors in validate_output_matches_json_schema

**Mock Configurations:**
```python
@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_factory_with_mocked_dependencies(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Mock all external dependencies for controlled testing."""
    # Mock AgentConfig
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    # Mock ChatOpenAI
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "test result"}
    
    # Mock validation function
    mock_validate.return_value = True
    
    # Mock message classes
    mock_system_message.return_value = Mock()
    mock_ai_message.return_value = Mock()
    
    # Mock JSON serialization
    mock_json.dumps.return_value = '{"result": "test result"}'
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and type annotations in the implementation:
def create_researcher_llm_node(config: AgentConfig, llm_researcher: ChatOpenAI) -> Callable[[ResearcherState], ResearcherState]:  # Direct type annotations
    """Create a researcher LLM node function with the given prompt and LLM.

    Args:
        config (AgentConfig): The configuration for the researcher agent
        llm_researcher (ChatOpenAI): The LLM for the researcher agent

    Returns:
        Callable[[ResearcherState], ResearcherState]: The researcher LLM node function
    """
    def researcher_llm_node(state: ResearcherState) -> ResearcherState:  # Direct function definition and type annotations
        """Researcher LLM node function with the given prompt and LLM."""
        sys_prompt = [SystemMessage(content=config.system_prompt)]  # Direct list creation and attribute access
        response = llm_researcher.invoke(sys_prompt + state["messages"])  # Direct method call and list concatenation
        if validate_output_matches_json_schema(response, config.output_schema.keys()):  # Direct function call and method chaining
            return {"messages": [AIMessage(content=json.dumps(response))], "result": response["result"]}  # Direct dict creation and function calls
        else:
            return {"messages": [response]}  # Direct dict creation and list wrapping
    return researcher_llm_node  # Direct function return
```

**Assertion Specifications:**
- Verify function returns a callable with correct signature
- Verify returned function processes ResearcherState correctly
- Verify LLM is invoked with correct system prompt and messages
- Verify JSON schema validation is called with correct parameters
- Verify appropriate response handling based on validation result
- Verify state updates include correct message and result fields
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_researcher_llm_node, ResearcherState
from langchain.schema import SystemMessage, AIMessage

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_create_researcher_llm_node_factory(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "test result"}
    
    # Execute factory function
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Verify factory returns a callable
    assert callable(researcher_node)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(researcher_node)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_valid_json_response(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with valid JSON response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "research result"}
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "research result"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    assert result["result"] == "research result"
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])
    
    # Verify validation was called
    mock_validate.assert_called_once_with({"result": "research result"}, ["result"])
    
    # Verify message creation
    mock_system_message.assert_called_once_with(content="Test system prompt")
    mock_ai_message.assert_called_once_with(content='{"result": "research result"}')
    mock_json.dumps.assert_called_once_with({"result": "research result"})

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_invalid_json_response(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with invalid JSON response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = "Invalid response format"
    
    mock_validate.return_value = False
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == ["Invalid response format"]
    assert "result" not in result
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])
    
    # Verify validation was called
    mock_validate.assert_called_once_with("Invalid response format", ["result"])
    
    # Verify message creation
    mock_system_message.assert_called_once_with(content="Test system prompt")
    
    # Verify AIMessage and json.dumps were not called
    mock_ai_message.assert_not_called()
    mock_json.dumps.assert_not_called()

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_empty_messages(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with empty message list."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "research result"}
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "research result"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state with empty messages
    test_state = ResearcherState(
        messages=[],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    assert result["result"] == "research result"
    
    # Verify LLM was called with only system message
    mock_llm.invoke.assert_called_once_with([mock_sys_msg])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_multiple_messages(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with multiple messages in state."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "research result"}
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "research result"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state with multiple messages
    message1 = Mock()
    message2 = Mock()
    test_state = ResearcherState(
        messages=[message1, message2],
        step_index=1,
        result="previous result"
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    assert result["result"] == "research result"
    
    # Verify LLM was called with system message and all state messages
    mock_llm.invoke.assert_called_once_with([mock_sys_msg, message1, message2])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_large_response(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with large LLM response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    large_response = {"result": "x" * 10000}  # Large response
    mock_llm = Mock()
    mock_llm.invoke.return_value = large_response
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "' + "x" * 10000 + '"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    assert result["result"] == "x" * 10000
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_empty_response(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with empty LLM response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = ""
    
    mock_validate.return_value = False
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [""]
    assert "result" not in result
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_special_characters(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with special characters in response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    special_response = {"result": "Special chars: Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"}
    mock_llm = Mock()
    mock_llm.invoke.return_value = special_response
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "Special chars: Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    assert result["result"] == "Special chars: Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_nested_json(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with nested JSON response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string", "details": "object"}
    
    nested_response = {
        "result": "research result",
        "details": {
            "sources": ["source1", "source2"],
            "confidence": 0.95,
            "metadata": {
                "timestamp": "2023-01-01",
                "version": "1.0"
            }
        }
    }
    mock_llm = Mock()
    mock_llm.invoke.return_value = nested_response
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "research result", "details": {...}}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    assert result["result"] == "research result"
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_missing_fields(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node with JSON missing expected fields."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string", "confidence": "number"}
    
    incomplete_response = {"result": "research result"}  # Missing confidence field
    mock_llm = Mock()
    mock_llm.invoke.return_value = incomplete_response
    
    mock_validate.return_value = False  # Validation fails due to missing field
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify results
    assert result["messages"] == [incomplete_response]
    assert "result" not in result
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_llm_invocation_failure(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node when LLM invocation fails."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.side_effect = Exception("LLM API error")
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node and expect exception
    with pytest.raises(Exception) as exc_info:
        researcher_node(test_state)
    
    assert "LLM API error" in str(exc_info.value)
    
    # Verify LLM was called
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_validation_function_failure(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node when validation function fails."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "research result"}
    
    mock_validate.side_effect = Exception("Validation error")
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node and expect exception
    with pytest.raises(Exception) as exc_info:
        researcher_node(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify LLM was called
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_json_serialization_failure(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test researcher node when JSON serialization fails."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "research result"}
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.side_effect = Exception("JSON serialization error")
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node and expect exception
    with pytest.raises(Exception) as exc_info:
        researcher_node(test_state)
    
    assert "JSON serialization error" in str(exc_info.value)
    
    # Verify LLM was called
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_return_type_validation(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test that returned function has correct return type structure."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "research result"}
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "research result"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ResearcherState(
        messages=[Mock()],
        step_index=0,
        result=None
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify return type structure
    assert isinstance(result, dict)
    assert "messages" in result
    assert "result" in result
    assert isinstance(result["messages"], list)
    assert isinstance(result["result"], str)

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
@patch('multi_agent_system.json')
def test_researcher_llm_node_state_preservation(mock_json, mock_ai_message, mock_system_message, mock_validate):
    """Test that state fields are preserved correctly."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test system prompt"
    mock_config.output_schema = {"result": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {"result": "new result"}
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    mock_json.dumps.return_value = '{"result": "new result"}'
    
    # Create researcher node
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Setup test state with existing data
    original_messages = [Mock(), Mock()]
    test_state = ResearcherState(
        messages=original_messages,
        step_index=5,
        result="original result"
    )
    
    # Execute researcher node
    result = researcher_node(test_state)
    
    # Verify that only messages and result are updated, other fields preserved
    assert result["messages"] == [mock_ai_msg]  # Updated
    assert result["result"] == "new result"     # Updated
    # Note: step_index is not returned in the result, so it's not preserved
    # This is expected behavior based on the function implementation
```
#### 3.5.2 Expert LLM Node Factory

**Function Information:**
- **Function Name**: `create_expert_llm_node`
- **Location**: `multi_agent_system.py:463`
- **Purpose**: Creates an expert LLM node function that processes ExpertState and returns updated state with expert answers and reasoning traces

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompt and output schema for the expert agent
- **Input**: `llm_expert: ChatOpenAI` - LangChain ChatOpenAI instance for LLM interactions
- **Return**: `Callable[[ExpertState], ExpertState]` - A function that takes and returns ExpertState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `llm_expert.invoke()` - LangChain LLM invocation method
  - `validate_output_matches_json_schema()` - Internal validation function
  - `SystemMessage` - LangChain message class
  - `AIMessage` - LangChain message class
  - `f-string` formatting - Built-in Python string formatting (for message content)
- **Dependencies to Use Directly**: 
  - `Callable` - Built-in Python type annotation
  - `ExpertState` - Internal TypedDict type
  - `AgentConfig` - Internal configuration class
  - `ChatOpenAI` - LangChain class (type annotation only)
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
- **Mock Configuration**: Mock LLM responses, validation function, and message classes for controlled testing

**Test Cases:**

**Happy Path:**
- **Valid JSON Response**: LLM returns properly formatted JSON that matches schema
- **Valid Non-JSON Response**: LLM returns non-JSON response that doesn't match schema
- **Multiple Messages**: State contains multiple messages for context
- **Empty Messages**: State with empty message list
- **Complex State**: State with various combinations of fields

**Edge Cases:**
- **Large Response**: Very large LLM responses
- **Empty Response**: Empty or minimal LLM responses
- **Special Characters**: Responses with special characters and Unicode
- **Nested JSON**: Complex nested JSON structures in responses
- **Missing Fields**: JSON responses missing expected fields

**Error Conditions:**
- **LLM Invocation Failure**: Network errors, API failures, timeout errors
- **Invalid JSON**: Malformed JSON responses from LLM
- **Schema Mismatch**: JSON that doesn't match expected schema
- **Missing Configuration**: Invalid or missing AgentConfig
- **Invalid State**: Corrupted or invalid ExpertState
- **Validation Function Failure**: Errors in validate_output_matches_json_schema

**Mock Configurations:**
```python
@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_factory_with_mocked_dependencies(mock_ai_message, mock_system_message, mock_validate):
    """Mock all external dependencies for controlled testing."""
    # Mock AgentConfig
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    # Mock ChatOpenAI
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "test answer",
        "reasoning_trace": "test reasoning"
    }
    
    # Mock validation function
    mock_validate.return_value = True
    
    # Mock message classes
    mock_system_message.return_value = Mock()
    mock_ai_message.return_value = Mock()
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and type annotations in the implementation:
def create_expert_llm_node(config: AgentConfig, llm_expert: ChatOpenAI) -> Callable[[ExpertState], ExpertState]:  # Direct type annotations
    """Create an expert LLM node function with the given prompt and LLM."""
    def expert_llm_node(state: ExpertState) -> ExpertState:  # Direct function definition and type annotations
        sys_prompt = [SystemMessage(content=config.system_prompt)]  # Direct list creation and attribute access
        response = llm_expert.invoke(sys_prompt + state["messages"])  # Direct method call and list concatenation
        if validate_output_matches_json_schema(response, config.output_schema.keys()):  # Direct function call and method chaining
            state["expert_answer"] = response["expert_answer"]  # Direct dictionary assignment
            state["expert_reasoning"] = response["reasoning_trace"]  # Direct dictionary assignment
            response = AIMessage(content=f"Expert answer: {state['expert_answer']}\nExpert reasoning: {state['expert_reasoning']}")  # Direct f-string formatting
        return {"messages": [response]}  # Direct dict creation and list wrapping
    return expert_llm_node  # Direct function return
```

**Assertion Specifications:**
- Verify function returns a callable with correct signature
- Verify returned function processes ExpertState correctly
- Verify LLM is invoked with correct system prompt and messages
- Verify JSON schema validation is called with correct parameters
- Verify appropriate response handling based on validation result
- Verify state updates include correct expert_answer and expert_reasoning fields
- Verify AIMessage creation with properly formatted content
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_expert_llm_node, ExpertState
from langchain.schema import SystemMessage, AIMessage

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_create_expert_llm_node_factory(mock_ai_message, mock_system_message, mock_validate):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "test answer",
        "reasoning_trace": "test reasoning"
    }
    
    # Execute factory function
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Verify factory returns a callable
    assert callable(expert_node)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(expert_node)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_valid_json_response(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with valid JSON response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "The answer is 42",
        "reasoning_trace": "Based on mathematical analysis"
    }
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="What is the answer?",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    
    # Verify state was updated
    assert test_state["expert_answer"] == "The answer is 42"
    assert test_state["expert_reasoning"] == "Based on mathematical analysis"
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])
    
    # Verify validation was called
    mock_validate.assert_called_once_with(
        {"expert_answer": "The answer is 42", "reasoning_trace": "Based on mathematical analysis"},
        ["expert_answer", "reasoning_trace"]
    )
    
    # Verify message creation
    mock_system_message.assert_called_once_with(content="Test expert system prompt")
    mock_ai_message.assert_called_once_with(
        content="Expert answer: The answer is 42\nExpert reasoning: Based on mathematical analysis"
    )

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_invalid_json_response(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with invalid JSON response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = "Invalid response format"
    
    mock_validate.return_value = False
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="What is the answer?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == ["Invalid response format"]
    
    # Verify state was not updated
    assert test_state["expert_answer"] == ""
    assert test_state["expert_reasoning"] == ""
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])
    
    # Verify validation was called
    mock_validate.assert_called_once_with("Invalid response format", ["expert_answer", "reasoning_trace"])
    
    # Verify message creation
    mock_system_message.assert_called_once_with(content="Test expert system prompt")
    
    # Verify AIMessage was not called
    mock_ai_message.assert_not_called()

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_empty_messages(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with empty message list."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "Empty context answer",
        "reasoning_trace": "No previous messages"
    }
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state with empty messages
    test_state = ExpertState(
        messages=[],
        question="What is the answer?",
        research_steps=[],
        research_results=[],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    
    # Verify state was updated
    assert test_state["expert_answer"] == "Empty context answer"
    assert test_state["expert_reasoning"] == "No previous messages"
    
    # Verify LLM was called with only system message
    mock_llm.invoke.assert_called_once_with([mock_sys_msg])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_multiple_messages(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with multiple messages in state."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "Multi-context answer",
        "reasoning_trace": "Based on multiple messages"
    }
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state with multiple messages
    message1 = Mock()
    message2 = Mock()
    test_state = ExpertState(
        messages=[message1, message2],
        question="Complex question?",
        research_steps=["step1", "step2", "step3"],
        research_results=["result1", "result2", "result3"],
        expert_answer="previous answer",
        expert_reasoning="previous reasoning"
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    
    # Verify state was updated
    assert test_state["expert_answer"] == "Multi-context answer"
    assert test_state["expert_reasoning"] == "Based on multiple messages"
    
    # Verify LLM was called with system message and all state messages
    mock_llm.invoke.assert_called_once_with([mock_sys_msg, message1, message2])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_large_response(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with large LLM response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    large_answer = "x" * 5000
    large_reasoning = "y" * 5000
    large_response = {
        "expert_answer": large_answer,
        "reasoning_trace": large_reasoning
    }
    mock_llm = Mock()
    mock_llm.invoke.return_value = large_response
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Large response question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    
    # Verify state was updated with large content
    assert test_state["expert_answer"] == large_answer
    assert test_state["expert_reasoning"] == large_reasoning
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_empty_response(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with empty LLM response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = ""
    
    mock_validate.return_value = False
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Empty response question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [""]
    
    # Verify state was not updated
    assert test_state["expert_answer"] == ""
    assert test_state["expert_reasoning"] == ""
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_special_characters(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with special characters in response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    special_response = {
        "expert_answer": "Special chars: Ã©Ã±Ã¼ÃŸÂ©Â®â„¢",
        "reasoning_trace": "Unicode reasoning: ðŸš€ðŸŒŸðŸ’¡"
    }
    mock_llm = Mock()
    mock_llm.invoke.return_value = special_response
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Special chars question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    
    # Verify state was updated with special characters
    assert test_state["expert_answer"] == "Special chars: Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"
    assert test_state["expert_reasoning"] == "Unicode reasoning: ðŸš€ðŸŒŸðŸ’¡"
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_nested_json(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with nested JSON response."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    nested_response = {
        "expert_answer": "Nested answer",
        "reasoning_trace": "Complex reasoning with nested structure",
        "metadata": {
            "confidence": 0.95,
            "sources": ["source1", "source2"],
            "details": {
                "analysis_type": "deep",
                "version": "1.0"
            }
        }
    }
    mock_llm = Mock()
    mock_llm.invoke.return_value = nested_response
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Nested JSON question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
    
    # Verify state was updated (only expected fields)
    assert test_state["expert_answer"] == "Nested answer"
    assert test_state["expert_reasoning"] == "Complex reasoning with nested structure"
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_missing_fields(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node with JSON missing expected fields."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    incomplete_response = {"expert_answer": "Only answer provided"}  # Missing reasoning_trace field
    mock_llm = Mock()
    mock_llm.invoke.return_value = incomplete_response
    
    mock_validate.return_value = False  # Validation fails due to missing field
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Missing fields question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify results
    assert result["messages"] == [incomplete_response]
    
    # Verify state was not updated
    assert test_state["expert_answer"] == ""
    assert test_state["expert_reasoning"] == ""
    
    # Verify LLM was called correctly
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_llm_invocation_failure(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node when LLM invocation fails."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.side_effect = Exception("LLM API error")
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="LLM failure question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node and expect exception
    with pytest.raises(Exception) as exc_info:
        expert_node(test_state)
    
    assert "LLM API error" in str(exc_info.value)
    
    # Verify LLM was called
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_validation_function_failure(mock_ai_message, mock_system_message, mock_validate):
    """Test expert node when validation function fails."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "test answer",
        "reasoning_trace": "test reasoning"
    }
    
    mock_validate.side_effect = Exception("Validation error")
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Validation failure question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node and expect exception
    with pytest.raises(Exception) as exc_info:
        expert_node(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify LLM was called
    mock_llm.invoke.assert_called_once_with([mock_sys_msg] + test_state["messages"])

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_return_type_validation(mock_ai_message, mock_system_message, mock_validate):
    """Test that returned function has correct return type structure."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "test answer",
        "reasoning_trace": "test reasoning"
    }
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Type validation question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify return type structure
    assert isinstance(result, dict)
    assert "messages" in result
    assert isinstance(result["messages"], list)
    assert len(result["messages"]) == 1

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_state_preservation(mock_ai_message, mock_system_message, mock_validate):
    """Test that state fields are preserved correctly."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "new answer",
        "reasoning_trace": "new reasoning"
    }
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state with existing data
    original_messages = [Mock(), Mock()]
    test_state = ExpertState(
        messages=original_messages,
        question="Original question",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_answer="original answer",
        expert_reasoning="original reasoning"
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify that expert_answer and expert_reasoning are updated
    assert test_state["expert_answer"] == "new answer"
    assert test_state["expert_reasoning"] == "new reasoning"
    
    # Verify that other fields are preserved
    assert test_state["question"] == "Original question"
    assert test_state["research_steps"] == ["step1", "step2"]
    assert test_state["research_results"] == ["result1", "result2"]
    
    # Verify return structure
    assert result["messages"] == [mock_ai_msg]

@patch('multi_agent_system.validate_output_matches_json_schema')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.AIMessage')
def test_expert_llm_node_aimessage_content_formatting(mock_ai_message, mock_system_message, mock_validate):
    """Test that AIMessage content is formatted correctly."""
    # Setup mocks
    mock_config = Mock()
    mock_config.system_prompt = "Test expert system prompt"
    mock_config.output_schema = {"expert_answer": "string", "reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = {
        "expert_answer": "The answer is 42",
        "reasoning_trace": "Based on deep analysis"
    }
    
    mock_validate.return_value = True
    
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    mock_ai_msg = Mock()
    mock_ai_message.return_value = mock_ai_msg
    
    # Create expert node
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Setup test state
    test_state = ExpertState(
        messages=[Mock()],
        question="Format test question?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert node
    result = expert_node(test_state)
    
    # Verify AIMessage was created with correct formatted content
    expected_content = "Expert answer: The answer is 42\nExpert reasoning: Based on deep analysis"
    mock_ai_message.assert_called_once_with(content=expected_content)
    
    # Verify results
    assert result["messages"] == [mock_ai_msg]
```
### 3.6 Subgraph Factories
#### 3.6.1 Researcher Subgraph Factory

**Function Information:**
- **Function Name**: `create_researcher_subgraph`
- **Location**: `multi_agent_system.py:393`
- **Purpose**: Creates and compiles a researcher subgraph with LLM node and research tools, establishing the graph structure and execution flow

**Function Signature and Parameters:**
- **Input**: `researcher_llm_node: Callable` - A callable function that processes ResearcherState and returns updated state
- **Input**: `research_tools: list` - List of research tools to be used by the researcher agent
- **Return**: `StateGraph` - A compiled LangGraph StateGraph representing the researcher subgraph
- **Side Effects**: None (creates and returns graph structure)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `StateGraph` - LangGraph class for creating graph structure
  - `ToolNode` - LangGraph class for tool execution nodes
  - `tools_condition` - Internal function for conditional edge routing
  - `START` - LangGraph constant for graph entry point
- **Dependencies to Use Directly**: 
  - `Callable` - Built-in Python type annotation
  - `list` - Built-in Python type annotation
  - `ResearcherState` - Internal TypedDict type
  - Function definition and method calls - Built-in Python features
- **Mock Configuration**: Mock StateGraph, ToolNode, tools_condition, and START for controlled testing

**Test Cases:**

**Happy Path:**
- **Valid LLM Node and Tools**: Function with valid callable and tool list
- **Empty Tools List**: Function with valid callable but empty tools list
- **Single Tool**: Function with valid callable and single tool
- **Multiple Tools**: Function with valid callable and multiple tools
- **Complex Tool List**: Function with various types of tools

**Edge Cases:**
- **Large Tool List**: Very large number of research tools
- **None Tools**: Tools list containing None values
- **Invalid Tool Types**: Tools that are not proper LangChain tools
- **Duplicate Tools**: Tools list with duplicate entries
- **Nested Tool Structures**: Complex tool configurations

**Error Conditions:**
- **Invalid LLM Node**: Non-callable researcher_llm_node
- **Invalid Tools**: Malformed or invalid tool objects
- **StateGraph Creation Failure**: Errors in StateGraph instantiation
- **Node Addition Failure**: Errors when adding nodes to graph
- **Edge Addition Failure**: Errors when adding edges to graph
- **Compilation Failure**: Errors during graph compilation

**Mock Configurations:**
```python
@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_factory_with_mocked_dependencies(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Mock all external dependencies for controlled testing."""
    # Mock StateGraph
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    # Mock ToolNode
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    # Mock tools_condition function
    mock_tools_condition.return_value = "next_node"
    
    # Mock START constant
    mock_start.value = "START"
    
    # Mock graph methods
    mock_graph.add_node.return_value = None
    mock_graph.add_edge.return_value = None
    mock_graph.add_conditional_edges.return_value = None
    mock_graph.compile.return_value = Mock()
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and type annotations in the implementation:
def create_researcher_subgraph(researcher_llm_node: Callable, research_tools: list) -> StateGraph:  # Direct type annotations
    """Create and compile a researcher subgraph with the given prompt and LLM.

    Args:
        researcher_llm_node (Callable): The researcher LLM node function
        research_tools (list): The list of research tools

    Returns:
        StateGraph: The compiled researcher subgraph
    """
    researcher_graph = StateGraph(ResearcherState)  # Direct class instantiation
    researcher_graph.add_node("researcher", researcher_llm_node)  # Direct method call
    researcher_graph.add_node("tools", ToolNode(research_tools))  # Direct method call with class instantiation
    researcher_graph.add_edge(START, "researcher")  # Direct method call with constants
    researcher_graph.add_conditional_edges("researcher", tools_condition)  # Direct method call with function
    researcher_graph.add_edge("tools", "researcher")  # Direct method call
    return researcher_graph.compile()  # Direct method call and return
```

**Assertion Specifications:**
- Verify function returns a compiled StateGraph
- Verify StateGraph is created with correct state type (ResearcherState)
- Verify nodes are added with correct names and functions
- Verify edges are added with correct source and destination nodes
- Verify conditional edges are configured properly
- Verify graph compilation is called
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_researcher_subgraph, ResearcherState

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_create_researcher_subgraph_factory(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test the factory function creates and compiles a researcher subgraph."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock(), Mock()]
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created with correct state type
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify edges were added
    mock_graph.add_edge.assert_any_call(mock_start, "researcher")
    mock_graph.add_edge.assert_any_call("tools", "researcher")
    
    # Verify conditional edges were added
    mock_graph.add_conditional_edges.assert_called_once_with("researcher", mock_tools_condition)
    
    # Verify ToolNode was created with correct tools
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_empty_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph creation with empty tools list."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = []  # Empty tools list
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with empty list
    mock_tool_node.assert_called_once_with([])
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_single_tool(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph creation with single tool."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock()]  # Single tool
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with single tool
    mock_tool_node.assert_called_once_with([research_tools[0]])
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_multiple_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph creation with multiple tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock(), Mock(), Mock(), Mock()]  # Multiple tools
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with all tools
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_large_tool_list(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph creation with large number of tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock() for _ in range(100)]  # Large number of tools
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with large tool list
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_none_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph creation with None values in tools list."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock(), None, Mock(), None]  # Tools with None values
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with tools including None values
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_duplicate_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph creation with duplicate tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    tool1 = Mock()
    tool2 = Mock()
    research_tools = [tool1, tool2, tool1, tool2]  # Duplicate tools
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with duplicate tools
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_stategraph_creation_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph when StateGraph creation fails."""
    # Setup mocks
    mock_state_graph.side_effect = Exception("StateGraph creation error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_researcher_subgraph(mock_llm_node, research_tools)
    
    assert "StateGraph creation error" in str(exc_info.value)
    
    # Verify StateGraph was attempted to be created
    mock_state_graph.assert_called_once_with(ResearcherState)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_node_addition_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph when node addition fails."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    # Mock node addition failure
    mock_graph.add_node.side_effect = Exception("Node addition error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_researcher_subgraph(mock_llm_node, research_tools)
    
    assert "Node addition error" in str(exc_info.value)
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_edge_addition_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph when edge addition fails."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    # Mock edge addition failure
    mock_graph.add_edge.side_effect = Exception("Edge addition error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_researcher_subgraph(mock_llm_node, research_tools)
    
    assert "Edge addition error" in str(exc_info.value)
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_compilation_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test researcher subgraph when compilation fails."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    # Mock compilation failure
    mock_graph.compile.side_effect = Exception("Compilation error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_researcher_subgraph(mock_llm_node, research_tools)
    
    assert "Compilation error" in str(exc_info.value)
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("researcher", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify edges were added
    mock_graph.add_edge.assert_any_call(mock_start, "researcher")
    mock_graph.add_edge.assert_any_call("tools", "researcher")
    
    # Verify conditional edges were added
    mock_graph.add_conditional_edges.assert_called_once_with("researcher", mock_tools_condition)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_return_type_validation(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that function returns correct type structure."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock()]
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify return type structure
    assert result == mock_compiled_graph
    assert isinstance(result, Mock)  # Mock object representing compiled graph

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_graph_structure_verification(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that graph structure is created correctly."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    research_tools = [Mock(), Mock()]
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify graph structure creation
    mock_state_graph.assert_called_once_with(ResearcherState)
    
    # Verify all nodes were added in correct order
    expected_add_node_calls = [
        (("researcher", mock_llm_node),),
        (("tools", mock_tool_node_instance),)
    ]
    assert mock_graph.add_node.call_args_list == expected_add_node_calls
    
    # Verify all edges were added in correct order
    expected_add_edge_calls = [
        ((mock_start, "researcher"),),
        (("tools", "researcher"),)
    ]
    assert mock_graph.add_edge.call_args_list == expected_add_edge_calls
    
    # Verify conditional edges were added
    mock_graph.add_conditional_edges.assert_called_once_with("researcher", mock_tools_condition)
    
    # Verify ToolNode was created with correct tools
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()
    
    # Verify result
    assert result == mock_compiled_graph

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_researcher_subgraph_toolnode_creation_verification(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that ToolNode is created correctly with research tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs with specific tools
    mock_llm_node = Mock()
    tool1 = Mock()
    tool1.name = "web_search"
    tool2 = Mock()
    tool2.name = "file_reader"
    research_tools = [tool1, tool2]
    
    # Execute factory function
    result = create_researcher_subgraph(mock_llm_node, research_tools)
    
    # Verify ToolNode was created with correct tools
    mock_tool_node.assert_called_once_with(research_tools)
    
    # Verify ToolNode instance was added to graph
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify result
    assert result == mock_compiled_graph
```
#### 3.6.2 Expert Subgraph Factory

**Function Information:**
- **Function Name**: `create_expert_subgraph`
- **Location**: `multi_agent_system.py:479`
- **Purpose**: Creates and compiles an expert subgraph with LLM node and expert tools, establishing the graph structure and execution flow

**Function Signature and Parameters:**
- **Input**: `expert_llm_node: Callable` - A callable function that processes ExpertState and returns updated state
- **Input**: `expert_tools: list` - List of expert tools to be used by the expert agent
- **Return**: `StateGraph` - A compiled LangGraph StateGraph representing the expert subgraph
- **Side Effects**: None (creates and returns graph structure)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `StateGraph` - LangGraph class for creating graph structure
  - `ToolNode` - LangGraph class for tool execution nodes
  - `tools_condition` - Internal function for conditional edge routing
  - `START` - LangGraph constant for graph entry point
- **Dependencies to Use Directly**: 
  - `Callable` - Built-in Python type annotation
  - `list` - Built-in Python type annotation
  - `ExpertState` - Internal TypedDict type
  - Function definition and method calls - Built-in Python features
- **Mock Configuration**: Mock StateGraph, ToolNode, tools_condition, and START for controlled testing

**Test Cases:**

**Happy Path:**
- **Valid LLM Node and Tools**: Function with valid callable and tool list
- **Empty Tools List**: Function with valid callable but empty tools list
- **Single Tool**: Function with valid callable and single tool
- **Multiple Tools**: Function with valid callable and multiple tools
- **Complex Tool List**: Function with various types of expert tools

**Edge Cases:**
- **Large Tool List**: Very large number of expert tools
- **None Tools**: Tools list containing None values
- **Invalid Tool Types**: Tools that are not proper LangChain tools
- **Duplicate Tools**: Tools list with duplicate entries
- **Nested Tool Structures**: Complex tool configurations

**Error Conditions:**
- **Invalid LLM Node**: Non-callable expert_llm_node
- **Invalid Tools**: Malformed or invalid tool objects
- **StateGraph Creation Failure**: Errors in StateGraph instantiation
- **Node Addition Failure**: Errors when adding nodes to graph
- **Edge Addition Failure**: Errors when adding edges to graph
- **Compilation Failure**: Errors during graph compilation

**Mock Configurations:**
```python
@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_factory_with_mocked_dependencies(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Mock all external dependencies for controlled testing."""
    # Mock StateGraph
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    # Mock ToolNode
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    # Mock tools_condition function
    mock_tools_condition.return_value = "next_node"
    
    # Mock START constant
    mock_start.value = "START"
    
    # Mock graph methods
    mock_graph.add_node.return_value = None
    mock_graph.add_edge.return_value = None
    mock_graph.add_conditional_edges.return_value = None
    mock_graph.compile.return_value = Mock()
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and type annotations in the implementation:
def create_expert_subgraph(expert_llm_node: Callable, expert_tools: list) -> StateGraph:  # Direct type annotations
    """Create and compile an expert subgraph with the given prompt and LLM."""
    expert_graph = StateGraph(ExpertState)  # Direct class instantiation
    expert_graph.add_node("expert", expert_llm_node)  # Direct method call
    expert_graph.add_node("tools", ToolNode(expert_tools))  # Direct method call with class instantiation
    expert_graph.add_edge(START, "expert")  # Direct method call with constants
    expert_graph.add_conditional_edges("expert", tools_condition)  # Direct method call with function
    expert_graph.add_edge("tools", "expert")  # Direct method call
    return expert_graph.compile()  # Direct method call and return
```

**Assertion Specifications:**
- Verify function returns a compiled StateGraph
- Verify StateGraph is created with correct state type (ExpertState)
- Verify nodes are added with correct names and functions
- Verify edges are added with correct source and destination nodes
- Verify conditional edges are configured properly
- Verify graph compilation is called
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_expert_subgraph, ExpertState

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_create_expert_subgraph_factory(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test the factory function creates and compiles an expert subgraph."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock(), Mock()]
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created with correct state type
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify edges were added
    mock_graph.add_edge.assert_any_call(mock_start, "expert")
    mock_graph.add_edge.assert_any_call("tools", "expert")
    
    # Verify conditional edges were added
    mock_graph.add_conditional_edges.assert_called_once_with("expert", mock_tools_condition)
    
    # Verify ToolNode was created with correct tools
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_empty_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph creation with empty tools list."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = []  # Empty tools list
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with empty list
    mock_tool_node.assert_called_once_with([])
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_single_tool(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph creation with single tool."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]  # Single tool
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with single tool
    mock_tool_node.assert_called_once_with([expert_tools[0]])
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_multiple_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph creation with multiple tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock(), Mock(), Mock(), Mock()]  # Multiple tools
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with all tools
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_large_tool_list(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph creation with large number of tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock() for _ in range(100)]  # Large number of tools
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with large tool list
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_none_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph creation with None values in tools list."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock(), None, Mock(), None]  # Tools with None values
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with tools including None values
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_duplicate_tools(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph creation with duplicate tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    tool1 = Mock()
    tool2 = Mock()
    expert_tools = [tool1, tool2, tool1, tool2]  # Duplicate tools
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify ToolNode was created with duplicate tools
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_stategraph_creation_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph when StateGraph creation fails."""
    # Setup mocks
    mock_state_graph.side_effect = Exception("StateGraph creation error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_expert_subgraph(mock_llm_node, expert_tools)
    
    assert "StateGraph creation error" in str(exc_info.value)
    
    # Verify StateGraph was attempted to be created
    mock_state_graph.assert_called_once_with(ExpertState)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_node_addition_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph when node addition fails."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    # Mock node addition failure
    mock_graph.add_node.side_effect = Exception("Node addition error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_expert_subgraph(mock_llm_node, expert_tools)
    
    assert "Node addition error" in str(exc_info.value)
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_edge_addition_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph when edge addition fails."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    # Mock edge addition failure
    mock_graph.add_edge.side_effect = Exception("Edge addition error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_expert_subgraph(mock_llm_node, expert_tools)
    
    assert "Edge addition error" in str(exc_info.value)
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_compilation_failure(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test expert subgraph when compilation fails."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    # Mock compilation failure
    mock_graph.compile.side_effect = Exception("Compilation error")
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]
    
    # Execute factory function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_expert_subgraph(mock_llm_node, expert_tools)
    
    assert "Compilation error" in str(exc_info.value)
    
    # Verify StateGraph was created
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify nodes were added
    mock_graph.add_node.assert_any_call("expert", mock_llm_node)
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify edges were added
    mock_graph.add_edge.assert_any_call(mock_start, "expert")
    mock_graph.add_edge.assert_any_call("tools", "expert")
    
    # Verify conditional edges were added
    mock_graph.add_conditional_edges.assert_called_once_with("expert", mock_tools_condition)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_return_type_validation(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that function returns correct type structure."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify return type structure
    assert result == mock_compiled_graph
    assert isinstance(result, Mock)  # Mock object representing compiled graph

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_graph_structure_verification(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that graph structure is created correctly."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock(), Mock()]
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify graph structure creation
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify all nodes were added in correct order
    expected_add_node_calls = [
        (("expert", mock_llm_node),),
        (("tools", mock_tool_node_instance),)
    ]
    assert mock_graph.add_node.call_args_list == expected_add_node_calls
    
    # Verify all edges were added in correct order
    expected_add_edge_calls = [
        ((mock_start, "expert"),),
        (("tools", "expert"),)
    ]
    assert mock_graph.add_edge.call_args_list == expected_add_edge_calls
    
    # Verify conditional edges were added
    mock_graph.add_conditional_edges.assert_called_once_with("expert", mock_tools_condition)
    
    # Verify ToolNode was created with correct tools
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify graph was compiled
    mock_graph.compile.assert_called_once()
    
    # Verify result
    assert result == mock_compiled_graph

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_toolnode_creation_verification(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that ToolNode is created correctly with expert tools."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs with specific expert tools
    mock_llm_node = Mock()
    tool1 = Mock()
    tool1.name = "unit_converter"
    tool2 = Mock()
    tool2.name = "calculator"
    tool3 = Mock()
    tool3.name = "python_repl"
    expert_tools = [tool1, tool2, tool3]
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify ToolNode was created with correct tools
    mock_tool_node.assert_called_once_with(expert_tools)
    
    # Verify ToolNode instance was added to graph
    mock_graph.add_node.assert_any_call("tools", mock_tool_node_instance)
    
    # Verify result
    assert result == mock_compiled_graph

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.ToolNode')
@patch('multi_agent_system.tools_condition')
@patch('multi_agent_system.START')
def test_expert_subgraph_expert_state_type_verification(mock_start, mock_tools_condition, mock_tool_node, mock_state_graph):
    """Test that StateGraph is created with ExpertState type."""
    # Setup mocks
    mock_graph = Mock()
    mock_state_graph.return_value = mock_graph
    
    mock_tool_node_instance = Mock()
    mock_tool_node.return_value = mock_tool_node_instance
    
    mock_tools_condition.return_value = "next_node"
    mock_start.value = "START"
    
    mock_compiled_graph = Mock()
    mock_graph.compile.return_value = mock_compiled_graph
    
    # Setup test inputs
    mock_llm_node = Mock()
    expert_tools = [Mock()]
    
    # Execute factory function
    result = create_expert_subgraph(mock_llm_node, expert_tools)
    
    # Verify StateGraph was created with ExpertState
    mock_state_graph.assert_called_once_with(ExpertState)
    
    # Verify result
    assert result == mock_compiled_graph
```
### 3.7 Main Graph Components
#### 3.7.1 Input Interface Factory

**Function Information:**
- **Function Name**: `create_input_interface`
- **Location**: `multi_agent_system.py:493`
- **Purpose**: Creates an input interface function that initializes and validates GraphState, setting up all required fields for the multi-agent system workflow

**Function Signature and Parameters:**
- **Input**: `agent_configs: dict[str, AgentConfig]` - Dictionary mapping agent names to their configuration objects containing retry limits
- **Return**: `Callable[[GraphState], GraphState]` - A function that takes and returns GraphState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `AgentConfig` - Internal configuration class (for retry_limit access)
- **Dependencies to Use Directly**: 
  - `dict` - Built-in Python type annotation and operations
  - `Callable` - Built-in Python type annotation
  - `GraphState` - Internal TypedDict type
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
  - String operations and comparisons - Built-in Python features
- **Mock Configuration**: Mock logger for consistent testing and AgentConfig objects for retry limit access

**Test Cases:**

**Happy Path:**
- **Valid Question and Configs**: Function with valid question and complete agent configurations
- **Valid Question with File**: Function with valid question and file attachment
- **Valid Question without File**: Function with valid question but no file
- **Complete Agent Configs**: All agent types (planner, researcher, expert) configured
- **Partial Agent Configs**: Some agent types configured

**Edge Cases:**
- **Empty Question**: Question with only whitespace characters
- **Very Long Question**: Extremely long question text
- **Special Characters**: Question with special characters and Unicode
- **Missing Agent Configs**: Some agent configurations missing
- **Zero Retry Limits**: Agents configured with zero retry limits
- **Large Retry Limits**: Agents configured with very large retry limits

**Error Conditions:**
- **No Question**: Missing or None question field
- **Empty Question**: Empty string question
- **Invalid Agent Configs**: Malformed or invalid agent configuration dictionary
- **Missing Required Agents**: Missing planner, researcher, or expert configurations
- **Invalid Retry Limits**: Negative or invalid retry limit values
- **Logger Failure**: Errors in logging operations

**Mock Configurations:**
```python
@patch('multi_agent_system.logger')
def test_input_interface_factory_with_mocked_dependencies(mock_logger):
    """Mock logger for consistent testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock agent configurations
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and type annotations in the implementation:
def create_input_interface(agent_configs: dict[str, AgentConfig]):  # Direct type annotations
    """Create an input interface function with the given retry limit."""
    def input_interface(state: GraphState) -> GraphState:  # Direct function definition and type annotations
        """Input interface with error handling and validation."""
        logger.info("Input interface starting execution")  # Direct logging call
        
        # Handle question extraction with proper error handling
        if not state.get("question") or len(state["question"]) == 0:  # Direct dictionary access and string operations
            raise ValueError("No question provided to input interface")  # Direct exception raising
        
        # Initialize all state fields first to ensure they exist for error handling
        state["file"] = state["file"] if state["file"] else None  # Direct conditional assignment
        # Planner Work
        state["research_steps"] = []  # Direct list assignment
        state["expert_steps"] = []  # Direct list assignment
        # Researcher Work
        state["current_research_index"] = -1  # Direct integer assignment
        state["researcher_states"] = dict()  # Direct dictionary creation
        state["research_results"] = []  # Direct list assignment
        # Expert Work
        state["expert_state"] = None  # Direct None assignment
        state["expert_answer"] = ""  # Direct string assignment
        state["expert_reasoning"] = ""  # Direct string assignment
        # Critic Work
        state["critic_planner_decision"] = ""  # Direct string assignment
        state["critic_planner_feedback"] = ""  # Direct string assignment
        state["critic_researcher_decision"] = ""  # Direct string assignment
        state["critic_researcher_feedback"] = ""  # Direct string assignment
        state["critic_expert_decision"] = ""  # Direct string assignment
        state["critic_expert_feedback"] = ""  # Direct string assignment
        # Finalizer Work
        state["final_answer"] = ""  # Direct string assignment
        state["final_reasoning_trace"] = ""  # Direct string assignment
        # Orchestrator Work
        state["agent_messages"] = []  # Direct list assignment
        state["current_step"] = "input"  # Direct string assignment
        state["next_step"] = "planner"  # Direct string assignment
        # Retry counts and limits
        state["planner_retry_count"] = 0  # Direct integer assignment
        state["researcher_retry_count"] = 0  # Direct integer assignment
        state["expert_retry_count"] = 0  # Direct integer assignment
        state["planner_retry_limit"] = agent_configs["planner"].retry_limit  # Direct dictionary access and attribute access
        state["researcher_retry_limit"] = agent_configs["researcher"].retry_limit  # Direct dictionary access and attribute access
        state["expert_retry_limit"] = agent_configs["expert"].retry_limit  # Direct dictionary access and attribute access
        
        logger.info("Input interface completed successfully")  # Direct logging call
        return state  # Direct return
    return input_interface  # Direct function return
```

**Assertion Specifications:**
- Verify function returns a callable with correct signature
- Verify returned function processes GraphState correctly
- Verify all required state fields are initialized with correct default values
- Verify retry limits are properly extracted from agent configurations
- Verify question validation works correctly
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_input_interface, GraphState

@patch('multi_agent_system.logger')
def test_create_input_interface_factory(mock_logger):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Execute factory function
    input_interface = create_input_interface(agent_configs)
    
    # Verify factory returns a callable
    assert callable(input_interface)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(input_interface)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.logger')
def test_input_interface_valid_question(mock_logger):
    """Test input interface with valid question."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="What is the capital of France?",
        file=None
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all fields are initialized correctly
    assert test_state["file"] is None
    assert test_state["research_steps"] == []
    assert test_state["expert_steps"] == []
    assert test_state["current_research_index"] == -1
    assert test_state["researcher_states"] == {}
    assert test_state["research_results"] == []
    assert test_state["expert_state"] is None
    assert test_state["expert_answer"] == ""
    assert test_state["expert_reasoning"] == ""
    assert test_state["critic_planner_decision"] == ""
    assert test_state["critic_planner_feedback"] == ""
    assert test_state["critic_researcher_decision"] == ""
    assert test_state["critic_researcher_feedback"] == ""
    assert test_state["critic_expert_decision"] == ""
    assert test_state["critic_expert_feedback"] == ""
    assert test_state["final_answer"] == ""
    assert test_state["final_reasoning_trace"] == ""
    assert test_state["agent_messages"] == []
    assert test_state["current_step"] == "input"
    assert test_state["next_step"] == "planner"
    assert test_state["planner_retry_count"] == 0
    assert test_state["researcher_retry_count"] == 0
    assert test_state["expert_retry_count"] == 0
    assert test_state["planner_retry_limit"] == 3
    assert test_state["researcher_retry_limit"] == 2
    assert test_state["expert_retry_limit"] == 1
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Input interface starting execution")
    mock_logger.info.assert_any_call("Input interface completed successfully")

@patch('multi_agent_system.logger')
def test_input_interface_valid_question_with_file(mock_logger):
    """Test input interface with valid question and file."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with file
    test_state = GraphState(
        question="Analyze this document",
        file="document.pdf"
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify file is preserved
    assert test_state["file"] == "document.pdf"
    
    # Verify all other fields are initialized correctly
    assert test_state["research_steps"] == []
    assert test_state["expert_steps"] == []
    assert test_state["current_step"] == "input"
    assert test_state["next_step"] == "planner"

@patch('multi_agent_system.logger')
def test_input_interface_empty_question(mock_logger):
    """Test input interface with empty question."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    agent_configs = {"planner": mock_planner_config}
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with empty question
    test_state = GraphState(
        question="",
        file=None
    )
    
    # Execute input interface and expect exception
    with pytest.raises(ValueError) as exc_info:
        input_interface(test_state)
    
    assert "No question provided to input interface" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Input interface starting execution")

@patch('multi_agent_system.logger')
def test_input_interface_none_question(mock_logger):
    """Test input interface with None question."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    agent_configs = {"planner": mock_planner_config}
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with None question
    test_state = GraphState(
        question=None,
        file=None
    )
    
    # Execute input interface and expect exception
    with pytest.raises(ValueError) as exc_info:
        input_interface(test_state)
    
    assert "No question provided to input interface" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Input interface starting execution")

@patch('multi_agent_system.logger')
def test_input_interface_missing_question(mock_logger):
    """Test input interface with missing question field."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    agent_configs = {"planner": mock_planner_config}
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state without question field
    test_state = GraphState(
        file=None
    )
    
    # Execute input interface and expect exception
    with pytest.raises(ValueError) as exc_info:
        input_interface(test_state)
    
    assert "No question provided to input interface" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Input interface starting execution")

@patch('multi_agent_system.logger')
def test_input_interface_whitespace_question(mock_logger):
    """Test input interface with whitespace-only question."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    agent_configs = {"planner": mock_planner_config}
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with whitespace question
    test_state = GraphState(
        question="   \n\t   ",
        file=None
    )
    
    # Execute input interface and expect exception
    with pytest.raises(ValueError) as exc_info:
        input_interface(test_state)
    
    assert "No question provided to input interface" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Input interface starting execution")

@patch('multi_agent_system.logger')
def test_input_interface_long_question(mock_logger):
    """Test input interface with very long question."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with long question
    long_question = "x" * 10000
    test_state = GraphState(
        question=long_question,
        file=None
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify question is preserved
    assert test_state["question"] == long_question
    
    # Verify all fields are initialized correctly
    assert test_state["research_steps"] == []
    assert test_state["expert_steps"] == []
    assert test_state["current_step"] == "input"
    assert test_state["next_step"] == "planner"

@patch('multi_agent_system.logger')
def test_input_interface_special_characters(mock_logger):
    """Test input interface with special characters in question."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with special characters
    special_question = "Special chars: Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ ðŸš€ðŸŒŸðŸ’¡"
    test_state = GraphState(
        question=special_question,
        file=None
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify question is preserved
    assert test_state["question"] == special_question
    
    # Verify all fields are initialized correctly
    assert test_state["research_steps"] == []
    assert test_state["expert_steps"] == []
    assert test_state["current_step"] == "input"
    assert test_state["next_step"] == "planner"

@patch('multi_agent_system.logger')
def test_input_interface_zero_retry_limits(mock_logger):
    """Test input interface with zero retry limits."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 0
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 0
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 0
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        file=None
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify retry limits are set correctly
    assert test_state["planner_retry_limit"] == 0
    assert test_state["researcher_retry_limit"] == 0
    assert test_state["expert_retry_limit"] == 0
    
    # Verify retry counts are initialized to 0
    assert test_state["planner_retry_count"] == 0
    assert test_state["researcher_retry_count"] == 0
    assert test_state["expert_retry_count"] == 0

@patch('multi_agent_system.logger')
def test_input_interface_large_retry_limits(mock_logger):
    """Test input interface with large retry limits."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 1000
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 500
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 250
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        file=None
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify retry limits are set correctly
    assert test_state["planner_retry_limit"] == 1000
    assert test_state["researcher_retry_limit"] == 500
    assert test_state["expert_retry_limit"] == 250

@patch('multi_agent_system.logger')
def test_input_interface_missing_agent_configs(mock_logger):
    """Test input interface with missing agent configurations."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    # Only provide planner config, missing researcher and expert
    agent_configs = {"planner": mock_planner_config}
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        file=None
    )
    
    # Execute input interface and expect KeyError
    with pytest.raises(KeyError) as exc_info:
        input_interface(test_state)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Input interface starting execution")

@patch('multi_agent_system.logger')
def test_input_interface_invalid_agent_configs(mock_logger):
    """Test input interface with invalid agent configurations."""
    # Setup invalid agent configs (missing retry_limit attribute)
    agent_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock()
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        file=None
    )
    
    # Execute input interface and expect AttributeError
    with pytest.raises(AttributeError) as exc_info:
        input_interface(test_state)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Input interface starting execution")

@patch('multi_agent_system.logger')
def test_input_interface_logger_failure(mock_logger):
    """Test input interface when logger fails."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Mock logger to raise exception
    mock_logger.info.side_effect = Exception("Logger error")
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        file=None
    )
    
    # Execute input interface and expect exception
    with pytest.raises(Exception) as exc_info:
        input_interface(test_state)
    
    assert "Logger error" in str(exc_info.value)

@patch('multi_agent_system.logger')
def test_input_interface_return_type_validation(mock_logger):
    """Test that returned function has correct return type structure."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        file=None
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify return type structure
    assert result is test_state
    assert isinstance(result, dict)  # GraphState is a TypedDict

@patch('multi_agent_system.logger')
def test_input_interface_state_preservation(mock_logger):
    """Test that existing state fields are preserved correctly."""
    # Setup mocks
    mock_planner_config = Mock()
    mock_planner_config.retry_limit = 3
    
    mock_researcher_config = Mock()
    mock_researcher_config.retry_limit = 2
    
    mock_expert_config = Mock()
    mock_expert_config.retry_limit = 1
    
    agent_configs = {
        "planner": mock_planner_config,
        "researcher": mock_researcher_config,
        "expert": mock_expert_config
    }
    
    # Create input interface
    input_interface = create_input_interface(agent_configs)
    
    # Setup test state with existing data
    test_state = GraphState(
        question="Test question",
        file="existing_file.pdf",
        research_steps=["existing_step"],
        expert_steps=["existing_expert_step"],
        current_research_index=5,
        researcher_states={"existing": "state"},
        research_results=["existing_result"],
        expert_state={"existing": "expert_state"},
        expert_answer="existing answer",
        expert_reasoning="existing reasoning",
        critic_planner_decision="existing decision",
        critic_planner_feedback="existing feedback",
        critic_researcher_decision="existing decision",
        critic_researcher_feedback="existing feedback",
        critic_expert_decision="existing decision",
        critic_expert_feedback="existing feedback",
        final_answer="existing final answer",
        final_reasoning_trace="existing reasoning trace",
        agent_messages=["existing message"],
        current_step="existing_step",
        next_step="existing_next_step",
        planner_retry_count=10,
        researcher_retry_count=5,
        expert_retry_count=2,
        planner_retry_limit=100,
        researcher_retry_limit=50,
        expert_retry_limit=25
    )
    
    # Execute input interface
    result = input_interface(test_state)
    
    # Verify that all fields are overwritten with default values
    assert test_state["file"] == "existing_file.pdf"  # Preserved if not None
    assert test_state["research_steps"] == []  # Overwritten
    assert test_state["expert_steps"] == []  # Overwritten
    assert test_state["current_research_index"] == -1  # Overwritten
    assert test_state["researcher_states"] == {}  # Overwritten
    assert test_state["research_results"] == []  # Overwritten
    assert test_state["expert_state"] is None  # Overwritten
    assert test_state["expert_answer"] == ""  # Overwritten
    assert test_state["expert_reasoning"] == ""  # Overwritten
    assert test_state["critic_planner_decision"] == ""  # Overwritten
    assert test_state["critic_planner_feedback"] == ""  # Overwritten
    assert test_state["critic_researcher_decision"] == ""  # Overwritten
    assert test_state["critic_researcher_feedback"] == ""  # Overwritten
    assert test_state["critic_expert_decision"] == ""  # Overwritten
    assert test_state["critic_expert_feedback"] == ""  # Overwritten
    assert test_state["final_answer"] == ""  # Overwritten
    assert test_state["final_reasoning_trace"] == ""  # Overwritten
    assert test_state["agent_messages"] == []  # Overwritten
    assert test_state["current_step"] == "input"  # Overwritten
    assert test_state["next_step"] == "planner"  # Overwritten
    assert test_state["planner_retry_count"] == 0  # Overwritten
    assert test_state["researcher_retry_count"] == 0  # Overwritten
    assert test_state["expert_retry_count"] == 0  # Overwritten
    assert test_state["planner_retry_limit"] == 3  # Overwritten with config value
    assert test_state["researcher_retry_limit"] == 2  # Overwritten with config value
    assert test_state["expert_retry_limit"] == 1  # Overwritten with config value
```
#### 3.7.2 Next Step Determination

**Function Information:**
- **Function Name**: `determine_next_step`
- **Location**: `multi_agent_system.py:548`
- **Purpose**: Orchestrator logic to determine the next step based on current step and critic decisions, handling critic decisions, initial state, non-critic steps, and retry counter incrementation

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing current_step, critic decisions, research steps, and retry counts
- **Return**: `GraphState` - The updated state with next_step determined and retry counts potentially incremented
- **Side Effects**: Modifies the input state by setting next_step and incrementing retry counts

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - None - This function is purely state-based logic with no external dependencies
- **Dependencies to Use Directly**: 
  - `GraphState` - Internal TypedDict type
  - Dictionary access and assignment - Built-in Python operations
  - String comparisons and equality checks - Built-in Python features
  - List length checking (`len()`) - Built-in Python function
  - Integer arithmetic and comparison - Built-in Python operations
  - Conditional logic and control flow - Built-in Python features
- **Mock Configuration**: No mocking required - function operates entirely on state data

**Test Cases:**

**Happy Path:**
- **Critic Planner Approve with Research Steps**: Planner approved with existing research steps
- **Critic Planner Approve without Research Steps**: Planner approved with no research steps
- **Critic Planner Reject**: Planner rejected, triggering retry
- **Critic Researcher Approve with More Steps**: Researcher approved with remaining research steps
- **Critic Researcher Approve with No More Steps**: Researcher approved with all steps completed
- **Critic Researcher Reject**: Researcher rejected, triggering retry
- **Critic Expert Approve**: Expert approved, proceeding to finalizer
- **Critic Expert Reject**: Expert rejected, triggering retry
- **Initial State**: Empty or "input" current step
- **Planner Step**: Current step is "planner"
- **Researcher Step**: Current step is "researcher"
- **Expert Step**: Current step is "expert"

**Edge Cases:**
- **Empty Research Steps List**: Research steps list is empty
- **Single Research Step**: Only one research step exists
- **Many Research Steps**: Large number of research steps
- **Zero Retry Counts**: All retry counts at zero
- **High Retry Counts**: Retry counts near limits
- **Unknown Current Step**: Current step not in expected values
- **Missing State Fields**: Some state fields missing or None

**Error Conditions:**
- **Invalid Critic Decision**: Critic decision not "approve" or "reject"
- **Missing Critic Decision**: Critic decision field missing or None
- **Invalid Current Step**: Current step with unexpected value
- **Missing Research Steps**: Research steps field missing
- **Invalid Research Index**: Current research index out of bounds
- **Missing Retry Counts**: Retry count fields missing
- **Negative Retry Counts**: Retry counts with negative values

**State Changes:**
- **Next Step Assignment**: next_step field is set based on logic
- **Retry Count Incrementation**: Retry counts incremented on rejections
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned

**Mock Configurations:**
```python
# No mocking required - function operates entirely on state data
# All test scenarios use direct state manipulation
def test_determine_next_step_no_mocking_required():
    """Function operates purely on state data, no external dependencies."""
    # Direct state creation and manipulation
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="approve",
        research_steps=["step1", "step2"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Direct function call
    result = determine_next_step(test_state)
    
    # Direct assertions
    assert result["next_step"] == "researcher"
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and state operations in the implementation:
def determine_next_step(state: GraphState) -> GraphState:  # Direct type annotations
    """Orchestrator logic to determine the next step based on current step and critic decisions."""
    
    # Handle critic decisions and set next step accordingly
    if state["current_step"] == "critic_planner":  # Direct dictionary access and string comparison
        if state["critic_planner_decision"] == "approve":  # Direct dictionary access and string comparison
            if len(state["research_steps"]) > 0:  # Direct list length checking
                state["next_step"] = "researcher"  # Direct dictionary assignment
            else:
                state["next_step"] = "expert"  # Direct dictionary assignment
        elif state["critic_planner_decision"] == "reject":  # Direct dictionary access and string comparison
            state["next_step"] = "planner"  # Direct dictionary assignment
            state["planner_retry_count"] += 1  # Direct integer arithmetic and assignment
    
    elif state["current_step"] == "critic_researcher":  # Direct dictionary access and string comparison
        if state["critic_researcher_decision"] == "approve":  # Direct dictionary access and string comparison
            # Check if all research steps are completed
            if state["current_research_index"] + 1 >= len(state["research_steps"]):  # Direct integer arithmetic and list length checking
                state["next_step"] = "expert"  # Direct dictionary assignment
            else:
                state["next_step"] = "researcher"  # Direct dictionary assignment
        elif state["critic_researcher_decision"] == "reject":  # Direct dictionary access and string comparison
            state["next_step"] = "researcher"  # Direct dictionary assignment
            state["researcher_retry_count"] += 1  # Direct integer arithmetic and assignment
    
    elif state["current_step"] == "critic_expert":  # Direct dictionary access and string comparison
        if state["critic_expert_decision"] == "approve":  # Direct dictionary access and string comparison
            state["next_step"] = "finalizer"  # Direct dictionary assignment
        elif state["critic_expert_decision"] == "reject":  # Direct dictionary access and string comparison
            state["next_step"] = "expert"  # Direct dictionary assignment
            state["expert_retry_count"] += 1  # Direct integer arithmetic and assignment
    
    # Handle initial state and non-critic steps
    elif state["current_step"] == "" or state["current_step"] == "input":  # Direct string comparison and logical OR
        state["next_step"] = "planner"  # Direct dictionary assignment
    elif state["current_step"] == "planner":  # Direct dictionary access and string comparison
        state["next_step"] = "critic_planner"  # Direct dictionary assignment
    elif state["current_step"] == "researcher":  # Direct dictionary access and string comparison
        state["next_step"] = "critic_researcher"  # Direct dictionary assignment
    elif state["current_step"] == "expert":  # Direct dictionary access and string comparison
        state["next_step"] = "critic_expert"  # Direct dictionary assignment
    
    return state  # Direct return
```

**Assertion Specifications:**
- Verify next_step is set correctly based on current_step and critic decisions
- Verify retry counts are incremented only on rejections
- Verify state object identity is preserved (same object returned)
- Verify other state fields remain unchanged
- Verify proper handling of research step completion logic
- Verify correct flow from initial state to planner
- Verify proper critic decision handling for all agent types

**Code Examples:**
```python
import pytest
from multi_agent_system import determine_next_step, GraphState

def test_critic_planner_approve_with_research_steps():
    """Test critic planner approve with existing research steps."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="approve",
        research_steps=["step1", "step2", "step3"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "researcher"
    
    # Verify retry count is not incremented
    assert test_state["planner_retry_count"] == 0
    
    # Verify other fields remain unchanged
    assert test_state["current_step"] == "critic_planner"
    assert test_state["critic_planner_decision"] == "approve"
    assert test_state["research_steps"] == ["step1", "step2", "step3"]

def test_critic_planner_approve_without_research_steps():
    """Test critic planner approve with no research steps."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="approve",
        research_steps=[],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "expert"
    
    # Verify retry count is not incremented
    assert test_state["planner_retry_count"] == 0

def test_critic_planner_reject():
    """Test critic planner reject."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="reject",
        research_steps=["step1", "step2"],
        next_step="",
        planner_retry_count=2,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "planner"
    
    # Verify retry count is incremented
    assert test_state["planner_retry_count"] == 3

def test_critic_researcher_approve_with_more_steps():
    """Test critic researcher approve with remaining research steps."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="approve",
        research_steps=["step1", "step2", "step3"],
        current_research_index=0,  # First step completed, 2 more remaining
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=1,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly (more steps remaining)
    assert test_state["next_step"] == "researcher"
    
    # Verify retry count is not incremented
    assert test_state["researcher_retry_count"] == 1

def test_critic_researcher_approve_with_no_more_steps():
    """Test critic researcher approve with all research steps completed."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="approve",
        research_steps=["step1", "step2", "step3"],
        current_research_index=2,  # Last step completed (index 2 of 3 steps)
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly (no more steps)
    assert test_state["next_step"] == "expert"
    
    # Verify retry count is not incremented
    assert test_state["researcher_retry_count"] == 0

def test_critic_researcher_reject():
    """Test critic researcher reject."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="reject",
        research_steps=["step1", "step2"],
        current_research_index=1,
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=1,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "researcher"
    
    # Verify retry count is incremented
    assert test_state["researcher_retry_count"] == 2

def test_critic_expert_approve():
    """Test critic expert approve."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_expert",
        critic_expert_decision="approve",
        research_steps=["step1", "step2"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "finalizer"
    
    # Verify retry count is not incremented
    assert test_state["expert_retry_count"] == 0

def test_critic_expert_reject():
    """Test critic expert reject."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_expert",
        critic_expert_decision="reject",
        research_steps=["step1", "step2"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=2
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "expert"
    
    # Verify retry count is incremented
    assert test_state["expert_retry_count"] == 3

def test_initial_state_empty():
    """Test initial state with empty current_step."""
    # Setup test state
    test_state = GraphState(
        current_step="",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "planner"

def test_initial_state_input():
    """Test initial state with 'input' current_step."""
    # Setup test state
    test_state = GraphState(
        current_step="input",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "planner"

def test_planner_step():
    """Test current step is 'planner'."""
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "critic_planner"

def test_researcher_step():
    """Test current step is 'researcher'."""
    # Setup test state
    test_state = GraphState(
        current_step="researcher",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "critic_researcher"

def test_expert_step():
    """Test current step is 'expert'."""
    # Setup test state
    test_state = GraphState(
        current_step="expert",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "critic_expert"

def test_edge_case_empty_research_steps():
    """Test edge case with empty research steps list."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="approve",
        research_steps=[],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly (empty list should go to expert)
    assert test_state["next_step"] == "expert"

def test_edge_case_single_research_step():
    """Test edge case with single research step."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="approve",
        research_steps=["single_step"],
        current_research_index=0,  # First and only step completed
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly (no more steps)
    assert test_state["next_step"] == "expert"

def test_edge_case_many_research_steps():
    """Test edge case with many research steps."""
    # Setup test state with many steps
    many_steps = [f"step_{i}" for i in range(100)]
    test_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="approve",
        research_steps=many_steps,
        current_research_index=50,  # Middle step completed
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly (more steps remaining)
    assert test_state["next_step"] == "researcher"

def test_edge_case_zero_retry_counts():
    """Test edge case with zero retry counts."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="reject",
        research_steps=["step1"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify retry count is incremented from zero
    assert test_state["planner_retry_count"] == 1

def test_edge_case_high_retry_counts():
    """Test edge case with high retry counts."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_expert",
        critic_expert_decision="reject",
        research_steps=["step1"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=999
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify retry count is incremented from high value
    assert test_state["expert_retry_count"] == 1000

def test_error_case_invalid_critic_decision():
    """Test error case with invalid critic decision."""
    # Setup test state with invalid decision
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="maybe",  # Invalid decision
        research_steps=["step1"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is not changed (no matching condition)
    assert test_state["next_step"] == ""
    
    # Verify retry count is not incremented
    assert test_state["planner_retry_count"] == 0

def test_error_case_missing_critic_decision():
    """Test error case with missing critic decision."""
    # Setup test state without critic decision
    test_state = GraphState(
        current_step="critic_planner",
        research_steps=["step1"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is not changed (no matching condition)
    assert test_state["next_step"] == ""
    
    # Verify retry count is not incremented
    assert test_state["planner_retry_count"] == 0

def test_error_case_unknown_current_step():
    """Test error case with unknown current step."""
    # Setup test state with unknown step
    test_state = GraphState(
        current_step="unknown_step",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is not changed (no matching condition)
    assert test_state["next_step"] == ""

def test_error_case_missing_research_steps():
    """Test error case with missing research steps field."""
    # Setup test state without research steps
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="approve",
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function and expect AttributeError or KeyError
    with pytest.raises((KeyError, AttributeError)):
        determine_next_step(test_state)

def test_error_case_invalid_research_index():
    """Test error case with invalid research index."""
    # Setup test state with invalid index
    test_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="approve",
        research_steps=["step1", "step2"],
        current_research_index=5,  # Index out of bounds
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0
    )
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly (index 5 >= len 2, so go to expert)
    assert test_state["next_step"] == "expert"

def test_state_preservation():
    """Test that other state fields are preserved."""
    # Setup test state with various fields
    test_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="approve",
        research_steps=["step1"],
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        question="Test question",
        file="test.pdf",
        expert_answer="existing answer",
        final_answer="existing final"
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_expert_answer = test_state["expert_answer"]
    original_final_answer = test_state["final_answer"]
    
    # Execute function
    result = determine_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is set correctly
    assert test_state["next_step"] == "expert"
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["expert_answer"] == original_expert_answer
    assert test_state["final_answer"] == original_final_answer

def test_retry_count_incrementation_logic():
    """Test that retry counts are only incremented on rejections."""
    # Test planner rejection
    planner_state = GraphState(
        current_step="critic_planner",
        critic_planner_decision="reject",
        research_steps=["step1"],
        next_step="",
        planner_retry_count=5,
        researcher_retry_count=3,
        expert_retry_count=1
    )
    
    result = determine_next_step(planner_state)
    assert result["planner_retry_count"] == 6
    assert result["researcher_retry_count"] == 3  # Unchanged
    assert result["expert_retry_count"] == 1  # Unchanged
    
    # Test researcher rejection
    researcher_state = GraphState(
        current_step="critic_researcher",
        critic_researcher_decision="reject",
        research_steps=["step1"],
        current_research_index=0,
        next_step="",
        planner_retry_count=5,
        researcher_retry_count=3,
        expert_retry_count=1
    )
    
    result = determine_next_step(researcher_state)
    assert result["planner_retry_count"] == 5  # Unchanged
    assert result["researcher_retry_count"] == 4  # Incremented
    assert result["expert_retry_count"] == 1  # Unchanged
    
    # Test expert rejection
    expert_state = GraphState(
        current_step="critic_expert",
        critic_expert_decision="reject",
        research_steps=["step1"],
        next_step="",
        planner_retry_count=5,
        researcher_retry_count=3,
        expert_retry_count=1
    )
    
    result = determine_next_step(expert_state)
    assert result["planner_retry_count"] == 5  # Unchanged
    assert result["researcher_retry_count"] == 3  # Unchanged
    assert result["expert_retry_count"] == 2  # Incremented
```
#### 3.7.3 Retry Limit Checking

**Function Information:**
- **Function Name**: `check_retry_limit`
- **Location**: `multi_agent_system.py:603`
- **Purpose**: Orchestrator logic to check retry count and handle limit exceeded, setting next step to finalizer and providing graceful failure messages when any retry limit is exceeded

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing retry counts, retry limits, next_step, and final answer fields
- **Return**: `GraphState` - The updated state with potential changes to next_step, final_answer, and final_reasoning_trace
- **Side Effects**: Modifies the input state by setting next_step to "finalizer" and failure messages when retry limits are exceeded

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
- **Dependencies to Use Directly**: 
  - `GraphState` - Internal TypedDict type
  - Dictionary access and assignment - Built-in Python operations
  - Integer comparison operators (`>=`) - Built-in Python features
  - Logical OR operator (`or`) - Built-in Python features
  - String formatting and assignment - Built-in Python operations
  - Conditional logic and control flow - Built-in Python features
- **Mock Configuration**: Mock logger for consistent testing and to verify logging calls

**Test Cases:**

**Happy Path:**
- **No Retry Limits Exceeded**: All retry counts below their respective limits
- **Single Retry Limit Exceeded**: One agent's retry count equals its limit
- **Multiple Retry Limits Exceeded**: Multiple agents' retry counts equal their limits
- **All Retry Limits Exceeded**: All agents' retry counts equal their limits

**Edge Cases:**
- **Zero Retry Limits**: All retry limits set to zero
- **Equal Retry Counts and Limits**: Retry counts exactly equal to limits
- **High Retry Limits**: Very large retry limit values
- **Negative Retry Counts**: Retry counts with negative values (should not trigger limit)
- **Missing Retry Fields**: Some retry count or limit fields missing
- **Different Next Step Values**: Various next_step values when limit is exceeded

**Error Conditions:**
- **Missing Retry Counts**: Retry count fields missing from state
- **Missing Retry Limits**: Retry limit fields missing from state
- **Invalid Retry Values**: Non-integer retry counts or limits
- **Logger Failure**: Errors in logging operations
- **Missing Next Step**: Next step field missing when limit is exceeded

**State Changes:**
- **No Limit Exceeded**: State remains unchanged
- **Limit Exceeded**: next_step set to "finalizer", final_answer and final_reasoning_trace set to failure messages
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned

**Mock Configurations:**
```python
@patch('multi_agent_system.logger')
def test_check_retry_limit_with_mocked_logger(mock_logger):
    """Mock logger for consistent testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="planner"
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for planner")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and state operations in the implementation:
def check_retry_limit(state: GraphState) -> GraphState:  # Direct type annotations
    """Orchestrator logic to check retry count and handle limit exceeded."""
    
    if (state["planner_retry_count"] >= state["planner_retry_limit"]  # Direct dictionary access and integer comparison
        or state["researcher_retry_count"] >= state["researcher_retry_limit"]  # Direct dictionary access and integer comparison
        or state["expert_retry_count"] >= state["expert_retry_limit"]):  # Direct dictionary access and integer comparison
        # Log the failure
        logger.info(f"Graceful Failure: Retry limit exceeded for {state['next_step']}")  # Direct logging call with f-string formatting
        state["next_step"] = "finalizer"  # Direct dictionary assignment
        state["final_answer"] = "The question could not be answered."  # Direct string assignment
        state["final_reasoning_trace"] = "The question could not be answered."  # Direct string assignment
    return state  # Direct return
```

**Assertion Specifications:**
- Verify retry limit checking logic works correctly for all combinations
- Verify state changes occur only when limits are exceeded
- Verify logging calls are made with correct messages when limits are exceeded
- Verify state object identity is preserved (same object returned)
- Verify other state fields remain unchanged
- Verify proper handling of edge cases (zero limits, equal counts/limits)
- Verify error conditions are properly handled

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import check_retry_limit, GraphState

@patch('multi_agent_system.logger')
def test_no_retry_limits_exceeded(mock_logger):
    """Test when no retry limits are exceeded."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=1,
        planner_retry_limit=3,
        researcher_retry_count=0,
        researcher_retry_limit=2,
        expert_retry_count=1,
        expert_retry_limit=5,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state remains unchanged
    assert test_state["next_step"] == "planner"
    assert test_state["final_answer"] == ""
    assert test_state["final_reasoning_trace"] == ""
    
    # Verify logging was not called
    mock_logger.info.assert_not_called()

@patch('multi_agent_system.logger')
def test_planner_retry_limit_exceeded(mock_logger):
    """Test when planner retry limit is exceeded."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called with correct message
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for planner")

@patch('multi_agent_system.logger')
def test_researcher_retry_limit_exceeded(mock_logger):
    """Test when researcher retry limit is exceeded."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=1,
        planner_retry_limit=3,
        researcher_retry_count=2,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="researcher",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called with correct message
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for researcher")

@patch('multi_agent_system.logger')
def test_expert_retry_limit_exceeded(mock_logger):
    """Test when expert retry limit is exceeded."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=1,
        planner_retry_limit=3,
        researcher_retry_count=0,
        researcher_retry_limit=2,
        expert_retry_count=1,
        expert_retry_limit=1,
        next_step="expert",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called with correct message
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for expert")

@patch('multi_agent_system.logger')
def test_multiple_retry_limits_exceeded(mock_logger):
    """Test when multiple retry limits are exceeded."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=2,
        researcher_retry_limit=2,
        expert_retry_count=1,
        expert_retry_limit=1,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called (should be called once for the first condition that matches)
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for planner")

@patch('multi_agent_system.logger')
def test_all_retry_limits_exceeded(mock_logger):
    """Test when all retry limits are exceeded."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=5,
        planner_retry_limit=5,
        researcher_retry_count=3,
        researcher_retry_limit=3,
        expert_retry_count=2,
        expert_retry_limit=2,
        next_step="expert",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called (should be called once for the first condition that matches)
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for expert")

@patch('multi_agent_system.logger')
def test_edge_case_zero_retry_limits(mock_logger):
    """Test edge case with zero retry limits."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=0,
        planner_retry_limit=0,
        researcher_retry_count=0,
        researcher_retry_limit=0,
        expert_retry_count=0,
        expert_retry_limit=0,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded (0 >= 0 is True)
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for planner")

@patch('multi_agent_system.logger')
def test_edge_case_equal_retry_counts_and_limits(mock_logger):
    """Test edge case with retry counts exactly equal to limits."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=5,
        planner_retry_limit=5,
        researcher_retry_count=10,
        researcher_retry_limit=10,
        expert_retry_count=1,
        expert_retry_limit=1,
        next_step="researcher",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded (5 >= 5 is True)
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for researcher")

@patch('multi_agent_system.logger')
def test_edge_case_high_retry_limits(mock_logger):
    """Test edge case with very high retry limits."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=1000,
        planner_retry_limit=1000,
        researcher_retry_count=500,
        researcher_retry_limit=500,
        expert_retry_count=250,
        expert_retry_limit=250,
        next_step="expert",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated for limit exceeded
    assert test_state["next_step"] == "finalizer"
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for expert")

@patch('multi_agent_system.logger')
def test_edge_case_negative_retry_counts(mock_logger):
    """Test edge case with negative retry counts."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=-1,
        planner_retry_limit=3,
        researcher_retry_count=-5,
        researcher_retry_limit=2,
        expert_retry_count=-10,
        expert_retry_limit=1,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state remains unchanged (negative counts should not trigger limit)
    assert test_state["next_step"] == "planner"
    assert test_state["final_answer"] == ""
    assert test_state["final_reasoning_trace"] == ""
    
    # Verify logging was not called
    mock_logger.info.assert_not_called()

@patch('multi_agent_system.logger')
def test_edge_case_different_next_step_values(mock_logger):
    """Test edge case with different next_step values when limit is exceeded."""
    # Test with "critic_planner" next_step
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="critic_planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    result = check_retry_limit(test_state)
    assert result["next_step"] == "finalizer"
    mock_logger.info.assert_called_with("Graceful Failure: Retry limit exceeded for critic_planner")
    
    # Reset mock for next test
    mock_logger.reset_mock()
    
    # Test with "finalizer" next_step
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="finalizer",
        final_answer="",
        final_reasoning_trace=""
    )
    
    result = check_retry_limit(test_state)
    assert result["next_step"] == "finalizer"
    mock_logger.info.assert_called_with("Graceful Failure: Retry limit exceeded for finalizer")

@patch('multi_agent_system.logger')
def test_error_case_missing_retry_counts(mock_logger):
    """Test error case with missing retry count fields."""
    # Setup test state without retry counts
    test_state = GraphState(
        planner_retry_limit=3,
        researcher_retry_limit=2,
        expert_retry_limit=1,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        check_retry_limit(test_state)
    
    # Verify logging was not called
    mock_logger.info.assert_not_called()

@patch('multi_agent_system.logger')
def test_error_case_missing_retry_limits(mock_logger):
    """Test error case with missing retry limit fields."""
    # Setup test state without retry limits
    test_state = GraphState(
        planner_retry_count=1,
        researcher_retry_count=0,
        expert_retry_count=1,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        check_retry_limit(test_state)
    
    # Verify logging was not called
    mock_logger.info.assert_not_called()

@patch('multi_agent_system.logger')
def test_error_case_logger_failure(mock_logger):
    """Test error case when logger fails."""
    # Setup test state
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="planner",
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Mock logger to raise exception
    mock_logger.info.side_effect = Exception("Logger error")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        check_retry_limit(test_state)
    
    assert "Logger error" in str(exc_info.value)

@patch('multi_agent_system.logger')
def test_error_case_missing_next_step(mock_logger):
    """Test error case with missing next_step when limit is exceeded."""
    # Setup test state without next_step
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        final_answer="",
        final_reasoning_trace=""
    )
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        check_retry_limit(test_state)
    
    # Verify logging was not called
    mock_logger.info.assert_not_called()

@patch('multi_agent_system.logger')
def test_state_preservation_when_limit_not_exceeded(mock_logger):
    """Test that other state fields are preserved when limit is not exceeded."""
    # Setup test state with various fields
    test_state = GraphState(
        planner_retry_count=1,
        planner_retry_limit=3,
        researcher_retry_count=0,
        researcher_retry_limit=2,
        expert_retry_count=1,
        expert_retry_limit=5,
        next_step="planner",
        final_answer="existing answer",
        final_reasoning_trace="existing reasoning",
        question="Test question",
        file="test.pdf",
        current_step="input"
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_current_step = test_state["current_step"]
    original_final_answer = test_state["final_answer"]
    original_final_reasoning = test_state["final_reasoning_trace"]
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step remains unchanged
    assert test_state["next_step"] == "planner"
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["current_step"] == original_current_step
    assert test_state["final_answer"] == original_final_answer
    assert test_state["final_reasoning_trace"] == original_final_reasoning
    
    # Verify logging was not called
    mock_logger.info.assert_not_called()

@patch('multi_agent_system.logger')
def test_state_preservation_when_limit_exceeded(mock_logger):
    """Test that other state fields are preserved when limit is exceeded."""
    # Setup test state with various fields
    test_state = GraphState(
        planner_retry_count=3,
        planner_retry_limit=3,
        researcher_retry_count=1,
        researcher_retry_limit=2,
        expert_retry_count=0,
        expert_retry_limit=1,
        next_step="planner",
        final_answer="existing answer",
        final_reasoning_trace="existing reasoning",
        question="Test question",
        file="test.pdf",
        current_step="input"
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_current_step = test_state["current_step"]
    
    # Execute function
    result = check_retry_limit(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify next_step is updated
    assert test_state["next_step"] == "finalizer"
    
    # Verify final answer fields are updated
    assert test_state["final_answer"] == "The question could not be answered."
    assert test_state["final_reasoning_trace"] == "The question could not be answered."
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["current_step"] == original_current_step
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Graceful Failure: Retry limit exceeded for planner")

@patch('multi_agent_system.logger')
def test_retry_limit_logic_combinations(mock_logger):
    """Test various combinations of retry limit logic."""
    # Test: planner exceeded, others not
    test_state = GraphState(
        planner_retry_count=5,
        planner_retry_limit=5,
        researcher_retry_count=1,
        researcher_retry_limit=3,
        expert_retry_count=0,
        expert_retry_limit=2,
        next_step="planner"
    )
    
    result = check_retry_limit(test_state)
    assert result["next_step"] == "finalizer"
    mock_logger.info.assert_called_with("Graceful Failure: Retry limit exceeded for planner")
    
    # Reset mock for next test
    mock_logger.reset_mock()
    
    # Test: researcher exceeded, others not
    test_state = GraphState(
        planner_retry_count=2,
        planner_retry_limit=5,
        researcher_retry_count=3,
        researcher_retry_limit=3,
        expert_retry_count=0,
        expert_retry_limit=2,
        next_step="researcher"
    )
    
    result = check_retry_limit(test_state)
    assert result["next_step"] == "finalizer"
    mock_logger.info.assert_called_with("Graceful Failure: Retry limit exceeded for researcher")
    
    # Reset mock for next test
    mock_logger.reset_mock()
    
    # Test: expert exceeded, others not
    test_state = GraphState(
        planner_retry_count=2,
        planner_retry_limit=5,
        researcher_retry_count=1,
        researcher_retry_limit=3,
        expert_retry_count=2,
        expert_retry_limit=2,
        next_step="expert"
    )
    
    result = check_retry_limit(test_state)
    assert result["next_step"] == "finalizer"
    mock_logger.info.assert_called_with("Graceful Failure: Retry limit exceeded for expert")
```
#### 3.7.4 Next Step Execution

**Function Information:**
- **Function Name**: `execute_next_step`
- **Location**: `multi_agent_system.py:625`
- **Purpose**: Orchestrator logic to execute the next step by setting current_step and sending appropriate messages to agents based on the current step and critic decisions

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing next_step, current_step, critic decisions, and various state fields
- **Return**: `GraphState` - The updated state with current_step set and message sent
- **Side Effects**: Modifies the input state by setting current_step and calling send_message to add messages to agent_messages

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `compose_agent_message()` - Internal function for creating agent messages
  - `send_message()` - Internal function for adding messages to state
- **Dependencies to Use Directly**: 
  - `GraphState` - Internal TypedDict type
  - Dictionary access and assignment - Built-in Python operations
  - String formatting and concatenation - Built-in Python features
  - String operations (`join()`) - Built-in Python methods
  - Conditional logic and control flow - Built-in Python features
  - List indexing and access - Built-in Python operations
  - Integer arithmetic (`+=`) - Built-in Python operations
  - Exception raising (`ValueError`) - Built-in Python features
- **Mock Configuration**: Mock compose_agent_message and send_message to verify correct message creation and sending

**Test Cases:**

**Happy Path:**
- **Planner Step - Initial Request**: Planner step with no previous critic decision
- **Planner Step - Retry with Feedback**: Planner step with critic_planner_decision "reject"
- **Critic Planner Step**: Critic planner step with research and expert steps
- **Researcher Step - Initial Request**: Researcher step with no previous critic decision
- **Researcher Step - Retry with Feedback**: Researcher step with critic_researcher_decision "reject"
- **Critic Researcher Step**: Critic researcher step with research topic and results
- **Expert Step - Initial Request**: Expert step with no previous critic decision
- **Expert Step - Retry with Feedback**: Expert step with critic_expert_decision "reject"
- **Critic Expert Step**: Critic expert step with expert answer and reasoning
- **Finalizer Step**: Finalizer step

**Edge Cases:**
- **Empty Research Steps**: Research steps list is empty
- **Empty Expert Steps**: Expert steps list is empty
- **Empty Research Results**: Research results list is empty
- **File with Multiple Lines**: File field with multiple lines
- **No File**: File field is None or empty
- **High Research Index**: Very high current_research_index value
- **Empty Critic Feedback**: Critic feedback fields are empty
- **Long Content**: Very long question, feedback, or content strings

**Error Conditions:**
- **Invalid Current Step**: Current step not in expected values
- **Missing Required Fields**: Missing fields needed for message composition
- **Invalid Research Index**: Current_research_index out of bounds
- **Missing Critic Decisions**: Critic decision fields missing
- **Missing Research Steps**: Research steps field missing
- **Missing Expert Steps**: Expert steps field missing
- **Missing Research Results**: Research results field missing
- **Compose Message Failure**: Errors in compose_agent_message function
- **Send Message Failure**: Errors in send_message function

**State Changes:**
- **Current Step Assignment**: current_step is set to next_step value
- **Research Index Increment**: current_research_index incremented for new researcher steps
- **Message Addition**: Message added to agent_messages via send_message
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned

**Mock Configurations:**
```python
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_execute_next_step_with_mocked_dependencies(mock_compose_message, mock_send_message):
    """Mock internal functions for testing."""
    # Mock compose_agent_message to return a test message
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    
    # Mock send_message to return the state
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Test question",
        file=None
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify compose_agent_message was called
    mock_compose_message.assert_called_once()
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and state operations in the implementation:
def execute_next_step(state: GraphState) -> GraphState:  # Direct type annotations
    """Orchestrator logic to execute the next step by setting current_step and sending message."""
    
    # Set current_step = next_step
    state["current_step"] = state["next_step"]  # Direct dictionary assignment
    
    # Send appropriate message based on the current step
    if state["current_step"] == "planner":  # Direct dictionary access and string comparison
        if state["critic_planner_decision"] == "reject":  # Direct dictionary access and string comparison
            # Retry with feedback
            message = compose_agent_message(  # Direct function call
                sender= "orchestrator",
                receiver= "planner",
                type= "instruction",
                content= f"Use the following feedback to improve the plan:\n{state['critic_planner_feedback']}",  # Direct f-string formatting
            )
        else:
            # Initial planning request
            file_info = ""  # Direct string assignment
            if state["file"]:  # Direct dictionary access and truthiness check
                file_info = "\n\nInclude using following file in any of the research steps:" + "\n".join(state["file"])  # Direct string concatenation and join operation
            
            message = compose_agent_message(  # Direct function call
                sender= "orchestrator",
                receiver= "planner",
                type= "instruction",
                content= f"Develop a logical plan to answer the following question:\n{state['question']}{file_info}",  # Direct f-string formatting
            )
    
    elif state["current_step"] == "researcher":  # Direct dictionary access and string comparison
        if state["critic_researcher_decision"] == "reject":  # Direct dictionary access and string comparison
            # Retry with feedback
            message = compose_agent_message(  # Direct function call
                sender= "orchestrator",
                receiver= "researcher",
                type= "instruction",
                content= f"Use the following feedback to improve the research:\n{state["critic_researcher_feedback"]}",  # Direct f-string formatting
                step_id=state["current_research_index"],  # Direct dictionary access
            )
        else:
            # Increment research index for new step
            state["current_research_index"] += 1  # Direct integer arithmetic and assignment
            
            message = compose_agent_message(  # Direct function call
                sender= "orchestrator",
                receiver= "researcher",
                type= "instruction",
                content= f"Research the following topic or question: {state['research_steps'][state['current_research_index']]}",  # Direct f-string formatting and list indexing
                step_id= state["current_research_index"],  # Direct dictionary access
            )
    
    else:
        raise ValueError(f"Invalid current step: {state['current_step']}")  # Direct exception raising with f-string formatting
    
    # Send the message
    send_message(state, message)  # Direct function call
    return state  # Direct return
```

**Assertion Specifications:**
- Verify current_step is set to next_step value
- Verify compose_agent_message is called with correct parameters for each step type
- Verify send_message is called with correct state and message
- Verify research index is incremented only for new researcher steps
- Verify state object identity is preserved (same object returned)
- Verify other state fields remain unchanged
- Verify proper handling of critic decisions (reject vs. approve)
- Verify proper message content construction for each step type
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import execute_next_step, GraphState

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_planner_step_initial_request(mock_compose_message, mock_send_message):
    """Test planner step with initial request (no previous critic decision)."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="What is the capital of France?",
        file=None,
        research_steps=[],
        expert_steps=[],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "planner"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="planner",
        type="instruction",
        content="Develop a logical plan to answer the following question:\nWhat is the capital of France?"
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_planner_step_retry_with_feedback(mock_compose_message, mock_send_message):
    """Test planner step with retry feedback."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="What is the capital of France?",
        file=None,
        critic_planner_decision="reject",
        critic_planner_feedback="The plan needs more specific research steps",
        research_steps=[],
        expert_steps=[],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "planner"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="planner",
        type="instruction",
        content="Use the following feedback to improve the plan:\nThe plan needs more specific research steps"
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_planner_step_with_file(mock_compose_message, mock_send_message):
    """Test planner step with file attachment."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state with file
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Analyze this document",
        file=["document.pdf", "data.xlsx"],
        research_steps=[],
        expert_steps=[],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "planner"
    
    # Verify compose_agent_message was called with correct parameters including file info
    expected_content = "Develop a logical plan to answer the following question:\nAnalyze this document\n\nInclude using following file in any of the research steps:document.pdf\ndata.xlsx"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="planner",
        type="instruction",
        content=expected_content
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_critic_planner_step(mock_compose_message, mock_send_message):
    """Test critic planner step."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "critic_planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="critic_planner",
        current_step="",
        question="What is the capital of France?",
        file=["document.pdf"],
        research_steps=["Research geography", "Check official sources"],
        expert_steps=["Analyze data", "Verify information"],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "critic_planner"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once()
    call_args = mock_compose_message.call_args
    assert call_args[1]["sender"] == "orchestrator"
    assert call_args[1]["receiver"] == "critic_planner"
    assert call_args[1]["type"] == "instruction"
    assert "Does the planner's plan have the correct and logical research and expert steps" in call_args[1]["content"]
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_researcher_step_initial_request(mock_compose_message, mock_send_message):
    """Test researcher step with initial request."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="researcher",
        current_step="",
        research_steps=["Research geography", "Check official sources"],
        current_research_index=-1,  # Will be incremented to 0
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "researcher"
    
    # Verify research index was incremented
    assert test_state["current_research_index"] == 0
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="researcher",
        type="instruction",
        content="Research the following topic or question: Research geography",
        step_id=0
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_researcher_step_retry_with_feedback(mock_compose_message, mock_send_message):
    """Test researcher step with retry feedback."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="researcher",
        current_step="",
        research_steps=["Research geography"],
        current_research_index=0,
        critic_researcher_decision="reject",
        critic_researcher_feedback="Need more detailed sources",
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "researcher"
    
    # Verify research index was NOT incremented (retry)
    assert test_state["current_research_index"] == 0
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="researcher",
        type="instruction",
        content="Use the following feedback to improve the research:\nNeed more detailed sources",
        step_id=0
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_critic_researcher_step(mock_compose_message, mock_send_message):
    """Test critic researcher step."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "critic_researcher", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="critic_researcher",
        current_step="",
        research_steps=["Research geography"],
        current_research_index=0,
        research_results=["Paris is the capital of France"],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "critic_researcher"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once()
    call_args = mock_compose_message.call_args
    assert call_args[1]["sender"] == "orchestrator"
    assert call_args[1]["receiver"] == "critic_researcher"
    assert call_args[1]["type"] == "instruction"
    assert "Does the researcher's results contain sufficient information" in call_args[1]["content"]
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_expert_step_initial_request(mock_compose_message, mock_send_message):
    """Test expert step with initial request."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="expert",
        current_step="",
        question="What is the capital of France?",
        research_results=["Paris is the capital", "France is in Europe"],
        expert_steps=["Analyze geography data", "Verify official sources"],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "expert"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once()
    call_args = mock_compose_message.call_args
    assert call_args[1]["sender"] == "orchestrator"
    assert call_args[1]["receiver"] == "expert"
    assert call_args[1]["type"] == "instruction"
    assert "Answer the question: What is the capital of France?" in call_args[1]["content"]
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_expert_step_retry_with_feedback(mock_compose_message, mock_send_message):
    """Test expert step with retry feedback."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="expert",
        current_step="",
        question="What is the capital of France?",
        critic_expert_decision="reject",
        critic_expert_feedback="Provide more detailed explanation",
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "expert"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="expert",
        type="instruction",
        content="Use the following feedback to improve your answer:\nProvide more detailed explanation"
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_critic_expert_step(mock_compose_message, mock_send_message):
    """Test critic expert step."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "critic_expert", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="critic_expert",
        current_step="",
        question="What is the capital of France?",
        research_results=["Paris is the capital", "France is in Europe"],
        expert_answer="Paris is the capital of France",
        expert_reasoning="Based on research results and official sources",
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "critic_expert"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once()
    call_args = mock_compose_message.call_args
    assert call_args[1]["sender"] == "orchestrator"
    assert call_args[1]["receiver"] == "critic_expert"
    assert call_args[1]["type"] == "instruction"
    assert "Does the expert's answer actually answer the question" in call_args[1]["content"]
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_finalizer_step(mock_compose_message, mock_send_message):
    """Test finalizer step."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "finalizer", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="finalizer",
        current_step="",
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "finalizer"
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="finalizer",
        type="instruction",
        content="Generate the final answer and reasoning trace (logical steps) to answer the question."
    )
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_edge_case_empty_research_steps(mock_compose_message, mock_send_message):
    """Test edge case with empty research steps."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state with empty research steps
    test_state = GraphState(
        next_step="researcher",
        current_step="",
        research_steps=[],
        current_research_index=-1,
        agent_messages=[]
    )
    
    # Execute function and expect IndexError
    with pytest.raises(IndexError):
        execute_next_step(test_state)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_edge_case_empty_expert_steps(mock_compose_message, mock_send_message):
    """Test edge case with empty expert steps."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state with empty expert steps
    test_state = GraphState(
        next_step="expert",
        current_step="",
        question="Test question",
        research_results=["result1"],
        expert_steps=[],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "expert"
    
    # Verify compose_agent_message was called (should handle empty expert_steps)
    mock_compose_message.assert_called_once()

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_edge_case_file_with_multiple_lines(mock_compose_message, mock_send_message):
    """Test edge case with file containing multiple lines."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state with multi-line file
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Analyze documents",
        file=["doc1.pdf", "doc2.xlsx", "doc3.txt"],
        research_steps=[],
        expert_steps=[],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "planner"
    
    # Verify compose_agent_message was called with correct file info
    expected_content = "Develop a logical plan to answer the following question:\nAnalyze documents\n\nInclude using following file in any of the research steps:doc1.pdf\ndoc2.xlsx\ndoc3.txt"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="planner",
        type="instruction",
        content=expected_content
    )

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_edge_case_no_file(mock_compose_message, mock_send_message):
    """Test edge case with no file."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state with no file
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Test question",
        file=None,
        research_steps=[],
        expert_steps=[],
        agent_messages=[]
    )
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is set to next_step
    assert test_state["current_step"] == "planner"
    
    # Verify compose_agent_message was called without file info
    expected_content = "Develop a logical plan to answer the following question:\nTest question"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator",
        receiver="planner",
        type="instruction",
        content=expected_content
    )

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_error_case_invalid_current_step(mock_compose_message, mock_send_message):
    """Test error case with invalid current step."""
    # Setup test state with invalid step
    test_state = GraphState(
        next_step="invalid_step",
        current_step="",
        agent_messages=[]
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        execute_next_step(test_state)
    
    assert "Invalid current step: invalid_step" in str(exc_info.value)
    
    # Verify compose_agent_message was not called
    mock_compose_message.assert_not_called()
    
    # Verify send_message was not called
    mock_send_message.assert_not_called()

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_error_case_missing_required_fields(mock_compose_message, mock_send_message):
    """Test error case with missing required fields."""
    # Setup test state missing required fields
    test_state = GraphState(
        next_step="planner",
        current_step="",
        # Missing question field
        agent_messages=[]
    )
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        execute_next_step(test_state)
    
    # Verify compose_agent_message was not called
    mock_compose_message.assert_not_called()
    
    # Verify send_message was not called
    mock_send_message.assert_not_called()

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_error_case_compose_message_failure(mock_compose_message, mock_send_message):
    """Test error case when compose_agent_message fails."""
    # Setup mocks
    mock_compose_message.side_effect = Exception("Compose message error")
    mock_send_message.return_value = None
    
    # Setup test state
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Test question",
        file=None,
        agent_messages=[]
    )
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        execute_next_step(test_state)
    
    assert "Compose message error" in str(exc_info.value)
    
    # Verify send_message was not called
    mock_send_message.assert_not_called()

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_error_case_send_message_failure(mock_compose_message, mock_send_message):
    """Test error case when send_message fails."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.side_effect = Exception("Send message error")
    
    # Setup test state
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Test question",
        file=None,
        agent_messages=[]
    )
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        execute_next_step(test_state)
    
    assert "Send message error" in str(exc_info.value)
    
    # Verify compose_agent_message was called
    mock_compose_message.assert_called_once()

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_state_preservation(mock_compose_message, mock_send_message):
    """Test that other state fields are preserved."""
    # Setup mocks
    mock_message = {"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}
    mock_compose_message.return_value = mock_message
    mock_send_message.return_value = None
    
    # Setup test state with various fields
    test_state = GraphState(
        next_step="planner",
        current_step="",
        question="Test question",
        file=["test.pdf"],
        research_steps=["step1", "step2"],
        expert_steps=["expert1", "expert2"],
        research_results=["result1"],
        expert_answer="test answer",
        expert_reasoning="test reasoning",
        agent_messages=["existing message"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=3
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_research_steps = test_state["research_steps"]
    original_expert_steps = test_state["expert_steps"]
    original_research_results = test_state["research_results"]
    original_expert_answer = test_state["expert_answer"]
    original_expert_reasoning = test_state["expert_reasoning"]
    original_agent_messages = test_state["agent_messages"]
    original_planner_retry_count = test_state["planner_retry_count"]
    original_researcher_retry_count = test_state["researcher_retry_count"]
    original_expert_retry_count = test_state["expert_retry_count"]
    
    # Execute function
    result = execute_next_step(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify current_step is updated
    assert test_state["current_step"] == "planner"
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["research_steps"] == original_research_steps
    assert test_state["expert_steps"] == original_expert_steps
    assert test_state["research_results"] == original_research_results
    assert test_state["expert_answer"] == original_expert_answer
    assert test_state["expert_reasoning"] == original_expert_reasoning
    assert test_state["agent_messages"] == original_agent_messages
    assert test_state["planner_retry_count"] == original_planner_retry_count
    assert test_state["researcher_retry_count"] == original_researcher_retry_count
    assert test_state["expert_retry_count"] == original_expert_retry_count
    
    # Verify compose_agent_message was called
    mock_compose_message.assert_called_once()
    
    # Verify send_message was called
    mock_send_message.assert_called_once_with(test_state, mock_message)
```
#### 3.7.5 Main Orchestrator

**Function Information:**
- **Function Name**: `orchestrator`
- **Location**: `multi_agent_system.py:766`
- **Purpose**: Orchestrator logic to coordinate the multi-agent workflow by following a 4-step process: determine next step, check retry limits, execute next step, and return state

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing all workflow state information
- **Return**: `GraphState` - The updated state after orchestrator processing
- **Side Effects**: Modifies the input state through calls to determine_next_step, check_retry_limit, and execute_next_step

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `determine_next_step()` - Internal function for determining next step
  - `check_retry_limit()` - Internal function for checking retry limits
  - `execute_next_step()` - Internal function for executing next step
- **Dependencies to Use Directly**: 
  - `GraphState` - Internal TypedDict type
  - Dictionary access (`get()`) - Built-in Python operations
  - String formatting with f-strings - Built-in Python features
  - Function calls and return value assignment - Built-in Python operations
- **Mock Configuration**: Mock logger for consistent testing and internal functions to verify correct orchestration flow

**Test Cases:**

**Happy Path:**
- **Normal Workflow Execution**: Complete 4-step process with valid state
- **Planner Step Orchestration**: Orchestrator with planner as current step
- **Researcher Step Orchestration**: Orchestrator with researcher as current step
- **Expert Step Orchestration**: Orchestrator with expert as current step
- **Critic Step Orchestration**: Orchestrator with critic steps as current step
- **Finalizer Step Orchestration**: Orchestrator with finalizer as current step
- **Initial State Orchestration**: Orchestrator with empty or "input" current step

**Edge Cases:**
- **Unknown Current Step**: Current step not in expected values
- **Missing Current Step**: Current step field missing from state
- **Empty State**: Minimal state with only required fields
- **Complex State**: State with all fields populated
- **High Retry Counts**: State with high retry counts near limits
- **Long Content**: State with very long content strings

**Error Conditions:**
- **Determine Next Step Failure**: Errors in determine_next_step function
- **Check Retry Limit Failure**: Errors in check_retry_limit function
- **Execute Next Step Failure**: Errors in execute_next_step function
- **Logger Failure**: Errors in logging operations
- **Missing Required Fields**: Missing fields needed by internal functions
- **Invalid State Structure**: Malformed or invalid state structure

**State Changes:**
- **State Flow**: State flows through all three internal functions
- **State Preservation**: State object identity is preserved throughout
- **Logging**: Logging calls are made at start and end
- **Return Value**: Same state object is returned after processing

**Mock Configurations:**
```python
@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_orchestrator_with_mocked_dependencies(mock_logger, mock_determine, mock_check, mock_execute):
    """Mock all internal functions and logger for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock internal functions to return the state
    mock_determine.return_value = GraphState(current_step="planner", next_step="critic_planner")
    mock_check.return_value = GraphState(current_step="planner", next_step="critic_planner")
    mock_execute.return_value = GraphState(current_step="critic_planner", next_step="critic_planner")
    
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify all functions were called in correct order
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once()
    mock_execute.assert_called_once()
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: planner")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: critic_planner")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and orchestration flow in the implementation:
def orchestrator(state: GraphState) -> GraphState:  # Direct type annotations
    """Orchestrator logic to coordinate the multi-agent workflow."""
    
    logger.info(f"Orchestrator starting execution. Current step: {state.get('current_step', 'unknown')}")  # Direct logging call with f-string formatting and dictionary get method
    
    # Step 1: Determine the next step
    state = determine_next_step(state)  # Direct function call and assignment
    
    # Step 2: Check retry count
    state = check_retry_limit(state)  # Direct function call and assignment
    
    # Step 3: Execute the next step
    state = execute_next_step(state)  # Direct function call and assignment
    
    logger.info(f"Orchestrator completed successfully. Next step: {state.get('next_step', 'unknown')}")  # Direct logging call with f-string formatting and dictionary get method
    return state  # Direct return
```

**Assertion Specifications:**
- Verify all three internal functions are called in correct order
- Verify logging calls are made with correct messages at start and end
- Verify state object identity is preserved throughout the process
- Verify return value is the final state after all processing
- Verify proper error propagation from internal functions
- Verify state flows correctly through the 4-step process
- Verify logging includes current step and next step information

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import orchestrator, GraphState

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_normal_workflow_execution(mock_logger, mock_determine, mock_check, mock_execute):
    """Test normal workflow execution with all steps."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock internal functions to return modified state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="What is the capital of France?"
    )
    
    # Mock determine_next_step to set next_step
    state_after_determine = GraphState(
        current_step="planner",
        next_step="critic_planner",
        question="What is the capital of France?"
    )
    mock_determine.return_value = state_after_determine
    
    # Mock check_retry_limit to return same state
    mock_check.return_value = state_after_determine
    
    # Mock execute_next_step to update current_step
    state_after_execute = GraphState(
        current_step="critic_planner",
        next_step="critic_planner",
        question="What is the capital of France?"
    )
    mock_execute.return_value = state_after_execute
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called in correct order
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(state_after_determine)
    mock_execute.assert_called_once_with(state_after_determine)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: planner")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: critic_planner")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_planner_step_orchestration(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with planner step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: planner")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_researcher_step_orchestration(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with researcher step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="researcher",
        next_step="",
        research_steps=["step1", "step2"],
        current_research_index=0
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: researcher")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_expert_step_orchestration(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with expert step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="expert",
        next_step="",
        question="Test question",
        research_results=["result1"],
        expert_steps=["expert1"]
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: expert")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_critic_step_orchestration(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with critic step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        next_step="",
        question="Test question",
        research_steps=["step1"],
        expert_steps=["expert1"]
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: critic_planner")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_finalizer_step_orchestration(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with finalizer step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="finalizer",
        next_step="",
        question="Test question",
        expert_answer="Test answer",
        expert_reasoning="Test reasoning"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: finalizer")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_initial_state_orchestration(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with initial state."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state with empty current_step
    test_state = GraphState(
        current_step="",
        next_step="",
        question="Test question"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: ")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_edge_case_unknown_current_step(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with unknown current step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state with unknown current_step
    test_state = GraphState(
        current_step="unknown_step",
        next_step="",
        question="Test question"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: unknown_step")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_edge_case_missing_current_step(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with missing current step field."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state without current_step field
    test_state = GraphState(
        next_step="",
        question="Test question"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls (should use 'unknown' for missing current_step)
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: unknown")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_edge_case_empty_state(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with minimal state."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup minimal test state
    test_state = GraphState()
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: unknown")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_edge_case_complex_state(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator with complex state."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup complex test state with all fields
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="What is the capital of France?",
        file=["document.pdf"],
        research_steps=["step1", "step2"],
        expert_steps=["expert1", "expert2"],
        research_results=["result1", "result2"],
        expert_answer="Paris is the capital",
        expert_reasoning="Based on research",
        agent_messages=["message1", "message2"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=2,
        expert_retry_limit=1
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify all functions were called
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: planner")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_error_case_determine_next_step_failure(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator when determine_next_step fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Mock determine_next_step to raise exception
    mock_determine.side_effect = Exception("Determine next step error")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        orchestrator(test_state)
    
    assert "Determine next step error" in str(exc_info.value)
    
    # Verify determine_next_step was called
    mock_determine.assert_called_once_with(test_state)
    
    # Verify other functions were not called
    mock_check.assert_not_called()
    mock_execute.assert_not_called()
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Orchestrator starting execution. Current step: planner")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_error_case_check_retry_limit_failure(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator when check_retry_limit fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.side_effect = Exception("Check retry limit error")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        orchestrator(test_state)
    
    assert "Check retry limit error" in str(exc_info.value)
    
    # Verify functions were called in order
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    
    # Verify execute_next_step was not called
    mock_execute.assert_not_called()
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Orchestrator starting execution. Current step: planner")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_error_case_execute_next_step_failure(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator when execute_next_step fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Mock internal functions
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.side_effect = Exception("Execute next step error")
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        orchestrator(test_state)
    
    assert "Execute next step error" in str(exc_info.value)
    
    # Verify functions were called in order
    mock_determine.assert_called_once_with(test_state)
    mock_check.assert_called_once_with(test_state)
    mock_execute.assert_called_once_with(test_state)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Orchestrator starting execution. Current step: planner")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_error_case_logger_failure(mock_logger, mock_determine, mock_check, mock_execute):
    """Test orchestrator when logger fails."""
    # Setup mocks
    mock_logger.info.side_effect = Exception("Logger error")
    
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        orchestrator(test_state)
    
    assert "Logger error" in str(exc_info.value)
    
    # Verify no internal functions were called
    mock_determine.assert_not_called()
    mock_check.assert_not_called()
    mock_execute.assert_not_called()

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_state_flow_verification(mock_logger, mock_determine, mock_check, mock_execute):
    """Test that state flows correctly through all three functions."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    initial_state = GraphState(
        current_step="planner",
        next_step="",
        question="Test question"
    )
    
    # Mock determine_next_step to modify state
    state_after_determine = GraphState(
        current_step="planner",
        next_step="critic_planner",
        question="Test question"
    )
    mock_determine.return_value = state_after_determine
    
    # Mock check_retry_limit to return same state
    mock_check.return_value = state_after_determine
    
    # Mock execute_next_step to modify state
    final_state = GraphState(
        current_step="critic_planner",
        next_step="critic_planner",
        question="Test question"
    )
    mock_execute.return_value = final_state
    
    # Execute function
    result = orchestrator(initial_state)
    
    # Verify result is the same state object
    assert result is initial_state
    
    # Verify functions were called with correct state objects
    mock_determine.assert_called_once_with(initial_state)
    mock_check.assert_called_once_with(state_after_determine)
    mock_execute.assert_called_once_with(state_after_determine)
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: planner")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: critic_planner")

@patch('multi_agent_system.execute_next_step')
@patch('multi_agent_system.check_retry_limit')
@patch('multi_agent_system.determine_next_step')
@patch('multi_agent_system.logger')
def test_function_call_order_verification(mock_logger, mock_determine, mock_check, mock_execute):
    """Test that functions are called in the correct order."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test state
    test_state = GraphState(
        current_step="researcher",
        next_step="",
        question="Test question"
    )
    
    # Mock all functions to return the state
    mock_determine.return_value = test_state
    mock_check.return_value = test_state
    mock_execute.return_value = test_state
    
    # Execute function
    result = orchestrator(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify call order by checking call_args_list
    assert mock_determine.call_args_list[0][0][0] is test_state
    assert mock_check.call_args_list[0][0][0] is test_state
    assert mock_execute.call_args_list[0][0][0] is test_state
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Orchestrator starting execution. Current step: researcher")
    mock_logger.info.assert_any_call("Orchestrator completed successfully. Next step: ")
```
#### 3.7.6 Orchestrator Routing

**Function Information:**
- **Function Name**: `route_from_orchestrator`
- **Location**: `multi_agent_system.py:798`
- **Purpose**: Route the state to the appropriate agent based on the current step, mapping current_step values to agent names

**Function Signature and Parameters:**
- **Input**: `state: GraphState` - The current state of the graph containing current_step field
- **Return**: `str` - The name of the agent to route to
- **Side Effects**: None - Pure function that only reads state and returns routing decision

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - None - This function is purely state-based logic with no external dependencies
- **Dependencies to Use Directly**: 
  - `GraphState` - Internal TypedDict type
  - Dictionary access - Built-in Python operations
  - String comparisons and equality checks - Built-in Python features
  - Conditional logic and control flow - Built-in Python features
  - Exception raising (`ValueError`) - Built-in Python features
  - String formatting with f-strings - Built-in Python features
- **Mock Configuration**: No mocking required - function operates entirely on state data

**Test Cases:**

**Happy Path:**
- **Planner Routing**: Current step is "planner"
- **Researcher Routing**: Current step is "researcher"
- **Expert Routing**: Current step is "expert"
- **Critic Planner Routing**: Current step is "critic_planner"
- **Critic Researcher Routing**: Current step is "critic_researcher"
- **Critic Expert Routing**: Current step is "critic_expert"
- **Finalizer Routing**: Current step is "finalizer"

**Edge Cases:**
- **Empty Current Step**: Current step is empty string
- **Whitespace Current Step**: Current step contains only whitespace
- **Case Sensitivity**: Current step with different case
- **Special Characters**: Current step with special characters
- **Long Current Step**: Very long current step string
- **Missing Current Step**: Current step field missing from state

**Error Conditions:**
- **Invalid Current Step**: Current step not in expected values
- **Unknown Current Step**: Current step with unexpected value
- **Missing State Field**: Current step field missing from state
- **None Current Step**: Current step is None

**State Changes:**
- **No State Changes**: Function does not modify the input state
- **Return Value**: Returns appropriate agent name string
- **State Preservation**: Input state remains completely unchanged

**Mock Configurations:**
```python
# No mocking required - function operates entirely on state data
# All test scenarios use direct state manipulation
def test_route_from_orchestrator_no_mocking_required():
    """Function operates purely on state data, no external dependencies."""
    # Direct state creation and manipulation
    test_state = GraphState(
        current_step="planner"
    )
    
    # Direct function call
    result = route_from_orchestrator(test_state)
    
    # Direct assertions
    assert result == "planner"
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and state operations in the implementation:
def route_from_orchestrator(state: GraphState) -> str:  # Direct type annotations
    """Route the state to the appropriate agent based on the current step."""
    
    if state["current_step"] == "planner":  # Direct dictionary access and string comparison
        return "planner"  # Direct string return
    elif state["current_step"] == "researcher":  # Direct dictionary access and string comparison
        return "researcher"  # Direct string return
    elif state["current_step"] == "expert":  # Direct dictionary access and string comparison
        return "expert"  # Direct string return
    elif state["current_step"] == "critic_planner":  # Direct dictionary access and string comparison
        return "critic"  # Direct string return
    elif state["current_step"] == "critic_researcher":  # Direct dictionary access and string comparison
        return "critic"  # Direct string return
    elif state["current_step"] == "critic_expert":  # Direct dictionary access and string comparison
        return "critic"  # Direct string return
    elif state["current_step"] == "finalizer":  # Direct dictionary access and string comparison
        return "finalizer"  # Direct string return
    else:
        raise ValueError(f"Invalid current step: {state['current_step']}")  # Direct exception raising with f-string formatting
```

**Assertion Specifications:**
- Verify correct agent name is returned for each current step value
- Verify state object is not modified (no side effects)
- Verify function returns string values
- Verify proper error handling for invalid current steps
- Verify all expected current step values are handled
- Verify critic steps are properly mapped to "critic" agent

**Code Examples:**
```python
import pytest
from multi_agent_system import route_from_orchestrator, GraphState

def test_planner_routing():
    """Test routing for planner current step."""
    # Setup test state
    test_state = GraphState(
        current_step="planner",
        question="Test question"
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "planner"
    
    # Verify state is not modified
    assert test_state["current_step"] == "planner"

def test_researcher_routing():
    """Test routing for researcher current step."""
    # Setup test state
    test_state = GraphState(
        current_step="researcher",
        research_steps=["step1", "step2"],
        current_research_index=0
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "researcher"
    
    # Verify state is not modified
    assert test_state["current_step"] == "researcher"

def test_expert_routing():
    """Test routing for expert current step."""
    # Setup test state
    test_state = GraphState(
        current_step="expert",
        question="Test question",
        research_results=["result1"],
        expert_steps=["expert1"]
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "expert"
    
    # Verify state is not modified
    assert test_state["current_step"] == "expert"

def test_critic_planner_routing():
    """Test routing for critic_planner current step."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="Test question",
        research_steps=["step1"],
        expert_steps=["expert1"]
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "critic"
    
    # Verify state is not modified
    assert test_state["current_step"] == "critic_planner"

def test_critic_researcher_routing():
    """Test routing for critic_researcher current step."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_researcher",
        research_steps=["step1"],
        current_research_index=0,
        research_results=["result1"]
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "critic"
    
    # Verify state is not modified
    assert test_state["current_step"] == "critic_researcher"

def test_critic_expert_routing():
    """Test routing for critic_expert current step."""
    # Setup test state
    test_state = GraphState(
        current_step="critic_expert",
        question="Test question",
        research_results=["result1"],
        expert_answer="Test answer",
        expert_reasoning="Test reasoning"
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "critic"
    
    # Verify state is not modified
    assert test_state["current_step"] == "critic_expert"

def test_finalizer_routing():
    """Test routing for finalizer current step."""
    # Setup test state
    test_state = GraphState(
        current_step="finalizer",
        question="Test question",
        expert_answer="Test answer",
        expert_reasoning="Test reasoning"
    )
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "finalizer"
    
    # Verify state is not modified
    assert test_state["current_step"] == "finalizer"

def test_edge_case_empty_current_step():
    """Test edge case with empty current step."""
    # Setup test state
    test_state = GraphState(
        current_step="",
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step: " in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == ""

def test_edge_case_whitespace_current_step():
    """Test edge case with whitespace current step."""
    # Setup test state
    test_state = GraphState(
        current_step="   \n\t   ",
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step:    \n\t   " in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == "   \n\t   "

def test_edge_case_case_sensitivity():
    """Test edge case with case sensitivity."""
    # Setup test state with uppercase
    test_state = GraphState(
        current_step="PLANNER",
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step: PLANNER" in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == "PLANNER"

def test_edge_case_special_characters():
    """Test edge case with special characters in current step."""
    # Setup test state
    test_state = GraphState(
        current_step="planner_123!@#",
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step: planner_123!@#" in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == "planner_123!@#"

def test_edge_case_long_current_step():
    """Test edge case with very long current step."""
    # Setup test state
    long_step = "planner" + "x" * 1000
    test_state = GraphState(
        current_step=long_step,
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert f"Invalid current step: {long_step}" in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == long_step

def test_error_case_invalid_current_step():
    """Test error case with invalid current step."""
    # Setup test state
    test_state = GraphState(
        current_step="invalid_step",
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step: invalid_step" in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == "invalid_step"

def test_error_case_unknown_current_step():
    """Test error case with unknown current step."""
    # Setup test state
    test_state = GraphState(
        current_step="unknown_step",
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step: unknown_step" in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] == "unknown_step"

def test_error_case_missing_current_step():
    """Test error case with missing current step field."""
    # Setup test state without current_step field
    test_state = GraphState(
        question="Test question"
    )
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        route_from_orchestrator(test_state)

def test_error_case_none_current_step():
    """Test error case with None current step."""
    # Setup test state
    test_state = GraphState(
        current_step=None,
        question="Test question"
    )
    
    # Execute function and expect ValueError
    with pytest.raises(ValueError) as exc_info:
        route_from_orchestrator(test_state)
    
    assert "Invalid current step: None" in str(exc_info.value)
    
    # Verify state is not modified
    assert test_state["current_step"] is None

def test_return_type_validation():
    """Test that function returns correct string type."""
    # Test all valid current steps
    valid_steps = ["planner", "researcher", "expert", "critic_planner", "critic_researcher", "critic_expert", "finalizer"]
    
    for step in valid_steps:
        test_state = GraphState(current_step=step)
        result = route_from_orchestrator(test_state)
        
        # Verify result is a string
        assert isinstance(result, str)
        
        # Verify result is not empty
        assert len(result) > 0
        
        # Verify state is not modified
        assert test_state["current_step"] == step

def test_critic_routing_consistency():
    """Test that all critic steps route to 'critic' agent."""
    # Test all critic steps
    critic_steps = ["critic_planner", "critic_researcher", "critic_expert"]
    
    for step in critic_steps:
        test_state = GraphState(current_step=step)
        result = route_from_orchestrator(test_state)
        
        # Verify all critic steps route to "critic"
        assert result == "critic"
        
        # Verify state is not modified
        assert test_state["current_step"] == step

def test_unique_routing_mapping():
    """Test that each valid current step maps to the expected agent."""
    # Define expected mappings
    expected_mappings = {
        "planner": "planner",
        "researcher": "researcher",
        "expert": "expert",
        "critic_planner": "critic",
        "critic_researcher": "critic",
        "critic_expert": "critic",
        "finalizer": "finalizer"
    }
    
    # Test each mapping
    for current_step, expected_agent in expected_mappings.items():
        test_state = GraphState(current_step=current_step)
        result = route_from_orchestrator(test_state)
        
        # Verify correct mapping
        assert result == expected_agent, f"Expected {current_step} to route to {expected_agent}, but got {result}"
        
        # Verify state is not modified
        assert test_state["current_step"] == current_step

def test_state_preservation():
    """Test that input state is completely preserved."""
    # Setup test state with various fields
    test_state = GraphState(
        current_step="planner",
        question="Test question",
        file=["test.pdf"],
        research_steps=["step1", "step2"],
        expert_steps=["expert1", "expert2"],
        research_results=["result1"],
        expert_answer="Test answer",
        expert_reasoning="Test reasoning",
        agent_messages=["message1"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0
    )
    
    # Store original state for comparison
    original_state = dict(test_state)
    
    # Execute function
    result = route_from_orchestrator(test_state)
    
    # Verify result
    assert result == "planner"
    
    # Verify state is completely preserved
    assert dict(test_state) == original_state
    
    # Verify all fields are unchanged
    assert test_state["current_step"] == "planner"
    assert test_state["question"] == "Test question"
    assert test_state["file"] == ["test.pdf"]
    assert test_state["research_steps"] == ["step1", "step2"]
    assert test_state["expert_steps"] == ["expert1", "expert2"]
    assert test_state["research_results"] == ["result1"]
    assert test_state["expert_answer"] == "Test answer"
    assert test_state["expert_reasoning"] == "Test reasoning"
    assert test_state["agent_messages"] == ["message1"]
    assert test_state["planner_retry_count"] == 1
    assert test_state["researcher_retry_count"] == 2
    assert test_state["expert_retry_count"] == 0

def test_error_message_format():
    """Test that error messages are properly formatted."""
    # Test various invalid current steps
    invalid_steps = ["invalid", "unknown", "test", "123", "step_1"]
    
    for step in invalid_steps:
        test_state = GraphState(current_step=step)
        
        # Execute function and expect ValueError
        with pytest.raises(ValueError) as exc_info:
            route_from_orchestrator(test_state)
        
        # Verify error message format
        expected_message = f"Invalid current step: {step}"
        assert str(exc_info.value) == expected_message
        
        # Verify state is not modified
        assert test_state["current_step"] == step
```
### 3.8 Agent Factories
#### 3.8.1 Planner Agent Factory

**Function Information:**
- **Function Name**: `create_planner_agent`
- **Location**: `multi_agent_system.py:828`
- **Purpose**: Creates a planner agent function with injected prompt and LLM, returning a callable that processes GraphState and generates research and expert steps

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompt and output schema
- **Input**: `llm_planner: ChatOpenAI` - Language model instance for planning
- **Return**: `Callable[[GraphState], GraphState]` - A function that takes and returns GraphState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `SystemMessage` - LangChain message class
  - `get_agent_conversation()` - Internal function for retrieving conversation history
  - `convert_agent_messages_to_langchain()` - Internal function for message conversion
  - `llm_planner.invoke()` - LLM invocation method
  - `validate_llm_response()` - Internal function for response validation
  - `compose_agent_message()` - Internal function for creating agent messages
  - `send_message()` - Internal function for adding messages to state
- **Dependencies to Use Directly**: 
  - `AgentConfig` - Internal configuration class
  - `ChatOpenAI` - LangChain LLM class
  - `GraphState` - Internal TypedDict type
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
  - String formatting with f-strings - Built-in Python features
  - List concatenation - Built-in Python operations
- **Mock Configuration**: Mock all external dependencies to isolate the planner agent logic and verify correct interactions

**Test Cases:**

**Happy Path:**
- **Valid Config and LLM**: Function with valid AgentConfig and ChatOpenAI
- **Planner Agent Execution**: Returned function processes state correctly
- **Valid LLM Response**: LLM returns properly formatted response
- **Complete Workflow**: Full planner agent workflow with all steps
- **State Updates**: State is properly updated with research and expert steps

**Edge Cases:**
- **Empty Message History**: No previous conversation messages
- **Large Message History**: Very long conversation history
- **Empty LLM Response**: LLM returns empty or minimal response
- **Large Response**: LLM returns very large response
- **Special Characters**: Response contains special characters and Unicode
- **Missing State Fields**: Some state fields missing or None

**Error Conditions:**
- **Invalid Config**: Malformed or invalid AgentConfig
- **Invalid LLM**: Malformed or invalid ChatOpenAI instance
- **LLM Invocation Failure**: Errors in LLM invoke method
- **Response Validation Failure**: Invalid or malformed LLM response
- **Missing Required Fields**: Missing fields needed for processing
- **Logger Failure**: Errors in logging operations
- **Message Conversion Failure**: Errors in message conversion
- **Agent Conversation Failure**: Errors in retrieving conversation history

**State Changes:**
- **Research Steps Assignment**: research_steps field is set from LLM response
- **Expert Steps Assignment**: expert_steps field is set from LLM response
- **Message Addition**: Agent message is added to state via send_message
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned after processing

**Mock Configurations:**
```python
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_factory_with_mocked_dependencies(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Mock all external dependencies for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock SystemMessage
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Mock conversation retrieval
    mock_conversation = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "test"}]
    mock_get_conversation.return_value = mock_conversation
    
    # Mock message conversion
    mock_langchain_messages = [Mock()]
    mock_convert.return_value = mock_langchain_messages
    
    # Mock LLM response
    mock_llm_response = Mock()
    mock_llm_response.content = '{"research_steps": ["step1", "step2"], "expert_steps": ["expert1"]}'
    
    # Mock LLM instance
    mock_llm = Mock()
    mock_llm.invoke.return_value = mock_llm_response
    
    # Mock response validation
    mock_validate.return_value = {"research_steps": ["step1", "step2"], "expert_steps": ["expert1"]}
    
    # Mock agent message composition
    mock_agent_message = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_compose.return_value = mock_agent_message
    
    # Mock message sending
    mock_send.return_value = None
    
    # Setup test config
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and factory pattern in the implementation:
def create_planner_agent(config: AgentConfig, llm_planner: ChatOpenAI) -> Callable[[GraphState], GraphState]:  # Direct type annotations
    """Create a planner agent function with the given prompt and LLM."""
    
    def planner_agent(state: GraphState) -> GraphState:  # Direct function definition and type annotations
        """Planner agent with injected prompt."""
        logger.info("Planner starting execution")  # Direct logging call
        
        sys_prompt = [SystemMessage(content=config.system_prompt)]  # Direct SystemMessage creation and list creation
        message_history = get_agent_conversation(state, "planner")  # Direct function call
        message_in = convert_agent_messages_to_langchain(message_history)  # Direct function call
        
        response = llm_planner.invoke(sys_prompt + message_in)  # Direct LLM invocation and list concatenation
        
        # Validate response
        response = validate_llm_response(response, config.output_schema.keys(), "planner")  # Direct function call
        
        state["research_steps"] = response["research_steps"]  # Direct dictionary assignment
        state["expert_steps"] = response["expert_steps"]  # Direct dictionary assignment
        
        agent_message = compose_agent_message(  # Direct function call
            sender= "planner",
            receiver= "orchestrator",
            type= "response",
            content= f"Planner complete. Research steps: {response['research_steps']}, Expert steps: {response['expert_steps']}",  # Direct f-string formatting
        )
        state = send_message(state, agent_message)  # Direct function call
        
        logger.info("Planner completed successfully")  # Direct logging call
        return state  # Direct return
    return planner_agent  # Direct function return
```

**Assertion Specifications:**
- Verify factory returns a callable with correct signature
- Verify returned function processes GraphState correctly
- Verify LLM is invoked with correct system prompt and message history
- Verify response is properly validated
- Verify state is updated with research and expert steps
- Verify agent message is composed and sent correctly
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_planner_agent, GraphState

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_create_planner_agent_factory(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": [], "expert_steps": []}
    mock_compose.return_value = {}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Execute factory function
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Verify factory returns a callable
    assert callable(planner_agent)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(planner_agent)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_execution(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent execution with valid inputs."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "Plan this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"research_steps": ["step1", "step2"], "expert_steps": ["expert1"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="What is the capital of France?",
        agent_messages=[]
    )
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["research_steps"] == ["step1", "step2"]
    assert test_state["expert_steps"] == ["expert1"]
    
    # Verify all mocks were called correctly
    mock_system_message.assert_called_once_with(content="You are a planner agent")
    mock_get_conversation.assert_called_once_with(test_state, "planner")
    mock_convert.assert_called_once_with([{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "Plan this"}])
    mock_llm.invoke.assert_called_once()
    mock_validate.assert_called_once_with(mock_llm.invoke.return_value, ["research_steps", "expert_steps"], "planner")
    mock_compose.assert_called_once()
    mock_send.assert_called_once_with(test_state, {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"})
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Planner starting execution")
    mock_logger.info.assert_any_call("Planner completed successfully")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_empty_message_history(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent with empty message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []  # Empty conversation
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": [], "expert_steps": []}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["research_steps"] == []
    assert test_state["expert_steps"] == []
    
    # Verify LLM was called with system prompt only
    mock_llm.invoke.assert_called_once()
    call_args = mock_llm.invoke.call_args[0][0]
    assert len(call_args) == 1  # Only system prompt
    assert call_args[0] == mock_sys_msg

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_large_message_history(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent with large message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "x" * 10000}]  # Large message
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"research_steps": ["step1"], "expert_steps": ["expert1"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["research_steps"] == ["step1"]
    assert test_state["expert_steps"] == ["expert1"]
    
    # Verify LLM was called with system prompt and large message
    mock_llm.invoke.assert_called_once()
    call_args = mock_llm.invoke.call_args[0][0]
    assert len(call_args) == 2  # System prompt + converted message

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_special_characters_response(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent with special characters in response."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": ["Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"], "expert_steps": ["ðŸš€ðŸŒŸðŸ’¡"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with special characters
    assert test_state["research_steps"] == ["Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"]
    assert test_state["expert_steps"] == ["ðŸš€ðŸŒŸðŸ’¡"]

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_missing_state_fields(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent with missing state fields."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": ["step1"], "expert_steps": ["expert1"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state with minimal fields
    test_state = GraphState()
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["research_steps"] == ["step1"]
    assert test_state["expert_steps"] == ["expert1"]

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_llm_invocation_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent when LLM invocation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.side_effect = Exception("LLM invocation error")
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent and expect exception
    with pytest.raises(Exception) as exc_info:
        planner_agent(test_state)
    
    assert "LLM invocation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Planner starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_response_validation_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent when response validation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.side_effect = Exception("Validation error")
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent and expect exception
    with pytest.raises(Exception) as exc_info:
        planner_agent(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Planner starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_logger_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test planner agent when logger fails."""
    # Setup mocks
    mock_logger.info.side_effect = Exception("Logger error")
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": ["step1"], "expert_steps": ["expert1"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent and expect exception
    with pytest.raises(Exception) as exc_info:
        planner_agent(test_state)
    
    assert "Logger error" in str(exc_info.value)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_state_preservation(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that other state fields are preserved."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": ["step1"], "expert_steps": ["expert1"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state with various fields
    test_state = GraphState(
        question="Test question",
        file=["test.pdf"],
        current_step="planner",
        next_step="critic_planner",
        agent_messages=["existing message"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_current_step = test_state["current_step"]
    original_next_step = test_state["next_step"]
    original_agent_messages = test_state["agent_messages"]
    original_planner_retry_count = test_state["planner_retry_count"]
    original_researcher_retry_count = test_state["researcher_retry_count"]
    original_expert_retry_count = test_state["expert_retry_count"]
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify research and expert steps are updated
    assert test_state["research_steps"] == ["step1"]
    assert test_state["expert_steps"] == ["expert1"]
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["current_step"] == original_current_step
    assert test_state["next_step"] == original_next_step
    assert test_state["agent_messages"] == original_agent_messages
    assert test_state["planner_retry_count"] == original_planner_retry_count
    assert test_state["researcher_retry_count"] == original_researcher_retry_count
    assert test_state["expert_retry_count"] == original_expert_retry_count

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_message_composition_verification(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that agent message is composed with correct content."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"research_steps": ["step1", "step2"], "expert_steps": ["expert1", "expert2"]}
    mock_compose.return_value = {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a planner agent"
    mock_config.output_schema = {"research_steps": "list", "expert_steps": "list"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create planner agent
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        agent_messages=[]
    )
    
    # Execute planner agent
    result = planner_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose.assert_called_once_with(
        sender="planner",
        receiver="orchestrator",
        type="response",
        content="Planner complete. Research steps: ['step1', 'step2'], Expert steps: ['expert1', 'expert2']"
    )
    
    # Verify send_message was called with correct parameters
    mock_send.assert_called_once_with(test_state, {"sender": "planner", "receiver": "orchestrator", "type": "response", "content": "test"})
```
#### 3.8.2 Researcher Agent Factory

**Function Information:**
- **Function Name**: `create_researcher_agent`
- **Location**: `multi_agent_system.py:862`
- **Purpose**: Creates a researcher agent function with injected prompt and compiled subgraph, returning a callable that processes GraphState and executes research steps using a subgraph

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompt and output schema
- **Input**: `compiled_researcher_graph: Callable` - Compiled subgraph function for research execution
- **Return**: `Callable[[GraphState], GraphState]` - A function that takes and returns GraphState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `get_agent_conversation()` - Internal function for retrieving conversation history
  - `convert_agent_messages_to_langchain()` - Internal function for message conversion
  - `compiled_researcher_graph.invoke()` - Subgraph execution method
  - `validate_llm_response()` - Internal function for response validation
  - `compose_agent_message()` - Internal function for creating agent messages
  - `send_message()` - Internal function for adding messages to state
- **Dependencies to Use Directly**: 
  - `AgentConfig` - Internal configuration class
  - `GraphState` - Internal TypedDict type
  - `ResearcherState` - Internal TypedDict type
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
  - List operations (append, extend, length checking) - Built-in Python features
  - String formatting with f-strings - Built-in Python features
  - Conditional logic and control flow - Built-in Python features
- **Mock Configuration**: Mock all external dependencies to isolate the researcher agent logic and verify correct interactions

**Test Cases:**

**Happy Path:**
- **Valid Config and Subgraph**: Function with valid AgentConfig and compiled subgraph
- **Researcher Agent Execution**: Returned function processes state correctly
- **New Research Step**: Creating new ResearcherState for step
- **Existing Research Step**: Updating existing ResearcherState
- **Valid Subgraph Response**: Subgraph returns properly formatted response
- **Complete Workflow**: Full researcher agent workflow with all steps
- **State Updates**: State is properly updated with research results

**Edge Cases:**
- **Empty Message History**: No previous conversation messages
- **Large Message History**: Very long conversation history
- **Empty Subgraph Response**: Subgraph returns empty or minimal response
- **Large Response**: Subgraph returns very large response
- **Special Characters**: Response contains special characters and Unicode
- **Missing State Fields**: Some state fields missing or None
- **Zero Research Index**: Current research index is 0
- **Large Research Index**: Very large research index value
- **Empty Research Results**: No existing research results
- **Existing Research Results**: Research results already exist

**Error Conditions:**
- **Invalid Config**: Malformed or invalid AgentConfig
- **Invalid Subgraph**: Malformed or invalid compiled subgraph
- **Subgraph Invocation Failure**: Errors in subgraph invoke method
- **Response Validation Failure**: Invalid or malformed subgraph response
- **Missing Required Fields**: Missing fields needed for processing
- **Logger Failure**: Errors in logging operations
- **Message Conversion Failure**: Errors in message conversion
- **Agent Conversation Failure**: Errors in retrieving conversation history
- **ResearcherState Creation Failure**: Errors in creating ResearcherState
- **List Index Errors**: Index out of bounds errors

**State Changes:**
- **ResearcherState Creation/Update**: ResearcherState is created or updated in state
- **Research Results Storage**: Research results are stored in state
- **Message Addition**: Agent message is added to state via send_message
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned after processing

**Mock Configurations:**
```python
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_factory_with_mocked_dependencies(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Mock all external dependencies for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock conversation retrieval
    mock_conversation = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "test"}]
    mock_get_conversation.return_value = mock_conversation
    
    # Mock message conversion
    mock_langchain_messages = [Mock()]
    mock_convert.return_value = mock_langchain_messages
    
    # Mock subgraph response
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    
    # Mock compiled subgraph
    mock_subgraph = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Mock response validation
    mock_validate.return_value = {"result": "test result"}
    
    # Mock agent message composition
    mock_agent_message = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_compose.return_value = mock_agent_message
    
    # Mock message sending
    mock_send.return_value = None
    
    # Setup test config
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and factory pattern in the implementation:
def create_researcher_agent(config: AgentConfig, compiled_researcher_graph: Callable) -> Callable[[GraphState], GraphState]:  # Direct type annotations
    """Create a researcher agent function with the given prompt and LLM."""
    
    def researcher_agent(state: GraphState) -> GraphState:  # Direct function definition and type annotations
        """Researcher agent with injected prompt."""
        logger.info("Researcher starting execution")  # Direct logging call
        
        idx = state["current_research_index"]  # Direct dictionary access
        message_history = get_agent_conversation(state, "researcher", step_id=idx)  # Direct function call
        message_in = convert_agent_messages_to_langchain(message_history)  # Direct function call
        
        # Get or create ResearcherState for this step
        if idx not in state["researcher_states"]:  # Direct dictionary membership check
            researcher_state = ResearcherState(  # Direct TypedDict creation
                messages=message_in,
                step_index=idx,
                result=None
            )
        else:
            researcher_state = state["researcher_states"][idx]  # Direct dictionary access
            researcher_state["messages"].append(message_in[-1])  # Direct list append operation

        # Execute research using subgraph
        researcher_state = compiled_researcher_graph.invoke(researcher_state)  # Direct function call
        
        # Validate response
        response = validate_llm_response(researcher_state["messages"][-1].content, config.output_schema.keys(), f"researcher step {state['current_research_index']}")  # Direct function call and f-string formatting
        # Store response in ResearcherState in case of text JSON response
        researcher_state["result"] = response["result"]  # Direct dictionary assignment

        # Update ResearcherState in state
        state["researcher_states"][idx] = researcher_state  # Direct dictionary assignment

        # Store results
        if len(state["research_results"]) <= idx:  # Direct list length check
            state["research_results"].append(response["result"])  # Direct list append operation
        else:
            state["research_results"][idx] = response["result"]  # Direct list assignment

        # Send response to orchestrator
        agent_message = compose_agent_message(  # Direct function call
            sender= "researcher",
            receiver= "orchestrator",
            type= "response",
            content= f"Researcher complete for step {state['current_research_index']}",  # Direct f-string formatting
            step_id= state["current_research_index"],  # Direct dictionary access
        )
        state = send_message(state, agent_message)  # Direct function call
        
        logger.info("Researcher completed successfully")  # Direct logging call
        return state  # Direct return
    return researcher_agent  # Direct function return
```

**Assertion Specifications:**
- Verify factory returns a callable with correct signature
- Verify returned function processes GraphState correctly
- Verify subgraph is invoked with correct ResearcherState
- Verify response is properly validated
- Verify state is updated with research results
- Verify ResearcherState is created or updated correctly
- Verify agent message is composed and sent correctly
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated
- Verify research results are stored at correct index

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_researcher_agent, GraphState, ResearcherState

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_create_researcher_agent_factory(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "test result"}
    mock_compose.return_value = {}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Execute factory function
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Verify factory returns a callable
    assert callable(researcher_agent)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(researcher_agent)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_execution_new_step(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent execution for new research step."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "research result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "research result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="What is the capital of France?",
        current_research_index=0,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert 0 in test_state["researcher_states"]
    assert test_state["research_results"] == ["research result"]
    
    # Verify all mocks were called correctly
    mock_get_conversation.assert_called_once_with(test_state, "researcher", step_id=0)
    mock_convert.assert_called_once_with([{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}])
    mock_subgraph.invoke.assert_called_once()
    mock_validate.assert_called_once_with(mock_researcher_state.__getitem__.return_value.content, ["result"], "researcher step 0")
    mock_compose.assert_called_once()
    mock_send.assert_called_once_with(test_state, {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"})
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Researcher starting execution")
    mock_logger.info.assert_any_call("Researcher completed successfully")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_execution_existing_step(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent execution for existing research step."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "updated result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "updated result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state with existing researcher state
    existing_researcher_state = ResearcherState(
        messages=[Mock()],
        step_index=1,
        result="old result"
    )
    test_state = GraphState(
        question="What is the capital of France?",
        current_research_index=1,
        researcher_states={1: existing_researcher_state},
        research_results=["old result"]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert 1 in test_state["researcher_states"]
    assert test_state["research_results"] == ["old result", "updated result"]
    
    # Verify existing researcher state was updated
    assert test_state["researcher_states"][1] == mock_researcher_state

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_empty_message_history(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent with empty message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []  # Empty conversation
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "test result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        current_research_index=0,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert 0 in test_state["researcher_states"]
    assert test_state["research_results"] == ["test result"]
    
    # Verify subgraph was called with empty messages
    mock_subgraph.invoke.assert_called_once()
    call_args = mock_subgraph.invoke.call_args[0][0]
    assert call_args["messages"] == []

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_large_research_index(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent with large research index."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "test result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state with large research index
    test_state = GraphState(
        question="Test question",
        current_research_index=999,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert 999 in test_state["researcher_states"]
    assert test_state["research_results"] == ["test result"]
    
    # Verify conversation was retrieved with correct step_id
    mock_get_conversation.assert_called_once_with(test_state, "researcher", step_id=999)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_existing_research_results(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent with existing research results."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "new result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "new result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state with existing research results
    test_state = GraphState(
        question="Test question",
        current_research_index=2,
        researcher_states={},
        research_results=["result1", "result2"]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert 2 in test_state["researcher_states"]
    assert test_state["research_results"] == ["result1", "result2", "new result"]

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_update_existing_result(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent updating existing research result."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "updated result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "updated result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state with existing research results
    test_state = GraphState(
        question="Test question",
        current_research_index=1,
        researcher_states={},
        research_results=["result1", "old result", "result3"]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert 1 in test_state["researcher_states"]
    assert test_state["research_results"] == ["result1", "updated result", "result3"]

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_special_characters_response(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent with special characters in response."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        current_research_index=0,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with special characters
    assert test_state["research_results"] == ["Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡"]

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_subgraph_invocation_failure(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent when subgraph invocation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_subgraph.invoke.side_effect = Exception("Subgraph invocation error")
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        current_research_index=0,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent and expect exception
    with pytest.raises(Exception) as exc_info:
        researcher_agent(test_state)
    
    assert "Subgraph invocation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Researcher starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_response_validation_failure(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test researcher agent when response validation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.side_effect = Exception("Validation error")
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        current_research_index=0,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent and expect exception
    with pytest.raises(Exception) as exc_info:
        researcher_agent(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Researcher starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_state_preservation(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that other state fields are preserved."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "test result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state with various fields
    test_state = GraphState(
        question="Test question",
        file=["test.pdf"],
        current_step="researcher",
        next_step="expert",
        current_research_index=0,
        researcher_states={},
        research_results=[],
        agent_messages=["existing message"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_current_step = test_state["current_step"]
    original_next_step = test_state["next_step"]
    original_agent_messages = test_state["agent_messages"]
    original_planner_retry_count = test_state["planner_retry_count"]
    original_researcher_retry_count = test_state["researcher_retry_count"]
    original_expert_retry_count = test_state["expert_retry_count"]
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify research results are updated
    assert test_state["research_results"] == ["test result"]
    assert 0 in test_state["researcher_states"]
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["current_step"] == original_current_step
    assert test_state["next_step"] == original_next_step
    assert test_state["agent_messages"] == original_agent_messages
    assert test_state["planner_retry_count"] == original_planner_retry_count
    assert test_state["researcher_retry_count"] == original_researcher_retry_count
    assert test_state["expert_retry_count"] == original_expert_retry_count

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_researcher_agent_message_composition_verification(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that agent message is composed with correct content."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"result": "test result"}
    mock_compose.return_value = {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"result": "string"}
    
    mock_subgraph = Mock()
    mock_researcher_state = Mock()
    mock_researcher_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"result": "test result"}')]}[key])
    mock_researcher_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_researcher_state
    
    # Create researcher agent
    researcher_agent = create_researcher_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        current_research_index=5,
        researcher_states={},
        research_results=[]
    )
    
    # Execute researcher agent
    result = researcher_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose.assert_called_once_with(
        sender="researcher",
        receiver="orchestrator",
        type="response",
        content="Researcher complete for step 5",
        step_id=5
    )
    
    # Verify send_message was called with correct parameters
    mock_send.assert_called_once_with(test_state, {"sender": "researcher", "receiver": "orchestrator", "type": "response", "content": "test"})
```
#### 3.8.3 Expert Agent Factory

**Function Information:**
- **Function Name**: `create_expert_agent`
- **Location**: `multi_agent_system.py:919`
- **Purpose**: Creates an expert agent function with injected prompt and compiled subgraph, returning a callable that processes GraphState and executes expert reasoning using a subgraph

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompt and output schema
- **Input**: `compiled_expert_graph: Callable` - Compiled subgraph function for expert execution
- **Return**: `Callable[[GraphState], GraphState]` - A function that takes and returns GraphState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `get_agent_conversation()` - Internal function for retrieving conversation history
  - `convert_agent_messages_to_langchain()` - Internal function for message conversion
  - `compiled_expert_graph.invoke()` - Subgraph execution method
  - `validate_llm_response()` - Internal function for response validation
  - `compose_agent_message()` - Internal function for creating agent messages
  - `send_message()` - Internal function for adding messages to state
- **Dependencies to Use Directly**: 
  - `AgentConfig` - Internal configuration class
  - `GraphState` - Internal TypedDict type
  - `ExpertState` - Internal TypedDict type
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
  - List operations (indexing, extend) - Built-in Python features
  - String formatting with f-strings - Built-in Python features
  - Conditional logic and control flow - Built-in Python features
- **Mock Configuration**: Mock all external dependencies to isolate the expert agent logic and verify correct interactions

**Test Cases:**

**Happy Path:**
- **Valid Config and Subgraph**: Function with valid AgentConfig and compiled subgraph
- **Expert Agent Execution**: Returned function processes state correctly
- **New Expert State**: Creating new ExpertState when none exists
- **Existing Expert State**: Updating existing ExpertState
- **Valid Subgraph Response**: Subgraph returns properly formatted response
- **Complete Workflow**: Full expert agent workflow with all steps
- **State Updates**: State is properly updated with expert answer and reasoning

**Edge Cases:**
- **Empty Message History**: No previous conversation messages
- **Large Message History**: Very long conversation history
- **Empty Subgraph Response**: Subgraph returns empty or minimal response
- **Large Response**: Subgraph returns very large response
- **Special Characters**: Response contains special characters and Unicode
- **Missing State Fields**: Some state fields missing or None
- **Empty Research Steps**: No research steps available
- **Empty Research Results**: No research results available
- **Large Research Data**: Very large research steps and results

**Error Conditions:**
- **Invalid Config**: Malformed or invalid AgentConfig
- **Invalid Subgraph**: Malformed or invalid compiled subgraph
- **Subgraph Invocation Failure**: Errors in subgraph invoke method
- **Response Validation Failure**: Invalid or malformed subgraph response
- **Missing Required Fields**: Missing fields needed for processing
- **Logger Failure**: Errors in logging operations
- **Message Conversion Failure**: Errors in message conversion
- **Agent Conversation Failure**: Errors in retrieving conversation history
- **ExpertState Creation Failure**: Errors in creating ExpertState
- **List Index Errors**: Index out of bounds errors

**State Changes:**
- **ExpertState Creation/Update**: ExpertState is created or updated in state
- **Expert Answer Storage**: Expert answer is stored in state
- **Expert Reasoning Storage**: Expert reasoning is stored in state
- **Message Addition**: Agent message is added to state via send_message
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned after processing

**Mock Configurations:**
```python
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_factory_with_mocked_dependencies(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Mock all external dependencies for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock conversation retrieval
    mock_conversation = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "test"}]
    mock_get_conversation.return_value = mock_conversation
    
    # Mock message conversion
    mock_langchain_messages = [Mock()]
    mock_convert.return_value = mock_langchain_messages
    
    # Mock subgraph response
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    
    # Mock compiled subgraph
    mock_subgraph = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Mock response validation
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    
    # Mock agent message composition
    mock_agent_message = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_compose.return_value = mock_agent_message
    
    # Mock message sending
    mock_send.return_value = None
    
    # Setup test config
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and factory pattern in the implementation:
def create_expert_agent(config: AgentConfig, compiled_expert_graph: Callable) -> Callable[[GraphState], GraphState]:  # Direct type annotations
    """Create an expert agent function with the given prompt and LLM."""
    
    def expert_agent(state: GraphState) -> GraphState:  # Direct function definition and type annotations
        """Expert agent with injected prompt."""
        logger.info("Expert starting execution")  # Direct logging call

        message_history = get_agent_conversation(state, "expert")  # Direct function call
        message_in = [convert_agent_messages_to_langchain(message_history)[-1]]  # Direct function call and list indexing
        
        expert_state = state["expert_state"]  # Direct dictionary access
        if expert_state is None:  # Direct None comparison
            expert_state = ExpertState(  # Direct TypedDict creation
                messages=message_in,
                question=state["question"],  # Direct dictionary access
                research_steps=state["research_steps"],  # Direct dictionary access
                research_results=state["research_results"],  # Direct dictionary access
                expert_answer="",
                expert_reasoning="",
            )
        else:
            expert_state["messages"].extend(message_in)  # Direct list extend operation

        # Execute expert reasoning using subgraph
        expert_state = compiled_expert_graph.invoke(expert_state)  # Direct function call

        # Validate response
        response = validate_llm_response(expert_state["messages"][-1].content, config.output_schema.keys(), "expert")  # Direct function call
        
        # Store response in ExpertState in case of text JSON response
        expert_state["expert_answer"] = response["expert_answer"]  # Direct dictionary assignment
        expert_state["expert_reasoning"] = response["expert_reasoning"]  # Direct dictionary assignment

        # Update ExpertState in state
        state["expert_state"] = expert_state  # Direct dictionary assignment

        # Store results
        state["expert_answer"] = response["expert_answer"]  # Direct dictionary assignment
        state["expert_reasoning"] = response["expert_reasoning"]  # Direct dictionary assignment
        
        agent_message = compose_agent_message(  # Direct function call
            sender= "expert",
            receiver= "orchestrator",
            type= "response",
            content= f"Expert complete. Answer: {expert_state['expert_answer']}, Reasoning: {expert_state['expert_reasoning']}",  # Direct f-string formatting
        )
        state = send_message(state, agent_message)  # Direct function call
        
        logger.info("Expert completed successfully")  # Direct logging call
        return state  # Direct return
    
    return expert_agent  # Direct function return
```

**Assertion Specifications:**
- Verify factory returns a callable with correct signature
- Verify returned function processes GraphState correctly
- Verify subgraph is invoked with correct ExpertState
- Verify response is properly validated
- Verify state is updated with expert answer and reasoning
- Verify ExpertState is created or updated correctly
- Verify agent message is composed and sent correctly
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated
- Verify expert answer and reasoning are stored in both ExpertState and main state

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_expert_agent, GraphState, ExpertState

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_create_expert_agent_factory(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    mock_compose.return_value = {}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Execute factory function
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Verify factory returns a callable
    assert callable(expert_agent)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(expert_agent)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_execution_new_state(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent execution for new expert state."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "expert analysis", "expert_reasoning": "detailed reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "expert analysis", "expert_reasoning": "detailed reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="What is the capital of France?",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_state=None
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["expert_answer"] == "expert analysis"
    assert test_state["expert_reasoning"] == "detailed reasoning"
    assert test_state["expert_state"] == mock_expert_state
    
    # Verify all mocks were called correctly
    mock_get_conversation.assert_called_once_with(test_state, "expert")
    mock_convert.assert_called_once_with([{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}])
    mock_subgraph.invoke.assert_called_once()
    mock_validate.assert_called_once_with(mock_expert_state.__getitem__.return_value.content, ["expert_answer", "expert_reasoning"], "expert")
    mock_compose.assert_called_once()
    mock_send.assert_called_once_with(test_state, {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"})
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Expert starting execution")
    mock_logger.info.assert_any_call("Expert completed successfully")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_execution_existing_state(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent execution for existing expert state."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "updated analysis", "expert_reasoning": "updated reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "updated analysis", "expert_reasoning": "updated reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state with existing expert state
    existing_expert_state = ExpertState(
        messages=[Mock()],
        question="What is the capital of France?",
        research_steps=["step1"],
        research_results=["result1"],
        expert_answer="old answer",
        expert_reasoning="old reasoning"
    )
    test_state = GraphState(
        question="What is the capital of France?",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_state=existing_expert_state
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["expert_answer"] == "updated analysis"
    assert test_state["expert_reasoning"] == "updated reasoning"
    assert test_state["expert_state"] == mock_expert_state
    
    # Verify existing expert state was updated
    assert test_state["expert_state"] == mock_expert_state

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_empty_message_history(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent with empty message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []  # Empty conversation
    mock_convert.return_value = []
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"],
        expert_state=None
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["expert_answer"] == "test answer"
    assert test_state["expert_reasoning"] == "test reasoning"
    
    # Verify subgraph was called with empty messages
    mock_subgraph.invoke.assert_called_once()
    call_args = mock_subgraph.invoke.call_args[0][0]
    assert call_args["messages"] == []

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_empty_research_data(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent with empty research steps and results."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state with empty research data
    test_state = GraphState(
        question="Test question",
        research_steps=[],
        research_results=[],
        expert_state=None
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["expert_answer"] == "test answer"
    assert test_state["expert_reasoning"] == "test reasoning"
    
    # Verify subgraph was called with empty research data
    mock_subgraph.invoke.assert_called_once()
    call_args = mock_subgraph.invoke.call_args[0][0]
    assert call_args["research_steps"] == []
    assert call_args["research_results"] == []

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_large_research_data(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent with large research steps and results."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state with large research data
    large_steps = ["step" + str(i) for i in range(1000)]
    large_results = ["result" + str(i) for i in range(1000)]
    test_state = GraphState(
        question="Test question",
        research_steps=large_steps,
        research_results=large_results,
        expert_state=None
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["expert_answer"] == "test answer"
    assert test_state["expert_reasoning"] == "test reasoning"
    
    # Verify subgraph was called with large research data
    mock_subgraph.invoke.assert_called_once()
    call_args = mock_subgraph.invoke.call_args[0][0]
    assert call_args["research_steps"] == large_steps
    assert call_args["research_results"] == large_results

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_special_characters_response(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent with special characters in response."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"expert_answer": "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡", "expert_reasoning": "ðŸŒŸðŸ’¡ðŸš€Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡", "expert_reasoning": "ðŸŒŸðŸ’¡ðŸš€Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"],
        expert_state=None
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with special characters
    assert test_state["expert_answer"] == "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡"
    assert test_state["expert_reasoning"] == "ðŸŒŸðŸ’¡ðŸš€Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_subgraph_invocation_failure(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent when subgraph invocation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_subgraph.invoke.side_effect = Exception("Subgraph invocation error")
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"],
        expert_state=None
    )
    
    # Execute expert agent and expect exception
    with pytest.raises(Exception) as exc_info:
        expert_agent(test_state)
    
    assert "Subgraph invocation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Expert starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_response_validation_failure(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test expert agent when response validation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.side_effect = Exception("Validation error")
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"],
        expert_state=None
    )
    
    # Execute expert agent and expect exception
    with pytest.raises(Exception) as exc_info:
        expert_agent(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Expert starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_state_preservation(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that other state fields are preserved."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state with various fields
    test_state = GraphState(
        question="Test question",
        file=["test.pdf"],
        current_step="expert",
        next_step="critic_expert",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_state=None,
        agent_messages=["existing message"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_current_step = test_state["current_step"]
    original_next_step = test_state["next_step"]
    original_research_steps = test_state["research_steps"]
    original_research_results = test_state["research_results"]
    original_agent_messages = test_state["agent_messages"]
    original_planner_retry_count = test_state["planner_retry_count"]
    original_researcher_retry_count = test_state["researcher_retry_count"]
    original_expert_retry_count = test_state["expert_retry_count"]
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify expert fields are updated
    assert test_state["expert_answer"] == "test answer"
    assert test_state["expert_reasoning"] == "test reasoning"
    assert test_state["expert_state"] == mock_expert_state
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["current_step"] == original_current_step
    assert test_state["next_step"] == original_next_step
    assert test_state["research_steps"] == original_research_steps
    assert test_state["research_results"] == original_research_results
    assert test_state["agent_messages"] == original_agent_messages
    assert test_state["planner_retry_count"] == original_planner_retry_count
    assert test_state["researcher_retry_count"] == original_researcher_retry_count
    assert test_state["expert_retry_count"] == original_expert_retry_count

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
def test_expert_agent_message_composition_verification(mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that agent message is composed with correct content."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"expert_answer": "test answer", "expert_reasoning": "test reasoning"}
    mock_compose.return_value = {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and subgraph
    mock_config = Mock()
    mock_config.output_schema = {"expert_answer": "string", "expert_reasoning": "string"}
    
    mock_subgraph = Mock()
    mock_expert_state = Mock()
    mock_expert_state.__getitem__ = Mock(side_effect=lambda key: {"messages": [Mock(content='{"expert_answer": "test answer", "expert_reasoning": "test reasoning"}')]}[key])
    mock_expert_state.__setitem__ = Mock()
    mock_subgraph.invoke.return_value = mock_expert_state
    
    # Create expert agent
    expert_agent = create_expert_agent(mock_config, mock_subgraph)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"],
        expert_state=None
    )
    
    # Execute expert agent
    result = expert_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose.assert_called_once_with(
        sender="expert",
        receiver="orchestrator",
        type="response",
        content="Expert complete. Answer: test answer, Reasoning: test reasoning"
    )
    
    # Verify send_message was called with correct parameters
    mock_send.assert_called_once_with(test_state, {"sender": "expert", "receiver": "orchestrator", "type": "response", "content": "test"})
```
#### 3.8.4 Critic Agent Factory

**Function Information:**
- **Function Name**: `create_critic_agent`
- **Location**: `multi_agent_system.py:976`
- **Purpose**: Creates a critic agent function with injected prompts for different critic types, returning a callable that processes GraphState and provides criticism based on the current step

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompts for different critic types and output schema
- **Input**: `llm_critic: ChatOpenAI` - Language model instance for criticism
- **Return**: `Callable[[GraphState], GraphState]` - A function that takes and returns GraphState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `SystemMessage` - LangChain message class
  - `get_agent_conversation()` - Internal function for retrieving conversation history
  - `convert_agent_messages_to_langchain()` - Internal function for message conversion
  - `llm_critic.invoke()` - LLM invocation method
  - `validate_llm_response()` - Internal function for response validation
  - `compose_agent_message()` - Internal function for creating agent messages
  - `send_message()` - Internal function for adding messages to state
- **Dependencies to Use Directly**: 
  - `AgentConfig` - Internal configuration class
  - `ChatOpenAI` - LangChain LLM class
  - `GraphState` - Internal TypedDict type
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
  - List operations (indexing, concatenation) - Built-in Python features
  - String formatting with f-strings - Built-in Python features
  - Conditional logic and control flow - Built-in Python features
  - Exception raising (`RuntimeError`) - Built-in Python features
- **Mock Configuration**: Mock all external dependencies to isolate the critic agent logic and verify correct interactions

**Test Cases:**

**Happy Path:**
- **Valid Config and LLM**: Function with valid AgentConfig and ChatOpenAI
- **Critic Agent Execution**: Returned function processes state correctly
- **Critic Planner**: Processing critic_planner current step
- **Critic Researcher**: Processing critic_researcher current step
- **Critic Expert**: Processing critic_expert current step
- **Valid LLM Response**: LLM returns properly formatted response
- **Complete Workflow**: Full critic agent workflow with all steps
- **State Updates**: State is properly updated with critic decisions and feedback

**Edge Cases:**
- **Empty Message History**: No previous conversation messages
- **Large Message History**: Very long conversation history
- **Empty LLM Response**: LLM returns empty or minimal response
- **Large Response**: LLM returns very large response
- **Special Characters**: Response contains special characters and Unicode
- **Missing State Fields**: Some state fields missing or None
- **Empty Critic Prompts**: Empty or minimal critic prompts
- **Large Critic Prompts**: Very large critic prompts

**Error Conditions:**
- **Invalid Config**: Malformed or invalid AgentConfig
- **Invalid LLM**: Malformed or invalid ChatOpenAI instance
- **Invalid Current Step**: Current step not in expected critic types
- **LLM Invocation Failure**: Errors in LLM invoke method
- **Response Validation Failure**: Invalid or malformed LLM response
- **Missing Required Fields**: Missing fields needed for processing
- **Logger Failure**: Errors in logging operations
- **Message Conversion Failure**: Errors in message conversion
- **Agent Conversation Failure**: Errors in retrieving conversation history
- **Missing Critic Prompts**: Missing critic prompts in config

**State Changes:**
- **Critic Decision Storage**: Critic decision is stored based on current step
- **Critic Feedback Storage**: Critic feedback is stored based on current step
- **Message Addition**: Agent message is added to state via send_message
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned after processing

**Mock Configurations:**
```python
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_factory_with_mocked_dependencies(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Mock all external dependencies for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock SystemMessage
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Mock conversation retrieval
    mock_conversation = [{"sender": "orchestrator", "receiver": "critic", "type": "instruction", "content": "test"}]
    mock_get_conversation.return_value = mock_conversation
    
    # Mock message conversion
    mock_langchain_messages = [Mock()]
    mock_convert.return_value = mock_langchain_messages
    
    # Mock LLM response
    mock_llm_response = Mock()
    mock_llm_response.content = '{"decision": "approve", "feedback": "good work"}'
    
    # Mock LLM instance
    mock_llm = Mock()
    mock_llm.invoke.return_value = mock_llm_response
    
    # Mock response validation
    mock_validate.return_value = {"decision": "approve", "feedback": "good work"}
    
    # Mock agent message composition
    mock_agent_message = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_compose.return_value = mock_agent_message
    
    # Mock message sending
    mock_send.return_value = None
    
    # Setup test config
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and factory pattern in the implementation:
def create_critic_agent(config: AgentConfig, llm_critic: ChatOpenAI) -> Callable[[GraphState], GraphState]:  # Direct type annotations
    """Create a critic agent function with the given prompts for different critic types."""
    
    def critic_agent(state: GraphState) -> GraphState:  # Direct function definition and type annotations
        """Critic agent with injected prompts for different critic types."""
        logger.info("Critic starting execution")  # Direct logging call

        # Determine which critic to run and get the appropriate prompt
        if state["current_step"] == "critic_planner":  # Direct dictionary access and string comparison
            critic_prompt = config.system_prompt["critic_planner"]  # Direct dictionary access
        elif state["current_step"] == "critic_researcher":  # Direct dictionary access and string comparison
            critic_prompt = config.system_prompt["critic_researcher"]  # Direct dictionary access
        elif state["current_step"] == "critic_expert":  # Direct dictionary access and string comparison
            critic_prompt = config.system_prompt["critic_expert"]  # Direct dictionary access
        else:
            raise RuntimeError(f"Invalid critic step: {state['current_step']}")  # Direct exception raising with f-string formatting
        
        sys_prompt = [SystemMessage(content=critic_prompt)]  # Direct SystemMessage creation and list creation
        message_history = get_agent_conversation(state, state["current_step"])  # Direct function call
        message_in = [convert_agent_messages_to_langchain(message_history)[-1]]  # Direct function call and list indexing

        response = llm_critic.invoke(sys_prompt + message_in)  # Direct LLM invocation and list concatenation
        
        # Validate response
        response = validate_llm_response(response, config.output_schema.keys(), "critic")  # Direct function call
        
        # Store critic decision and feedback based on current step
        if state["current_step"] == "critic_planner":  # Direct dictionary access and string comparison
            state["critic_planner_decision"] = response["decision"]  # Direct dictionary assignment
            state["critic_planner_feedback"] = response["feedback"]  # Direct dictionary assignment
        elif state["current_step"] == "critic_researcher":  # Direct dictionary access and string comparison
            state["critic_researcher_decision"] = response["decision"]  # Direct dictionary assignment
            state["critic_researcher_feedback"] = response["feedback"]  # Direct dictionary assignment
        elif state["current_step"] == "critic_expert":  # Direct dictionary access and string comparison
            state["critic_expert_decision"] = response["decision"]  # Direct dictionary assignment
            state["critic_expert_feedback"] = response["feedback"]  # Direct dictionary assignment
        
        message = compose_agent_message(  # Direct function call
            sender= "critic",
            receiver= "orchestrator",
            type= "response",
            content= f"Critic complete. Decision: {response['decision']}, Feedback: {response['feedback']}",  # Direct f-string formatting
        )
        state = send_message(state, message)  # Direct function call
        
        logger.info("Critic completed successfully")  # Direct logging call
        return state  # Direct return
    
    return critic_agent  # Direct function return
```

**Assertion Specifications:**
- Verify factory returns a callable with correct signature
- Verify returned function processes GraphState correctly
- Verify correct critic prompt is selected based on current step
- Verify LLM is invoked with correct system prompt and message history
- Verify response is properly validated
- Verify state is updated with correct critic decision and feedback
- Verify agent message is composed and sent correctly
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated
- Verify invalid current steps raise appropriate exceptions

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_critic_agent, GraphState

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_create_critic_agent_factory(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"decision": "approve", "feedback": "good work"}
    mock_compose.return_value = {}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Execute factory function
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Verify factory returns a callable
    assert callable(critic_agent)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(critic_agent)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_execution_planner(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent execution for critic_planner."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "critic", "type": "instruction", "content": "Criticize this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"decision": "approve", "feedback": "good planning"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="What is the capital of France?"
    )
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["critic_planner_decision"] == "approve"
    assert test_state["critic_planner_feedback"] == "good planning"
    
    # Verify all mocks were called correctly
    mock_system_message.assert_called_once_with(content="You are a planner critic")
    mock_get_conversation.assert_called_once_with(test_state, "critic_planner")
    mock_convert.assert_called_once_with([{"sender": "orchestrator", "receiver": "critic", "type": "instruction", "content": "Criticize this"}])
    mock_llm.invoke.assert_called_once()
    mock_validate.assert_called_once_with(mock_llm.invoke.return_value, ["decision", "feedback"], "critic")
    mock_compose.assert_called_once()
    mock_send.assert_called_once_with(test_state, {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"})
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Critic starting execution")
    mock_logger.info.assert_any_call("Critic completed successfully")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_execution_researcher(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent execution for critic_researcher."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "critic", "type": "instruction", "content": "Criticize this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"decision": "reject", "feedback": "needs more research"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_researcher",
        question="What is the capital of France?"
    )
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["critic_researcher_decision"] == "reject"
    assert test_state["critic_researcher_feedback"] == "needs more research"
    
    # Verify correct prompt was used
    mock_system_message.assert_called_once_with(content="You are a researcher critic")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_execution_expert(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent execution for critic_expert."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "critic", "type": "instruction", "content": "Criticize this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"decision": "approve", "feedback": "excellent analysis"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_expert",
        question="What is the capital of France?"
    )
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["critic_expert_decision"] == "approve"
    assert test_state["critic_expert_feedback"] == "excellent analysis"
    
    # Verify correct prompt was used
    mock_system_message.assert_called_once_with(content="You are an expert critic")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_empty_message_history(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent with empty message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []  # Empty conversation
    mock_convert.return_value = []
    mock_validate.return_value = {"decision": "approve", "feedback": "good work"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="Test question"
    )
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["critic_planner_decision"] == "approve"
    assert test_state["critic_planner_feedback"] == "good work"
    
    # Verify LLM was called with system prompt only
    mock_llm.invoke.assert_called_once()
    call_args = mock_llm.invoke.call_args[0][0]
    assert len(call_args) == 1  # Only system prompt
    assert call_args[0] == mock_sys_msg

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_special_characters_response(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent with special characters in response."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"decision": "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢", "feedback": "ðŸš€ðŸŒŸðŸ’¡"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="Test question"
    )
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with special characters
    assert test_state["critic_planner_decision"] == "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"
    assert test_state["critic_planner_feedback"] == "ðŸš€ðŸŒŸðŸ’¡"

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_invalid_current_step(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent with invalid current step."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state with invalid current step
    test_state = GraphState(
        current_step="invalid_critic",
        question="Test question"
    )
    
    # Execute critic agent and expect exception
    with pytest.raises(RuntimeError) as exc_info:
        critic_agent(test_state)
    
    assert "Invalid critic step: invalid_critic" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Critic starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_llm_invocation_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent when LLM invocation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.side_effect = Exception("LLM invocation error")
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="Test question"
    )
    
    # Execute critic agent and expect exception
    with pytest.raises(Exception) as exc_info:
        critic_agent(test_state)
    
    assert "LLM invocation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Critic starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_response_validation_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test critic agent when response validation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.side_effect = Exception("Validation error")
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="Test question"
    )
    
    # Execute critic agent and expect exception
    with pytest.raises(Exception) as exc_info:
        critic_agent(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Critic starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_state_preservation(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that other state fields are preserved."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"decision": "approve", "feedback": "good work"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state with various fields
    test_state = GraphState(
        question="Test question",
        file=["test.pdf"],
        current_step="critic_planner",
        next_step="finalizer",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_answer="test answer",
        expert_reasoning="test reasoning",
        agent_messages=["existing message"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_next_step = test_state["next_step"]
    original_research_steps = test_state["research_steps"]
    original_research_results = test_state["research_results"]
    original_expert_answer = test_state["expert_answer"]
    original_expert_reasoning = test_state["expert_reasoning"]
    original_agent_messages = test_state["agent_messages"]
    original_planner_retry_count = test_state["planner_retry_count"]
    original_researcher_retry_count = test_state["researcher_retry_count"]
    original_expert_retry_count = test_state["expert_retry_count"]
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify critic fields are updated
    assert test_state["critic_planner_decision"] == "approve"
    assert test_state["critic_planner_feedback"] == "good work"
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["next_step"] == original_next_step
    assert test_state["research_steps"] == original_research_steps
    assert test_state["research_results"] == original_research_results
    assert test_state["expert_answer"] == original_expert_answer
    assert test_state["expert_reasoning"] == original_expert_reasoning
    assert test_state["agent_messages"] == original_agent_messages
    assert test_state["planner_retry_count"] == original_planner_retry_count
    assert test_state["researcher_retry_count"] == original_researcher_retry_count
    assert test_state["expert_retry_count"] == original_expert_retry_count

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_message_composition_verification(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that agent message is composed with correct content."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"decision": "approve", "feedback": "excellent work"}
    mock_compose.return_value = {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = {
        "critic_planner": "You are a planner critic",
        "critic_researcher": "You are a researcher critic",
        "critic_expert": "You are an expert critic"
    }
    mock_config.output_schema = {"decision": "string", "feedback": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create critic agent
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        current_step="critic_planner",
        question="Test question"
    )
    
    # Execute critic agent
    result = critic_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose.assert_called_once_with(
        sender="critic",
        receiver="orchestrator",
        type="response",
        content="Critic complete. Decision: approve, Feedback: excellent work"
    )
    
    # Verify send_message was called with correct parameters
    mock_send.assert_called_once_with(test_state, {"sender": "critic", "receiver": "orchestrator", "type": "response", "content": "test"})
```
#### 3.8.5 Finalizer Agent Factory

**Function Information:**
- **Function Name**: `create_finalizer_agent`
- **Location**: `multi_agent_system.py:1028`
- **Purpose**: Creates a finalizer agent function with injected prompt and LLM, returning a callable that processes GraphState and generates final answers and reasoning traces

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing system prompt and output schema
- **Input**: `llm_finalizer: ChatOpenAI` - Language model instance for finalization
- **Return**: `Callable[[GraphState], GraphState]` - A function that takes and returns GraphState
- **Side Effects**: None (function factory, side effects occur in returned function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for consistent testing
  - `SystemMessage` - LangChain message class
  - `get_agent_conversation()` - Internal function for retrieving conversation history
  - `convert_agent_messages_to_langchain()` - Internal function for message conversion
  - `llm_finalizer.invoke()` - LLM invocation method
  - `validate_llm_response()` - Internal function for response validation
  - `compose_agent_message()` - Internal function for creating agent messages
  - `send_message()` - Internal function for adding messages to state
- **Dependencies to Use Directly**: 
  - `AgentConfig` - Internal configuration class
  - `ChatOpenAI` - LangChain LLM class
  - `GraphState` - Internal TypedDict type
  - Function definition and closure creation - Built-in Python features
  - Dictionary access and assignment - Built-in Python operations
  - List concatenation - Built-in Python operations
  - String formatting with f-strings - Built-in Python features
- **Mock Configuration**: Mock all external dependencies to isolate the finalizer agent logic and verify correct interactions

**Test Cases:**

**Happy Path:**
- **Valid Config and LLM**: Function with valid AgentConfig and ChatOpenAI
- **Finalizer Agent Execution**: Returned function processes state correctly
- **Valid LLM Response**: LLM returns properly formatted response
- **Complete Workflow**: Full finalizer agent workflow with all steps
- **State Updates**: State is properly updated with final answer and reasoning trace

**Edge Cases:**
- **Empty Message History**: No previous conversation messages
- **Large Message History**: Very long conversation history
- **Empty LLM Response**: LLM returns empty or minimal response
- **Large Response**: LLM returns very large response
- **Special Characters**: Response contains special characters and Unicode
- **Missing State Fields**: Some state fields missing or None
- **Empty Final Answer**: LLM returns empty final answer
- **Empty Reasoning Trace**: LLM returns empty reasoning trace
- **Large Final Answer**: Very large final answer
- **Large Reasoning Trace**: Very large reasoning trace

**Error Conditions:**
- **Invalid Config**: Malformed or invalid AgentConfig
- **Invalid LLM**: Malformed or invalid ChatOpenAI instance
- **LLM Invocation Failure**: Errors in LLM invoke method
- **Response Validation Failure**: Invalid or malformed LLM response
- **Missing Required Fields**: Missing fields needed for processing
- **Logger Failure**: Errors in logging operations
- **Message Conversion Failure**: Errors in message conversion
- **Agent Conversation Failure**: Errors in retrieving conversation history
- **Missing Response Fields**: Missing final_answer or final_reasoning_trace in response

**State Changes:**
- **Final Answer Storage**: final_answer field is set from LLM response
- **Final Reasoning Trace Storage**: final_reasoning_trace field is set from LLM response
- **Message Addition**: Agent message is added to state via send_message
- **State Preservation**: Other state fields remain unchanged
- **Return Value**: Same state object is returned after processing

**Mock Configurations:**
```python
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_factory_with_mocked_dependencies(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Mock all external dependencies for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock SystemMessage
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    
    # Mock conversation retrieval
    mock_conversation = [{"sender": "orchestrator", "receiver": "finalizer", "type": "instruction", "content": "test"}]
    mock_get_conversation.return_value = mock_conversation
    
    # Mock message conversion
    mock_langchain_messages = [Mock()]
    mock_convert.return_value = mock_langchain_messages
    
    # Mock LLM response
    mock_llm_response = Mock()
    mock_llm_response.content = '{"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}'
    
    # Mock LLM instance
    mock_llm = Mock()
    mock_llm.invoke.return_value = mock_llm_response
    
    # Mock response validation
    mock_validate.return_value = {"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}
    
    # Mock agent message composition
    mock_agent_message = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_compose.return_value = mock_agent_message
    
    # Mock message sending
    mock_send.return_value = None
    
    # Setup test config
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Test implementation
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and factory pattern in the implementation:
def create_finalizer_agent(config: AgentConfig, llm_finalizer: ChatOpenAI) -> Callable[[GraphState], GraphState]:  # Direct type annotations
    """Create a finalizer agent function with the given prompt and LLM."""
    
    def finalizer_agent(state: GraphState) -> GraphState:  # Direct function definition and type annotations
        """Finalizer agent with injected prompt."""
        logger.info("Finalizer starting execution")  # Direct logging call
        
        sys_prompt = [SystemMessage(content=config.system_prompt)]  # Direct SystemMessage creation and list creation
        message_history = get_agent_conversation(state, "finalizer")  # Direct function call
        message_in = convert_agent_messages_to_langchain(message_history)  # Direct function call
        
        response = llm_finalizer.invoke(sys_prompt + message_in)  # Direct LLM invocation and list concatenation
        
        # Validate response
        response = validate_llm_response(response, config.output_schema.keys(), "finalizer")  # Direct function call
        
        state["final_answer"] = response["final_answer"]  # Direct dictionary assignment
        state["final_reasoning_trace"] = response["final_reasoning_trace"]  # Direct dictionary assignment
        
        message = compose_agent_message(  # Direct function call
            sender= "finalizer",
            receiver= "orchestrator",
            type= "response",
            content= f"Finalizer complete. The final answer is:\n{response['final_answer']}\n\n## The final reasoning trace is:\n{response['final_reasoning_trace']}",  # Direct f-string formatting
        )
        state = send_message(state, message)  # Direct function call
        
        logger.info("Finalizer completed successfully")  # Direct logging call
        return state  # Direct return
    
    return finalizer_agent  # Direct function return
```

**Assertion Specifications:**
- Verify factory returns a callable with correct signature
- Verify returned function processes GraphState correctly
- Verify LLM is invoked with correct system prompt and message history
- Verify response is properly validated
- Verify state is updated with final answer and reasoning trace
- Verify agent message is composed and sent correctly
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated
- Verify final answer and reasoning trace are stored correctly

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import create_finalizer_agent, GraphState

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_create_finalizer_agent_factory(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test the factory function returns a callable."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}
    mock_compose.return_value = {}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Execute factory function
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Verify factory returns a callable
    assert callable(finalizer_agent)
    
    # Verify function signature (basic check)
    import inspect
    sig = inspect.signature(finalizer_agent)
    assert len(sig.parameters) == 1  # Takes one parameter (state)

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_execution(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent execution with valid inputs."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "finalizer", "type": "instruction", "content": "Finalize this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"final_answer": "The capital of France is Paris", "final_reasoning_trace": "Based on research and expert analysis..."}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="What is the capital of France?",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_answer="Paris",
        expert_reasoning="Based on geography"
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["final_answer"] == "The capital of France is Paris"
    assert test_state["final_reasoning_trace"] == "Based on research and expert analysis..."
    
    # Verify all mocks were called correctly
    mock_system_message.assert_called_once_with(content="You are a finalizer agent")
    mock_get_conversation.assert_called_once_with(test_state, "finalizer")
    mock_convert.assert_called_once_with([{"sender": "orchestrator", "receiver": "finalizer", "type": "instruction", "content": "Finalize this"}])
    mock_llm.invoke.assert_called_once()
    mock_validate.assert_called_once_with(mock_llm.invoke.return_value, ["final_answer", "final_reasoning_trace"], "finalizer")
    mock_compose.assert_called_once()
    mock_send.assert_called_once_with(test_state, {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"})
    
    # Verify logging calls
    mock_logger.info.assert_any_call("Finalizer starting execution")
    mock_logger.info.assert_any_call("Finalizer completed successfully")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_empty_message_history(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent with empty message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []  # Empty conversation
    mock_convert.return_value = []
    mock_validate.return_value = {"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["final_answer"] == "test answer"
    assert test_state["final_reasoning_trace"] == "test reasoning"
    
    # Verify LLM was called with system prompt only
    mock_llm.invoke.assert_called_once()
    call_args = mock_llm.invoke.call_args[0][0]
    assert len(call_args) == 1  # Only system prompt
    assert call_args[0] == mock_sys_msg

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_large_message_history(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent with large message history."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "finalizer", "type": "instruction", "content": "x" * 10000}]  # Large message
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly
    assert test_state["final_answer"] == "test answer"
    assert test_state["final_reasoning_trace"] == "test reasoning"
    
    # Verify LLM was called with system prompt and large message
    mock_llm.invoke.assert_called_once()
    call_args = mock_llm.invoke.call_args[0][0]
    assert len(call_args) == 2  # System prompt + converted message

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_special_characters_response(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent with special characters in response."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"final_answer": "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡", "final_reasoning_trace": "ðŸŒŸðŸ’¡ðŸš€Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with special characters
    assert test_state["final_answer"] == "Ã©Ã±Ã¼ÃŸÂ©Â®â„¢ðŸš€ðŸŒŸðŸ’¡"
    assert test_state["final_reasoning_trace"] == "ðŸŒŸðŸ’¡ðŸš€Ã©Ã±Ã¼ÃŸÂ©Â®â„¢"

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_empty_response_fields(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent with empty final answer and reasoning trace."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"final_answer": "", "final_reasoning_trace": ""}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with empty fields
    assert test_state["final_answer"] == ""
    assert test_state["final_reasoning_trace"] == ""

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_large_response_fields(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent with large final answer and reasoning trace."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    large_answer = "x" * 10000
    large_reasoning = "y" * 10000
    mock_validate.return_value = {"final_answer": large_answer, "final_reasoning_trace": large_reasoning}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify state is updated correctly with large fields
    assert test_state["final_answer"] == large_answer
    assert test_state["final_reasoning_trace"] == large_reasoning

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_llm_invocation_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent when LLM invocation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.side_effect = Exception("LLM invocation error")
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent and expect exception
    with pytest.raises(Exception) as exc_info:
        finalizer_agent(test_state)
    
    assert "LLM invocation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Finalizer starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_response_validation_failure(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test finalizer agent when response validation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.side_effect = Exception("Validation error")
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent and expect exception
    with pytest.raises(Exception) as exc_info:
        finalizer_agent(test_state)
    
    assert "Validation error" in str(exc_info.value)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with("Finalizer starting execution")

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_state_preservation(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that other state fields are preserved."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state with various fields
    test_state = GraphState(
        question="Test question",
        file=["test.pdf"],
        current_step="finalizer",
        next_step="",
        research_steps=["step1", "step2"],
        research_results=["result1", "result2"],
        expert_answer="test answer",
        expert_reasoning="test reasoning",
        agent_messages=["existing message"],
        planner_retry_count=1,
        researcher_retry_count=2,
        expert_retry_count=0
    )
    
    # Store original values
    original_question = test_state["question"]
    original_file = test_state["file"]
    original_current_step = test_state["current_step"]
    original_next_step = test_state["next_step"]
    original_research_steps = test_state["research_steps"]
    original_research_results = test_state["research_results"]
    original_expert_answer = test_state["expert_answer"]
    original_expert_reasoning = test_state["expert_reasoning"]
    original_agent_messages = test_state["agent_messages"]
    original_planner_retry_count = test_state["planner_retry_count"]
    original_researcher_retry_count = test_state["researcher_retry_count"]
    original_expert_retry_count = test_state["expert_retry_count"]
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify finalizer fields are updated
    assert test_state["final_answer"] == "test answer"
    assert test_state["final_reasoning_trace"] == "test reasoning"
    
    # Verify other fields are preserved
    assert test_state["question"] == original_question
    assert test_state["file"] == original_file
    assert test_state["current_step"] == original_current_step
    assert test_state["next_step"] == original_next_step
    assert test_state["research_steps"] == original_research_steps
    assert test_state["research_results"] == original_research_results
    assert test_state["expert_answer"] == original_expert_answer
    assert test_state["expert_reasoning"] == original_expert_reasoning
    assert test_state["agent_messages"] == original_agent_messages
    assert test_state["planner_retry_count"] == original_planner_retry_count
    assert test_state["researcher_retry_count"] == original_researcher_retry_count
    assert test_state["expert_retry_count"] == original_expert_retry_count

@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.logger')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_message_composition_verification(mock_system_message, mock_logger, mock_get_conversation, mock_convert, mock_validate, mock_compose, mock_send):
    """Test that agent message is composed with correct content."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_sys_msg = Mock()
    mock_system_message.return_value = mock_sys_msg
    mock_get_conversation.return_value = []
    mock_convert.return_value = []
    mock_validate.return_value = {"final_answer": "test answer", "final_reasoning_trace": "test reasoning"}
    mock_compose.return_value = {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"}
    mock_send.return_value = None
    
    # Setup test config and LLM
    mock_config = Mock()
    mock_config.system_prompt = "You are a finalizer agent"
    mock_config.output_schema = {"final_answer": "string", "final_reasoning_trace": "string"}
    
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Create finalizer agent
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Setup test state
    test_state = GraphState(
        question="Test question",
        research_steps=["step1"],
        research_results=["result1"]
    )
    
    # Execute finalizer agent
    result = finalizer_agent(test_state)
    
    # Verify result is the same state object
    assert result is test_state
    
    # Verify compose_agent_message was called with correct parameters
    mock_compose.assert_called_once_with(
        sender="finalizer",
        receiver="orchestrator",
        type="response",
        content="Finalizer complete. The final answer is:\ntest answer\n\n## The final reasoning trace is:\ntest reasoning"
    )
    
    # Verify send_message was called with correct parameters
    mock_send.assert_called_once_with(test_state, {"sender": "finalizer", "receiver": "orchestrator", "type": "response", "content": "test"})
```
### 3.9 LLM Factories
#### 3.9.1 OpenAI LLM Factory

**Function Information:**
- **Function Name**: `openai_llm_factory`
- **Location**: `multi_agent_system.py:1059`
- **Purpose**: Creates an OpenAI LLM instance with specified model, temperature, and structured output schema using JSON mode

**Function Signature and Parameters:**
- **Input**: `model: str` - The OpenAI model name to use (e.g., "gpt-4", "gpt-3.5-turbo")
- **Input**: `temperature: float` - The temperature setting for response randomness (0.0 to 2.0)
- **Input**: `output_schema: dict` - The JSON schema defining the expected output structure
- **Return**: `ChatOpenAI` - Configured LangChain ChatOpenAI instance with structured output
- **Side Effects**: None (pure factory function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `ChatOpenAI` - LangChain LLM class constructor
  - `ChatOpenAI.with_structured_output()` - Method to configure structured output
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, parameter passing, variable assignment, return statement)
  - String and float parameter types
  - Dictionary parameter type
- **Mock Configuration**: Mock ChatOpenAI constructor and with_structured_output method to verify correct configuration

**Test Cases:**

**Happy Path:**
- **Valid Model and Temperature**: Function with valid model name and temperature
- **Valid Output Schema**: Function with valid JSON schema dictionary
- **Complete Configuration**: Full configuration with all parameters
- **Structured Output Setup**: Proper setup of structured output with JSON mode
- **Return Value**: Correct ChatOpenAI instance is returned

**Edge Cases:**
- **Zero Temperature**: Temperature set to 0.0 (deterministic responses)
- **High Temperature**: Temperature set to 2.0 (maximum randomness)
- **Empty Output Schema**: Empty dictionary for output schema
- **Complex Output Schema**: Nested and complex JSON schema
- **Large Output Schema**: Very large JSON schema dictionary
- **Special Characters in Model**: Model name with special characters
- **Float Precision**: Temperature with high precision (e.g., 0.123456789)

**Error Conditions:**
- **Invalid Model Name**: Non-existent or invalid model name
- **Invalid Temperature**: Temperature outside valid range (negative or > 2.0)
- **Invalid Output Schema**: Malformed or invalid JSON schema
- **ChatOpenAI Constructor Failure**: Errors in LLM instantiation
- **Structured Output Setup Failure**: Errors in with_structured_output method
- **Missing Parameters**: Missing required parameters
- **Wrong Parameter Types**: Incorrect data types for parameters

**State Changes:**
- **No State Changes**: Function is pure and doesn't modify any external state
- **Return Value**: New ChatOpenAI instance is created and returned
- **Object Configuration**: Returned object is properly configured with parameters

**Mock Configurations:**
```python
@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_with_mocked_dependencies(mock_chat_openai):
    """Mock ChatOpenAI for testing."""
    # Mock ChatOpenAI constructor
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    
    # Mock with_structured_output method
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters
    test_model = "gpt-4"
    test_temperature = 0.7
    test_output_schema = {"type": "object", "properties": {"answer": {"type": "string"}}}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called with correct parameters
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=test_temperature)
    
    # Verify with_structured_output was called with correct parameters
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and factory pattern in the implementation:
def openai_llm_factory(model: str, temperature: float, output_schema: dict) -> ChatOpenAI:  # Direct type annotations
    """Create an OpenAI LLM with the given model and temperature.

    Args:
        model (str): The model to use  # Direct string type annotation
        temperature (float): The temperature to use  # Direct float type annotation
        output_schema (dict): The output schema to use (JSON schema)  # Direct dict type annotation
    """
    llm = ChatOpenAI(model=model, temperature=temperature)  # Direct ChatOpenAI instantiation with parameter passing
    llm = llm.with_structured_output(output_schema, method="json_mode")  # Direct method call with string literal
    return llm  # Direct return statement
```

**Assertion Specifications:**
- Verify ChatOpenAI constructor is called with correct model and temperature parameters
- Verify with_structured_output method is called with correct output_schema and method="json_mode"
- Verify the returned object is the configured ChatOpenAI instance
- Verify error conditions are properly handled and propagated
- Verify parameter types are correctly passed through
- Verify the function returns the expected ChatOpenAI type

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import openai_llm_factory

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_valid_parameters(mock_chat_openai):
    """Test OpenAI LLM factory with valid parameters."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters
    test_model = "gpt-4"
    test_temperature = 0.7
    test_output_schema = {
        "type": "object",
        "properties": {
            "answer": {"type": "string"},
            "reasoning": {"type": "string"}
        },
        "required": ["answer", "reasoning"]
    }
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called with correct parameters
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=test_temperature)
    
    # Verify with_structured_output was called with correct parameters
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_zero_temperature(mock_chat_openai):
    """Test OpenAI LLM factory with zero temperature."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters
    test_model = "gpt-3.5-turbo"
    test_temperature = 0.0
    test_output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called with zero temperature
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=0.0)
    
    # Verify with_structured_output was called correctly
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_high_temperature(mock_chat_openai):
    """Test OpenAI LLM factory with high temperature."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters
    test_model = "gpt-4"
    test_temperature = 2.0
    test_output_schema = {"type": "object", "properties": {"response": {"type": "string"}}}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called with high temperature
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=2.0)
    
    # Verify with_structured_output was called correctly
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_empty_output_schema(mock_chat_openai):
    """Test OpenAI LLM factory with empty output schema."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters
    test_model = "gpt-4"
    test_temperature = 0.5
    test_output_schema = {}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called correctly
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=0.5)
    
    # Verify with_structured_output was called with empty schema
    mock_llm_instance.with_structured_output.assert_called_once_with({}, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_complex_output_schema(mock_chat_openai):
    """Test OpenAI LLM factory with complex nested output schema."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters with complex schema
    test_model = "gpt-4"
    test_temperature = 0.3
    test_output_schema = {
        "type": "object",
        "properties": {
            "analysis": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "details": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "point": {"type": "string"},
                                "confidence": {"type": "number"}
                            }
                        }
                    }
                }
            },
            "recommendations": {
                "type": "array",
                "items": {"type": "string"}
            }
        },
        "required": ["analysis", "recommendations"]
    }
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called correctly
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=0.3)
    
    # Verify with_structured_output was called with complex schema
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_large_output_schema(mock_chat_openai):
    """Test OpenAI LLM factory with large output schema."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters with large schema
    test_model = "gpt-4"
    test_temperature = 0.8
    test_output_schema = {
        "type": "object",
        "properties": {
            f"field_{i}": {"type": "string", "description": f"Field number {i}"}
            for i in range(100)  # Large schema with 100 fields
        }
    }
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called correctly
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=0.8)
    
    # Verify with_structured_output was called with large schema
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_special_characters_model(mock_chat_openai):
    """Test OpenAI LLM factory with special characters in model name."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters with special characters
    test_model = "gpt-4-turbo-preview-0125"
    test_temperature = 0.6
    test_output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called with special characters in model name
    mock_chat_openai.assert_called_once_with(model="gpt-4-turbo-preview-0125", temperature=0.6)
    
    # Verify with_structured_output was called correctly
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_float_precision(mock_chat_openai):
    """Test OpenAI LLM factory with high precision temperature."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters with high precision
    test_model = "gpt-3.5-turbo"
    test_temperature = 0.123456789
    test_output_schema = {"type": "object", "properties": {"output": {"type": "string"}}}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify ChatOpenAI was called with precise temperature
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=0.123456789)
    
    # Verify with_structured_output was called correctly
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_chatopenai_constructor_failure(mock_chat_openai):
    """Test OpenAI LLM factory when ChatOpenAI constructor fails."""
    # Setup mock to raise exception
    mock_chat_openai.side_effect = Exception("ChatOpenAI initialization error")
    
    # Test parameters
    test_model = "invalid-model"
    test_temperature = 0.5
    test_output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    assert "ChatOpenAI initialization error" in str(exc_info.value)
    
    # Verify ChatOpenAI was called
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=test_temperature)

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_structured_output_failure(mock_chat_openai):
    """Test OpenAI LLM factory when with_structured_output fails."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.side_effect = Exception("Structured output setup error")
    
    # Test parameters
    test_model = "gpt-4"
    test_temperature = 0.7
    test_output_schema = {"invalid": "schema"}
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    assert "Structured output setup error" in str(exc_info.value)
    
    # Verify ChatOpenAI was called
    mock_chat_openai.assert_called_once_with(model=test_model, temperature=test_temperature)
    
    # Verify with_structured_output was called
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_return_type_verification(mock_chat_openai):
    """Test that the function returns the correct type."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters
    test_model = "gpt-4"
    test_temperature = 0.5
    test_output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify result is the same object returned by with_structured_output
    assert result == mock_llm_instance
    
    # Verify the object has the expected methods (basic type check)
    assert hasattr(result, 'with_structured_output')

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_parameter_preservation(mock_chat_openai):
    """Test that parameters are preserved exactly as passed."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test parameters with exact values
    test_model = "gpt-4o-mini"
    test_temperature = 0.999
    test_output_schema = {
        "type": "object",
        "properties": {
            "exact": {"type": "string", "description": "Exact preservation test"}
        }
    }
    
    # Execute function
    result = openai_llm_factory(test_model, test_temperature, test_output_schema)
    
    # Verify parameters are passed exactly as provided
    mock_chat_openai.assert_called_once_with(model="gpt-4o-mini", temperature=0.999)
    mock_llm_instance.with_structured_output.assert_called_once_with(test_output_schema, method="json_mode")
    
    # Verify result is the configured LLM instance
    assert result == mock_llm_instance

@patch('multi_agent_system.ChatOpenAI')
def test_openai_llm_factory_json_mode_verification(mock_chat_openai):
    """Test that json_mode is always used for structured output."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_chat_openai.return_value = mock_llm_instance
    mock_llm_instance.with_structured_output.return_value = mock_llm_instance
    
    # Test with different schemas to ensure json_mode is always used
    test_cases = [
        {"type": "object", "properties": {"simple": {"type": "string"}}},
        {"type": "array", "items": {"type": "string"}},
        {"type": "string"},
        {"type": "number"}
    ]
    
    for test_schema in test_cases:
        # Reset mock call count
        mock_chat_openai.reset_mock()
        mock_llm_instance.with_structured_output.reset_mock()
        
        # Execute function
        result = openai_llm_factory("gpt-4", 0.5, test_schema)
        
        # Verify json_mode is always used
        mock_llm_instance.with_structured_output.assert_called_once_with(test_schema, method="json_mode")
        
        # Verify result is the configured LLM instance
        assert result == mock_llm_instance
```
#### 3.9.2 LLM Factory

**Function Information:**
- **Function Name**: `llm_factory`
- **Location**: `multi_agent_system.py:1072`
- **Purpose**: Routes to the appropriate LLM factory based on the provider specified in the AgentConfig, currently supporting OpenAI provider

**Function Signature and Parameters:**
- **Input**: `config: AgentConfig` - Configuration object containing provider, model, temperature, and output schema
- **Return**: `ChatOpenAI` - Configured LangChain ChatOpenAI instance with structured output
- **Side Effects**: None (pure factory function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `openai_llm_factory()` - Internal function for creating OpenAI LLMs
- **Dependencies to Use Directly**: 
  - `AgentConfig` - Internal configuration class
  - Built-in Python features (function definition, conditional logic, exception raising, f-string formatting, return statement)
  - Dictionary access for config attributes
  - String comparison for provider checking
- **Mock Configuration**: Mock openai_llm_factory to verify correct routing and parameter passing

**Test Cases:**

**Happy Path:**
- **Valid OpenAI Config**: Function with valid AgentConfig for OpenAI provider
- **Correct Parameter Passing**: Parameters are correctly passed to openai_llm_factory
- **Return Value**: Correct ChatOpenAI instance is returned
- **Provider Routing**: Correct routing to OpenAI factory

**Edge Cases:**
- **Empty Provider String**: Provider field is empty string
- **Whitespace Provider**: Provider field contains only whitespace
- **Case Sensitivity**: Provider field with different case (e.g., "OpenAI", "OPENAI")
- **Large Config Object**: AgentConfig with large/complex attributes
- **Minimal Config**: AgentConfig with minimal required attributes
- **Special Characters in Provider**: Provider field with special characters

**Error Conditions:**
- **Invalid Provider**: Provider not supported (e.g., "anthropic", "google")
- **Missing Provider**: Provider field is None or missing
- **OpenAI Factory Failure**: Errors in openai_llm_factory function
- **Invalid Config**: Malformed or invalid AgentConfig
- **Missing Required Fields**: Missing model, temperature, or output_schema in config
- **Wrong Config Type**: Config parameter is not AgentConfig type

**State Changes:**
- **No State Changes**: Function is pure and doesn't modify any external state
- **Return Value**: New ChatOpenAI instance is created and returned
- **Object Configuration**: Returned object is properly configured with parameters

**Mock Configurations:**
```python
@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_with_mocked_dependencies(mock_openai_factory):
    """Mock openai_llm_factory for testing."""
    # Mock openai_llm_factory return value
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.7
    mock_config.output_schema = {"type": "object", "properties": {"answer": {"type": "string"}}}
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with correct parameters
    mock_openai_factory.assert_called_once_with("gpt-4", 0.7, {"type": "object", "properties": {"answer": {"type": "string"}}})
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and routing logic in the implementation:
def llm_factory(config:AgentConfig) -> ChatOpenAI:  # Direct type annotations
    """Get the appropriate LLM factory based on the provider.

    Args:
        config (AgentConfig): The configuration for the agent  # Direct AgentConfig type annotation

    Returns:
        ChatOpenAI: The LLM with structured output  # Direct return type annotation
    """
    if config.provider == "openai":  # Direct dictionary access and string comparison
        return openai_llm_factory(config.model, config.temperature, config.output_schema)  # Direct function call with parameter passing
    else:
        raise ValueError(f"Invalid provider: {config.provider}")  # Direct exception raising with f-string formatting
```

**Assertion Specifications:**
- Verify openai_llm_factory is called with correct parameters when provider is "openai"
- Verify the returned object is the ChatOpenAI instance from openai_llm_factory
- Verify error conditions are properly handled and propagated
- Verify invalid providers raise appropriate ValueError exceptions
- Verify parameter types are correctly passed through
- Verify the function returns the expected ChatOpenAI type

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import llm_factory

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_provider(mock_openai_factory):
    """Test LLM factory with OpenAI provider."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.7
    mock_config.output_schema = {
        "type": "object",
        "properties": {
            "answer": {"type": "string"},
            "reasoning": {"type": "string"}
        },
        "required": ["answer", "reasoning"]
    }
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with correct parameters
    mock_openai_factory.assert_called_once_with("gpt-4", 0.7, {
        "type": "object",
        "properties": {
            "answer": {"type": "string"},
            "reasoning": {"type": "string"}
        },
        "required": ["answer", "reasoning"]
    })
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_provider_different_model(mock_openai_factory):
    """Test LLM factory with OpenAI provider and different model."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config with different model
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-3.5-turbo"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with correct parameters
    mock_openai_factory.assert_called_once_with("gpt-3.5-turbo", 0.5, {"type": "object", "properties": {"result": {"type": "string"}}})
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_provider_zero_temperature(mock_openai_factory):
    """Test LLM factory with OpenAI provider and zero temperature."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config with zero temperature
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.0
    mock_config.output_schema = {"type": "object", "properties": {"output": {"type": "string"}}}
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with zero temperature
    mock_openai_factory.assert_called_once_with("gpt-4", 0.0, {"type": "object", "properties": {"output": {"type": "string"}}})
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_provider_high_temperature(mock_openai_factory):
    """Test LLM factory with OpenAI provider and high temperature."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config with high temperature
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 2.0
    mock_config.output_schema = {"type": "object", "properties": {"response": {"type": "string"}}}
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with high temperature
    mock_openai_factory.assert_called_once_with("gpt-4", 2.0, {"type": "object", "properties": {"response": {"type": "string"}}})
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_provider_complex_schema(mock_openai_factory):
    """Test LLM factory with OpenAI provider and complex output schema."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config with complex schema
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.3
    mock_config.output_schema = {
        "type": "object",
        "properties": {
            "analysis": {
                "type": "object",
                "properties": {
                    "summary": {"type": "string"},
                    "details": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "point": {"type": "string"},
                                "confidence": {"type": "number"}
                            }
                        }
                    }
                }
            },
            "recommendations": {
                "type": "array",
                "items": {"type": "string"}
            }
        },
        "required": ["analysis", "recommendations"]
    }
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with complex schema
    mock_openai_factory.assert_called_once_with("gpt-4", 0.3, mock_config.output_schema)
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_provider_empty_schema(mock_openai_factory):
    """Test LLM factory with OpenAI provider and empty output schema."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config with empty schema
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {}
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify openai_llm_factory was called with empty schema
    mock_openai_factory.assert_called_once_with("gpt-4", 0.5, {})
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

def test_llm_factory_invalid_provider():
    """Test LLM factory with invalid provider."""
    # Setup test config with invalid provider
    mock_config = Mock()
    mock_config.provider = "anthropic"
    mock_config.model = "claude-3"
    mock_config.temperature = 0.7
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider: anthropic" in str(exc_info.value)

def test_llm_factory_google_provider():
    """Test LLM factory with Google provider (unsupported)."""
    # Setup test config with Google provider
    mock_config = Mock()
    mock_config.provider = "google"
    mock_config.model = "gemini-pro"
    mock_config.temperature = 0.8
    mock_config.output_schema = {"type": "object", "properties": {"output": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider: google" in str(exc_info.value)

def test_llm_factory_empty_provider():
    """Test LLM factory with empty provider string."""
    # Setup test config with empty provider
    mock_config = Mock()
    mock_config.provider = ""
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider: " in str(exc_info.value)

def test_llm_factory_whitespace_provider():
    """Test LLM factory with whitespace-only provider."""
    # Setup test config with whitespace provider
    mock_config = Mock()
    mock_config.provider = "   "
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider:    " in str(exc_info.value)

def test_llm_factory_case_sensitive_provider():
    """Test LLM factory with case-sensitive provider (should fail)."""
    # Setup test config with different case
    mock_config = Mock()
    mock_config.provider = "OpenAI"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider: OpenAI" in str(exc_info.value)

def test_llm_factory_uppercase_provider():
    """Test LLM factory with uppercase provider (should fail)."""
    # Setup test config with uppercase provider
    mock_config = Mock()
    mock_config.provider = "OPENAI"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider: OPENAI" in str(exc_info.value)

def test_llm_factory_special_characters_provider():
    """Test LLM factory with special characters in provider."""
    # Setup test config with special characters
    mock_config = Mock()
    mock_config.provider = "openai-v2"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(ValueError) as exc_info:
        llm_factory(mock_config)
    
    assert "Invalid provider: openai-v2" in str(exc_info.value)

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_openai_factory_failure(mock_openai_factory):
    """Test LLM factory when openai_llm_factory fails."""
    # Setup mock to raise exception
    mock_openai_factory.side_effect = Exception("OpenAI factory error")
    
    # Setup test config
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.7
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        llm_factory(mock_config)
    
    assert "OpenAI factory error" in str(exc_info.value)
    
    # Verify openai_llm_factory was called
    mock_openai_factory.assert_called_once_with("gpt-4", 0.7, {"type": "object", "properties": {"result": {"type": "string"}}})

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_return_type_verification(mock_openai_factory):
    """Test that the function returns the correct type."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.5
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify result is the same object returned by openai_llm_factory
    assert result == mock_llm_instance
    
    # Verify the object has the expected type (basic check)
    assert isinstance(result, Mock)

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_parameter_preservation(mock_openai_factory):
    """Test that parameters are preserved exactly as passed."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config with exact values
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4o-mini"
    mock_config.temperature = 0.999
    mock_config.output_schema = {
        "type": "object",
        "properties": {
            "exact": {"type": "string", "description": "Exact preservation test"}
        }
    }
    
    # Execute function
    result = llm_factory(mock_config)
    
    # Verify parameters are passed exactly as provided
    mock_openai_factory.assert_called_once_with("gpt-4o-mini", 0.999, {
        "type": "object",
        "properties": {
            "exact": {"type": "string", "description": "Exact preservation test"}
        }
    })
    
    # Verify result is the LLM instance from openai_llm_factory
    assert result == mock_llm_instance

@patch('multi_agent_system.openai_llm_factory')
def test_llm_factory_multiple_calls_same_config(mock_openai_factory):
    """Test that multiple calls with same config work correctly."""
    # Setup mocks
    mock_llm_instance = Mock()
    mock_openai_factory.return_value = mock_llm_instance
    
    # Setup test config
    mock_config = Mock()
    mock_config.provider = "openai"
    mock_config.model = "gpt-4"
    mock_config.temperature = 0.7
    mock_config.output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    # Execute function multiple times
    result1 = llm_factory(mock_config)
    result2 = llm_factory(mock_config)
    result3 = llm_factory(mock_config)
    
    # Verify all results are the same
    assert result1 == mock_llm_instance
    assert result2 == mock_llm_instance
    assert result3 == mock_llm_instance
    
    # Verify openai_llm_factory was called three times with same parameters
    assert mock_openai_factory.call_count == 3
    expected_call = (("gpt-4", 0.7, {"type": "object", "properties": {"result": {"type": "string"}}}),)
    assert mock_openai_factory.call_args_list == [expected_call, expected_call, expected_call]
```
#### 3.9.3 Multi-Agent Graph Factory

**Function Information:**
- **Function Name**: `create_multi_agent_graph`
- **Location**: `multi_agent_system.py:1087`
- **Purpose**: Creates and compiles a complete multi-agent graph with all agents, tools, and routing logic

**Function Signature and Parameters:**
- **Input**: `agent_configs: dict[str, AgentConfig]` - Dictionary containing all agent configurations
- **Return**: `StateGraph` - Compiled LangGraph state graph ready for invocation
- **Side Effects**: None (pure factory function)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `llm_factory()` - Internal function for creating LLMs
  - `asyncio.run()` - Async execution for research tools
  - `get_research_tools()` - Internal function for research tools
  - `get_expert_tools()` - Internal function for expert tools
  - `create_researcher_llm_node()` - Internal function for researcher node
  - `create_expert_llm_node()` - Internal function for expert node
  - `create_researcher_subgraph()` - Internal function for researcher subgraph
  - `create_expert_subgraph()` - Internal function for expert subgraph
  - `create_planner_agent()` - Internal function for planner agent
  - `create_researcher_agent()` - Internal function for researcher agent
  - `create_expert_agent()` - Internal function for expert agent
  - `create_critic_agent()` - Internal function for critic agent
  - `create_finalizer_agent()` - Internal function for finalizer agent
  - `create_input_interface()` - Internal function for input interface
  - `StateGraph` - LangGraph state graph class
  - `START`, `END` - LangGraph constants
  - `orchestrator` - Internal orchestrator function
  - `route_from_orchestrator` - Internal routing function
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, dictionary access, variable assignment, return statement)
  - Dictionary operations (key access, value assignment)
  - List operations and method calls
- **Mock Configuration**: Mock all external dependencies to isolate the graph building logic

**Test Cases:**

**Happy Path:**
- **Valid Agent Configs**: Function with complete valid agent configurations
- **Complete Graph Creation**: All agents, tools, and nodes created successfully
- **Graph Compilation**: Graph compiles successfully and returns StateGraph
- **All Required Agents**: All five agent types (planner, researcher, expert, critic, finalizer) included

**Edge Cases:**
- **Empty Agent Configs**: Dictionary with minimal required configurations
- **Large Agent Configs**: Dictionary with complex/large configurations
- **Missing Optional Fields**: Some optional fields missing from configs
- **Duplicate Agent Types**: Multiple configs for same agent type
- **Special Characters in Names**: Special characters in agent names or configs

**Error Conditions:**
- **Missing Required Agents**: Missing one or more required agent configs
- **Invalid Agent Configs**: Malformed or invalid AgentConfig objects
- **LLM Factory Failure**: Errors in LLM creation
- **Tool Creation Failure**: Errors in research or expert tool creation
- **Agent Creation Failure**: Errors in agent function creation
- **Subgraph Creation Failure**: Errors in researcher or expert subgraph creation
- **Graph Building Failure**: Errors in StateGraph construction
- **Graph Compilation Failure**: Errors in graph compilation

**State Changes:**
- **No State Changes**: Function is pure and doesn't modify external state
- **Return Value**: New compiled StateGraph instance is created and returned
- **Graph Structure**: Returned graph contains all required nodes and edges

**Mock Configurations:**
```python
@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.route_from_orchestrator')
@patch('multi_agent_system.orchestrator')
@patch('multi_agent_system.create_input_interface')
@patch('multi_agent_system.create_finalizer_agent')
@patch('multi_agent_system.create_critic_agent')
@patch('multi_agent_system.create_expert_agent')
@patch('multi_agent_system.create_researcher_agent')
@patch('multi_agent_system.create_planner_agent')
@patch('multi_agent_system.create_expert_subgraph')
@patch('multi_agent_system.create_researcher_subgraph')
@patch('multi_agent_system.create_expert_llm_node')
@patch('multi_agent_system.create_researcher_llm_node')
@patch('multi_agent_system.get_expert_tools')
@patch('multi_agent_system.get_research_tools')
@patch('multi_agent_system.asyncio')
@patch('multi_agent_system.llm_factory')
def test_multi_agent_graph_factory_with_mocked_dependencies(mock_llm_factory, mock_asyncio, mock_get_research_tools, mock_get_expert_tools, mock_create_researcher_node, mock_create_expert_node, mock_create_researcher_subgraph, mock_create_expert_subgraph, mock_create_planner_agent, mock_create_researcher_agent, mock_create_expert_agent, mock_create_critic_agent, mock_create_finalizer_agent, mock_create_input_interface, mock_orchestrator, mock_route_from_orchestrator, mock_state_graph):
    """Mock all external dependencies for testing."""
    # Setup mocks for LLM creation
    mock_llm_planner = Mock()
    mock_llm_researcher = Mock()
    mock_llm_expert = Mock()
    mock_llm_critic = Mock()
    mock_llm_finalizer = Mock()
    mock_llm_factory.side_effect = [mock_llm_planner, mock_llm_researcher, mock_llm_expert, mock_llm_critic, mock_llm_finalizer]
    
    # Setup mocks for tools
    mock_research_tools = [Mock(), Mock()]
    mock_expert_tools = [Mock(), Mock()]
    mock_get_research_tools.return_value = mock_research_tools
    mock_get_expert_tools.return_value = mock_expert_tools
    mock_asyncio.run.return_value = mock_research_tools
    
    # Setup mocks for nodes and subgraphs
    mock_researcher_node = Mock()
    mock_expert_node = Mock()
    mock_researcher_graph = Mock()
    mock_expert_graph = Mock()
    mock_create_researcher_node.return_value = mock_researcher_node
    mock_create_expert_node.return_value = mock_expert_node
    mock_create_researcher_subgraph.return_value = mock_researcher_graph
    mock_create_expert_subgraph.return_value = mock_expert_graph
    
    # Setup mocks for agents
    mock_planner_agent = Mock()
    mock_researcher_agent = Mock()
    mock_expert_agent = Mock()
    mock_critic_agent = Mock()
    mock_finalizer_agent = Mock()
    mock_input_interface = Mock()
    mock_create_planner_agent.return_value = mock_planner_agent
    mock_create_researcher_agent.return_value = mock_researcher_agent
    mock_create_expert_agent.return_value = mock_expert_agent
    mock_create_critic_agent.return_value = mock_critic_agent
    mock_create_finalizer_agent.return_value = mock_finalizer_agent
    mock_create_input_interface.return_value = mock_input_interface
    
    # Setup mock for StateGraph
    mock_builder = Mock()
    mock_state_graph.return_value = mock_builder
    mock_compiled_graph = Mock()
    mock_builder.compile.return_value = mock_compiled_graph
    
    # Setup test configs
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    # Execute function
    result = create_multi_agent_graph(mock_configs)
    
    # Verify result
    assert result == mock_compiled_graph
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and graph building logic in the implementation:
def create_multi_agent_graph(agent_configs: dict[str, AgentConfig]) -> StateGraph:  # Direct type annotations
    """Factory function that creates and compiles a multi-agent graph with injected prompts."""
    
    # Create LLMs dynamically
    llm_planner = llm_factory(agent_configs["planner"])  # Direct dictionary access and function call
    llm_researcher = llm_factory(agent_configs["researcher"])  # Direct dictionary access and function call
    llm_expert = llm_factory(agent_configs["expert"])  # Direct dictionary access and function call
    llm_critic = llm_factory(agent_configs["critic"])  # Direct dictionary access and function call
    llm_finalizer = llm_factory(agent_configs["finalizer"])  # Direct dictionary access and function call
   
    # Create Researcher Subgraphs
    research_tools = asyncio.run(get_research_tools())  # Direct async function call
    llm_researcher = llm_researcher.bind_tools(research_tools)  # Direct method call
    researcher_node = create_researcher_llm_node(agent_configs["researcher"], llm_researcher, research_tools)  # Direct function call
    researcher_graph = create_researcher_subgraph(researcher_node, research_tools)  # Direct function call
    
    # Create Expert Subgraphs
    expert_tools = get_expert_tools()  # Direct function call
    llm_expert = llm_expert.bind_tools(expert_tools)  # Direct method call
    expert_node = create_expert_llm_node(agent_configs["expert"], llm_expert)  # Direct function call
    expert_graph = create_expert_subgraph(expert_node, expert_tools)  # Direct function call

    # Create agent functions with injected prompts and LLMs
    planner_agent = create_planner_agent(agent_configs["planner"], llm_planner)  # Direct function call
    researcher_agent = create_researcher_agent(agent_configs["researcher"], researcher_graph)  # Direct function call
    expert_agent = create_expert_agent(agent_configs["expert"], expert_graph)  # Direct function call
    critic_agent = create_critic_agent(agent_configs["critic"], llm_critic)  # Direct function call
    finalizer_agent = create_finalizer_agent(agent_configs["finalizer"], llm_finalizer)  # Direct function call
    
    # Create input interface with retry limit
    input_interface = create_input_interface(agent_configs)  # Direct function call
    
    # Build the graph
    builder = StateGraph(GraphState)  # Direct class instantiation
    
    builder.add_node("input_interface", input_interface)  # Direct method calls
    builder.add_node("orchestrator", orchestrator)
    builder.add_node("planner", planner_agent)
    builder.add_node("researcher", researcher_agent)
    builder.add_node("expert", expert_agent)
    builder.add_node("critic", critic_agent)
    builder.add_node("finalizer", finalizer_agent)
    
    builder.add_edge(START, "input_interface")  # Direct method calls with constants
    builder.add_edge("input_interface", "orchestrator")
    builder.add_edge("planner", "orchestrator")
    builder.add_edge("researcher", "orchestrator")
    builder.add_edge("expert", "orchestrator")
    builder.add_edge("critic", "orchestrator")
    builder.add_edge("finalizer", END)
    
    builder.add_conditional_edges("orchestrator", route_from_orchestrator, {  # Direct method call with dictionary
            "planner": "planner",
            "researcher": "researcher",
            "expert": "expert",
            "critic": "critic",
            "finalizer": "finalizer"
            }
        )
    
    # Compile and return the graph
    return builder.compile()  # Direct method call and return
```

**Assertion Specifications:**
- Verify all required LLMs are created with correct parameters
- Verify research and expert tools are retrieved correctly
- Verify all agent functions are created with correct parameters
- Verify all subgraphs are created correctly
- Verify StateGraph is built with all required nodes and edges
- Verify graph compiles successfully and returns StateGraph
- Verify error conditions are properly handled and propagated
- Verify all dependencies are called with correct parameters

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import create_multi_agent_graph

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.route_from_orchestrator')
@patch('multi_agent_system.orchestrator')
@patch('multi_agent_system.create_input_interface')
@patch('multi_agent_system.create_finalizer_agent')
@patch('multi_agent_system.create_critic_agent')
@patch('multi_agent_system.create_expert_agent')
@patch('multi_agent_system.create_researcher_agent')
@patch('multi_agent_system.create_planner_agent')
@patch('multi_agent_system.create_expert_subgraph')
@patch('multi_agent_system.create_researcher_subgraph')
@patch('multi_agent_system.create_expert_llm_node')
@patch('multi_agent_system.create_researcher_llm_node')
@patch('multi_agent_system.get_expert_tools')
@patch('multi_agent_system.get_research_tools')
@patch('multi_agent_system.asyncio')
@patch('multi_agent_system.llm_factory')
def test_create_multi_agent_graph_valid_configs(mock_llm_factory, mock_asyncio, mock_get_research_tools, mock_get_expert_tools, mock_create_researcher_node, mock_create_expert_node, mock_create_researcher_subgraph, mock_create_expert_subgraph, mock_create_planner_agent, mock_create_researcher_agent, mock_create_expert_agent, mock_create_critic_agent, mock_create_finalizer_agent, mock_create_input_interface, mock_orchestrator, mock_route_from_orchestrator, mock_state_graph):
    """Test multi-agent graph creation with valid configurations."""
    # Setup mocks for LLM creation
    mock_llm_planner = Mock()
    mock_llm_researcher = Mock()
    mock_llm_expert = Mock()
    mock_llm_critic = Mock()
    mock_llm_finalizer = Mock()
    mock_llm_factory.side_effect = [mock_llm_planner, mock_llm_researcher, mock_llm_expert, mock_llm_critic, mock_llm_finalizer]
    
    # Setup mocks for tools
    mock_research_tools = [Mock(), Mock()]
    mock_expert_tools = [Mock(), Mock()]
    mock_get_research_tools.return_value = mock_research_tools
    mock_get_expert_tools.return_value = mock_expert_tools
    mock_asyncio.run.return_value = mock_research_tools
    
    # Setup mocks for nodes and subgraphs
    mock_researcher_node = Mock()
    mock_expert_node = Mock()
    mock_researcher_graph = Mock()
    mock_expert_graph = Mock()
    mock_create_researcher_node.return_value = mock_researcher_node
    mock_create_expert_node.return_value = mock_expert_node
    mock_create_researcher_subgraph.return_value = mock_researcher_graph
    mock_create_expert_subgraph.return_value = mock_expert_graph
    
    # Setup mocks for agents
    mock_planner_agent = Mock()
    mock_researcher_agent = Mock()
    mock_expert_agent = Mock()
    mock_critic_agent = Mock()
    mock_finalizer_agent = Mock()
    mock_input_interface = Mock()
    mock_create_planner_agent.return_value = mock_planner_agent
    mock_create_researcher_agent.return_value = mock_researcher_agent
    mock_create_expert_agent.return_value = mock_expert_agent
    mock_create_critic_agent.return_value = mock_critic_agent
    mock_create_finalizer_agent.return_value = mock_finalizer_agent
    mock_create_input_interface.return_value = mock_input_interface
    
    # Setup mock for StateGraph
    mock_builder = Mock()
    mock_state_graph.return_value = mock_builder
    mock_compiled_graph = Mock()
    mock_builder.compile.return_value = mock_compiled_graph
    
    # Setup test configs
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    # Execute function
    result = create_multi_agent_graph(mock_configs)
    
    # Verify result
    assert result == mock_compiled_graph
    
    # Verify LLM factory calls
    assert mock_llm_factory.call_count == 5
    mock_llm_factory.assert_any_call(mock_configs["planner"])
    mock_llm_factory.assert_any_call(mock_configs["researcher"])
    mock_llm_factory.assert_any_call(mock_configs["expert"])
    mock_llm_factory.assert_any_call(mock_configs["critic"])
    mock_llm_factory.assert_any_call(mock_configs["finalizer"])
    
    # Verify tool creation
    mock_asyncio.run.assert_called_once_with(mock_get_research_tools.return_value)
    mock_get_expert_tools.assert_called_once()
    
    # Verify agent creation
    mock_create_planner_agent.assert_called_once_with(mock_configs["planner"], mock_llm_planner)
    mock_create_researcher_agent.assert_called_once_with(mock_configs["researcher"], mock_researcher_graph)
    mock_create_expert_agent.assert_called_once_with(mock_configs["expert"], mock_expert_graph)
    mock_create_critic_agent.assert_called_once_with(mock_configs["critic"], mock_llm_critic)
    mock_create_finalizer_agent.assert_called_once_with(mock_configs["finalizer"], mock_llm_finalizer)
    mock_create_input_interface.assert_called_once_with(mock_configs)
    
    # Verify graph building
    mock_state_graph.assert_called_once()
    assert mock_builder.add_node.call_count == 7
    assert mock_builder.add_edge.call_count == 7
    mock_builder.add_conditional_edges.assert_called_once()
    mock_builder.compile.assert_called_once()

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.route_from_orchestrator')
@patch('multi_agent_system.orchestrator')
@patch('multi_agent_system.create_input_interface')
@patch('multi_agent_system.create_finalizer_agent')
@patch('multi_agent_system.create_critic_agent')
@patch('multi_agent_system.create_expert_agent')
@patch('multi_agent_system.create_researcher_agent')
@patch('multi_agent_system.create_planner_agent')
@patch('multi_agent_system.create_expert_subgraph')
@patch('multi_agent_system.create_researcher_subgraph')
@patch('multi_agent_system.create_expert_llm_node')
@patch('multi_agent_system.create_researcher_llm_node')
@patch('multi_agent_system.get_expert_tools')
@patch('multi_agent_system.get_research_tools')
@patch('multi_agent_system.asyncio')
@patch('multi_agent_system.llm_factory')
def test_create_multi_agent_graph_missing_required_agent(mock_llm_factory, mock_asyncio, mock_get_research_tools, mock_get_expert_tools, mock_create_researcher_node, mock_create_expert_node, mock_create_researcher_subgraph, mock_create_expert_subgraph, mock_create_planner_agent, mock_create_researcher_agent, mock_create_expert_agent, mock_create_critic_agent, mock_create_finalizer_agent, mock_create_input_interface, mock_orchestrator, mock_route_from_orchestrator, mock_state_graph):
    """Test multi-agent graph creation with missing required agent."""
    # Setup test configs missing planner
    mock_configs = {
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError) as exc_info:
        create_multi_agent_graph(mock_configs)
    
    assert "planner" in str(exc_info.value)

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.route_from_orchestrator')
@patch('multi_agent_system.orchestrator')
@patch('multi_agent_system.create_input_interface')
@patch('multi_agent_system.create_finalizer_agent')
@patch('multi_agent_system.create_critic_agent')
@patch('multi_agent_system.create_expert_agent')
@patch('multi_agent_system.create_researcher_agent')
@patch('multi_agent_system.create_planner_agent')
@patch('multi_agent_system.create_expert_subgraph')
@patch('multi_agent_system.create_researcher_subgraph')
@patch('multi_agent_system.create_expert_llm_node')
@patch('multi_agent_system.create_researcher_llm_node')
@patch('multi_agent_system.get_expert_tools')
@patch('multi_agent_system.get_research_tools')
@patch('multi_agent_system.asyncio')
@patch('multi_agent_system.llm_factory')
def test_create_multi_agent_graph_llm_factory_failure(mock_llm_factory, mock_asyncio, mock_get_research_tools, mock_get_expert_tools, mock_create_researcher_node, mock_create_expert_node, mock_create_researcher_subgraph, mock_create_expert_subgraph, mock_create_planner_agent, mock_create_researcher_agent, mock_create_expert_agent, mock_create_critic_agent, mock_create_finalizer_agent, mock_create_input_interface, mock_orchestrator, mock_route_from_orchestrator, mock_state_graph):
    """Test multi-agent graph creation when LLM factory fails."""
    # Setup mock to raise exception
    mock_llm_factory.side_effect = Exception("LLM factory error")
    
    # Setup test configs
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_multi_agent_graph(mock_configs)
    
    assert "LLM factory error" in str(exc_info.value)
    
    # Verify llm_factory was called
    mock_llm_factory.assert_called_once_with(mock_configs["planner"])

@patch('multi_agent_system.StateGraph')
@patch('multi_agent_system.route_from_orchestrator')
@patch('multi_agent_system.orchestrator')
@patch('multi_agent_system.create_input_interface')
@patch('multi_agent_system.create_finalizer_agent')
@patch('multi_agent_system.create_critic_agent')
@patch('multi_agent_system.create_expert_agent')
@patch('multi_agent_system.create_researcher_agent')
@patch('multi_agent_system.create_planner_agent')
@patch('multi_agent_system.create_expert_subgraph')
@patch('multi_agent_system.create_researcher_subgraph')
@patch('multi_agent_system.create_expert_llm_node')
@patch('multi_agent_system.create_researcher_llm_node')
@patch('multi_agent_system.get_expert_tools')
@patch('multi_agent_system.get_research_tools')
@patch('multi_agent_system.asyncio')
@patch('multi_agent_system.llm_factory')
def test_create_multi_agent_graph_graph_compilation_failure(mock_llm_factory, mock_asyncio, mock_get_research_tools, mock_get_expert_tools, mock_create_researcher_node, mock_create_expert_node, mock_create_researcher_subgraph, mock_create_expert_subgraph, mock_create_planner_agent, mock_create_researcher_agent, mock_create_expert_agent, mock_create_critic_agent, mock_create_finalizer_agent, mock_create_input_interface, mock_orchestrator, mock_route_from_orchestrator, mock_state_graph):
    """Test multi-agent graph creation when graph compilation fails."""
    # Setup all mocks for successful creation
    mock_llm_planner = Mock()
    mock_llm_researcher = Mock()
    mock_llm_expert = Mock()
    mock_llm_critic = Mock()
    mock_llm_finalizer = Mock()
    mock_llm_factory.side_effect = [mock_llm_planner, mock_llm_researcher, mock_llm_expert, mock_llm_critic, mock_llm_finalizer]
    
    mock_research_tools = [Mock(), Mock()]
    mock_expert_tools = [Mock(), Mock()]
    mock_get_research_tools.return_value = mock_research_tools
    mock_get_expert_tools.return_value = mock_expert_tools
    mock_asyncio.run.return_value = mock_research_tools
    
    mock_researcher_node = Mock()
    mock_expert_node = Mock()
    mock_researcher_graph = Mock()
    mock_expert_graph = Mock()
    mock_create_researcher_node.return_value = mock_researcher_node
    mock_create_expert_node.return_value = mock_expert_node
    mock_create_researcher_subgraph.return_value = mock_researcher_graph
    mock_create_expert_subgraph.return_value = mock_expert_graph
    
    mock_planner_agent = Mock()
    mock_researcher_agent = Mock()
    mock_expert_agent = Mock()
    mock_critic_agent = Mock()
    mock_finalizer_agent = Mock()
    mock_input_interface = Mock()
    mock_create_planner_agent.return_value = mock_planner_agent
    mock_create_researcher_agent.return_value = mock_researcher_agent
    mock_create_expert_agent.return_value = mock_expert_agent
    mock_create_critic_agent.return_value = mock_critic_agent
    mock_create_finalizer_agent.return_value = mock_finalizer_agent
    mock_create_input_interface.return_value = mock_input_interface
    
    # Setup mock for StateGraph with compilation failure
    mock_builder = Mock()
    mock_state_graph.return_value = mock_builder
    mock_builder.compile.side_effect = Exception("Graph compilation error")
    
    # Setup test configs
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        create_multi_agent_graph(mock_configs)
    
    assert "Graph compilation error" in str(exc_info.value)
    
    # Verify graph was built but compilation failed
    mock_state_graph.assert_called_once()
    mock_builder.compile.assert_called_once()
```
### 3.10 Main Application Components
#### 3.10.1 Prompt Loading Functions

**Function Information:**
- **Function Name**: `load_prompt_from_file`
- **Location**: `main.py:25`
- **Purpose**: Loads a prompt from a text file with UTF-8 encoding and returns the stripped content

**Function Signature and Parameters:**
- **Input**: `file_path: str` - Path to the prompt file to load
- **Return**: `str` - Content of the prompt file with whitespace stripped
- **Side Effects**: None (pure function, but may log debug/error messages)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.debug()` - Logging dependency for debug messages
  - `logger.error()` - Logging dependency for error messages
  - `open()` - Built-in file opening function (file system access)
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, try/except blocks, with statement, string methods)
  - `FileNotFoundError` - Built-in exception class
  - `Exception` - Built-in exception class
  - String methods (`strip()`, `read()`)
  - Context manager (`with` statement)
- **Mock Configuration**: Mock logger calls and file system operations to isolate the function logic

**Test Cases:**

**Happy Path:**
- **Valid File Path**: Function with valid path to existing prompt file
- **UTF-8 Content**: File contains UTF-8 encoded text
- **Content Loading**: File content is properly loaded and stripped
- **Return Value**: Correct string content is returned

**Edge Cases:**
- **Empty File**: File exists but contains no content
- **Whitespace Only**: File contains only whitespace characters
- **Large File**: File contains very large content
- **Special Characters**: File contains special characters and Unicode
- **Long File Path**: Very long file path
- **File with Newlines**: File contains multiple newlines and formatting
- **File with Tabs**: File contains tab characters and other whitespace

**Error Conditions:**
- **File Not Found**: File path doesn't exist
- **Permission Denied**: No read permission for the file
- **Invalid Encoding**: File has non-UTF-8 encoding
- **Corrupted File**: File is corrupted or unreadable
- **Empty File Path**: Empty string as file path
- **None File Path**: None as file path
- **Invalid File Path**: Malformed file path
- **Directory Path**: Path points to a directory instead of file

**State Changes:**
- **No State Changes**: Function is pure and doesn't modify external state
- **Return Value**: String content from file is returned
- **Logging**: Debug and error messages may be logged

**Mock Configurations:**
```python
@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_with_mocked_dependencies(mock_open, mock_logger):
    """Mock file operations and logging for testing."""
    # Mock logger calls
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    mock_file_content = "Test prompt content\nwith newlines"
    mock_file = Mock()
    mock_file.read.return_value = mock_file_content
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify file content was read
    mock_file.read.assert_called_once()
    
    # Verify result is stripped content
    assert result == "Test prompt content\nwith newlines"
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and file handling in the implementation:
def load_prompt_from_file(file_path: str) -> str:  # Direct type annotations
    """Load a prompt from a text file.
    
    Args:
        file_path (str): Path to the prompt file  # Direct string type annotation
        
    Returns:
        str: Content of the prompt file  # Direct return type annotation
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:  # Direct file opening with context manager
            content = f.read().strip()  # Direct method calls (read and strip)
            logger.debug(f"Successfully loaded prompt from: {file_path}")  # Direct logging call
            return content  # Direct return statement
    except FileNotFoundError:  # Direct exception handling
        logger.error(f"Prompt file not found: {file_path}")  # Direct logging call
        raise FileNotFoundError(f"Prompt file not found: {file_path}")  # Direct exception raising with f-string formatting
    except Exception as e:  # Direct exception handling
        logger.error(f"Error reading prompt file {file_path}: {str(e)}")  # Direct logging call
        raise Exception(f"Error reading prompt file {file_path}: {str(e)}")  # Direct exception raising with f-string formatting
```

**Assertion Specifications:**
- Verify file is opened with correct path, mode, and encoding
- Verify file content is read and stripped properly
- Verify correct string content is returned
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated
- Verify exceptions are raised with correct error messages
- Verify file handle is properly closed (context manager)

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, mock_open
from main import load_prompt_from_file

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_valid_content(mock_open, mock_logger):
    """Test loading prompt from file with valid content."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    test_content = "You are a helpful AI assistant.\nPlease answer the question."
    mock_file = Mock()
    mock_file.read.return_value = test_content
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result
    assert result == test_content
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify file content was read
    mock_file.read.assert_called_once()
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_empty_content(mock_open, mock_logger):
    """Test loading prompt from file with empty content."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock empty file content
    mock_file = Mock()
    mock_file.read.return_value = ""
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/empty_prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result
    assert result == ""
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_whitespace_content(mock_open, mock_logger):
    """Test loading prompt from file with whitespace-only content."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock whitespace-only content
    mock_file = Mock()
    mock_file.read.return_value = "   \n\t  \n  "
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/whitespace_prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result is stripped
    assert result == ""
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_special_characters(mock_open, mock_logger):
    """Test loading prompt from file with special characters."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock content with special characters
    test_content = "ðŸš€ðŸŒŸðŸ’¡Ã©Ã±Ã¼ÃŸÂ©Â®â„¢\nSpecial characters test"
    mock_file = Mock()
    mock_file.read.return_value = test_content
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/special_prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result
    assert result == test_content
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_large_content(mock_open, mock_logger):
    """Test loading prompt from file with large content."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock large content
    large_content = "x" * 10000
    mock_file = Mock()
    mock_file.read.return_value = large_content
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/large_prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result
    assert result == large_content
    assert len(result) == 10000
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_file_not_found(mock_open, mock_logger):
    """Test loading prompt from file that doesn't exist."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file not found exception
    mock_open.side_effect = FileNotFoundError("No such file or directory")
    
    # Test parameters
    test_file_path = "/path/to/nonexistent_prompt.txt"
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError) as exc_info:
        load_prompt_from_file(test_file_path)
    
    assert "Prompt file not found: /path/to/nonexistent_prompt.txt" in str(exc_info.value)
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify error logging was called
    mock_logger.error.assert_called_once_with(f"Prompt file not found: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_permission_denied(mock_open, mock_logger):
    """Test loading prompt from file with permission denied."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock permission denied exception
    mock_open.side_effect = PermissionError("Permission denied")
    
    # Test parameters
    test_file_path = "/path/to/restricted_prompt.txt"
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        load_prompt_from_file(test_file_path)
    
    assert "Error reading prompt file /path/to/restricted_prompt.txt: Permission denied" in str(exc_info.value)
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify error logging was called
    mock_logger.error.assert_called_once_with(f"Error reading prompt file {test_file_path}: Permission denied")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_encoding_error(mock_open, mock_logger):
    """Test loading prompt from file with encoding error."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock encoding error
    mock_open.side_effect = UnicodeDecodeError("utf-8", b"", 0, 1, "invalid utf-8")
    
    # Test parameters
    test_file_path = "/path/to/encoding_error_prompt.txt"
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        load_prompt_from_file(test_file_path)
    
    assert "Error reading prompt file /path/to/encoding_error_prompt.txt:" in str(exc_info.value)
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify error logging was called
    mock_logger.error.assert_called_once()

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_empty_file_path(mock_open, mock_logger):
    """Test loading prompt from file with empty file path."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file not found exception for empty path
    mock_open.side_effect = FileNotFoundError("No such file or directory")
    
    # Test parameters
    test_file_path = ""
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError) as exc_info:
        load_prompt_from_file(test_file_path)
    
    assert "Prompt file not found: " in str(exc_info.value)
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with("", "r", encoding="utf-8")
    
    # Verify error logging was called
    mock_logger.error.assert_called_once_with("Prompt file not found: ")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_none_file_path(mock_open, mock_logger):
    """Test loading prompt from file with None file path."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock type error for None path
    mock_open.side_effect = TypeError("expected str, bytes or os.PathLike object, not NoneType")
    
    # Test parameters
    test_file_path = None
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        load_prompt_from_file(test_file_path)
    
    assert "Error reading prompt file None:" in str(exc_info.value)
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with(None, "r", encoding="utf-8")
    
    # Verify error logging was called
    mock_logger.error.assert_called_once()

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_long_file_path(mock_open, mock_logger):
    """Test loading prompt from file with very long file path."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    test_content = "Test content for long path"
    mock_file = Mock()
    mock_file.read.return_value = test_content
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters with very long path
    test_file_path = "/" + "a" * 1000 + "/very/long/path/to/prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result
    assert result == test_content
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_with_newlines_and_tabs(mock_open, mock_logger):
    """Test loading prompt from file with newlines and tabs."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock content with newlines and tabs
    test_content = "\n\n\tYou are a helpful assistant.\n\tPlease help with the task.\n\n"
    mock_file = Mock()
    mock_file.read.return_value = test_content
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test parameters
    test_file_path = "/path/to/formatted_prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result (should be stripped)
    assert result == "\n\n\tYou are a helpful assistant.\n\tPlease help with the task.\n\n"
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")

@patch('main.logger')
@patch('builtins.open')
def test_load_prompt_from_file_context_manager_verification(mock_open, mock_logger):
    """Test that file is properly closed using context manager."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    test_content = "Test content"
    mock_file = Mock()
    mock_file.read.return_value = test_content
    mock_context_manager = Mock()
    mock_context_manager.__enter__.return_value = mock_file
    mock_context_manager.__exit__.return_value = None
    mock_open.return_value = mock_context_manager
    
    # Test parameters
    test_file_path = "/path/to/prompt.txt"
    
    # Execute function
    result = load_prompt_from_file(test_file_path)
    
    # Verify result
    assert result == test_content
    
    # Verify context manager was used properly
    mock_context_manager.__enter__.assert_called_once()
    mock_context_manager.__exit__.assert_called_once()
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_file_path, "r", encoding="utf-8")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Successfully loaded prompt from: {test_file_path}")
```
#### 3.10.2 Baseline Prompt Loading

**Function Information:**
- **Function Name**: `load_baseline_prompts`
- **Location**: `main.py:47`
- **Purpose**: Loads all baseline prompts from the prompts/baseline directory, automatically finding the directory path if not provided

**Function Signature and Parameters:**
- **Input**: `prompts_dir: str = None` - Optional path to the prompts/baseline directory
- **Return**: `dict` - Dictionary containing all agent prompts with agent names as keys
- **Side Effects**: None (pure function, but may log info/debug messages)

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for info messages
  - `logger.debug()` - Logging dependency for debug messages
  - `os.path.dirname()` - OS path operations for script directory
  - `os.path.abspath()` - OS path operations for absolute path
  - `os.path.join()` - OS path operations for path joining
  - `os.path.normpath()` - OS path operations for path normalization
  - `load_prompt_from_file()` - Internal function for loading individual prompt files
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, conditional logic, dictionary operations, loops)
  - Dictionary definition and iteration
  - String operations and f-string formatting
  - Variable assignment and return statement
- **Mock Configuration**: Mock OS path operations, logging calls, and the internal prompt loading function

**Test Cases:**

**Happy Path:**
- **Valid Directory Path**: Function with valid prompts directory path
- **Auto-Discovery**: Function with None prompts_dir (auto-discovers path)
- **All Prompts Loaded**: All seven prompt files are successfully loaded
- **Correct Return Structure**: Dictionary with correct agent names as keys
- **Complete Prompt Set**: All required agent prompts are included

**Edge Cases:**
- **Empty Directory**: Directory exists but contains no prompt files
- **Missing Some Files**: Some prompt files are missing from directory
- **Large Prompt Files**: Prompt files contain very large content
- **Special Characters in Path**: Directory path contains special characters
- **Long Directory Path**: Very long directory path
- **Relative Path**: Relative path instead of absolute path
- **Different File Extensions**: Files with different extensions than expected

**Error Conditions:**
- **Invalid Directory Path**: Directory path doesn't exist
- **Permission Denied**: No read permission for directory
- **Missing All Files**: All prompt files are missing
- **File Loading Failure**: Individual prompt file loading fails
- **Path Resolution Failure**: OS path operations fail
- **Empty Directory Path**: Empty string as directory path
- **None Directory Path**: None as directory path (should auto-discover)

**State Changes:**
- **No State Changes**: Function is pure and doesn't modify external state
- **Return Value**: Dictionary with all loaded prompts is returned
- **Logging**: Info and debug messages may be logged

**Mock Configurations:**
```python
@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_with_mocked_dependencies(mock_os_path, mock_logger, mock_load_prompt):
    """Mock OS operations, logging, and prompt loading for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock OS path operations
    mock_os_path.dirname.return_value = "/path/to/script"
    mock_os_path.abspath.return_value = "/absolute/path/to/script"
    mock_os_path.join.return_value = "/path/to/prompts/baseline"
    mock_os_path.normpath.return_value = "/normalized/path/to/prompts/baseline"
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt content",
        "critic_planner": "Critic planner prompt content",
        "researcher": "Researcher prompt content",
        "critic_researcher": "Critic researcher prompt content",
        "expert": "Expert prompt content",
        "critic_expert": "Critic expert prompt content",
        "finalizer": "Finalizer prompt content"
    }
    mock_load_prompt.side_effect = lambda path: mock_prompts.get(path.split("/")[-1].replace(".txt", ""), "")
    
    # Test parameters
    test_prompts_dir = "/path/to/prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result
    assert result == mock_prompts
    assert len(result) == 7
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Loading prompts from directory: {test_prompts_dir}")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and path handling in the implementation:
def load_baseline_prompts(prompts_dir: str = None) -> dict:  # Direct type annotations
    """Load all baseline prompts from the prompts/baseline directory."""
    
    if prompts_dir is None:  # Direct conditional logic
        # Try to find the baseline prompts directory relative to this script
        script_dir = os.path.dirname(os.path.abspath(__file__))  # Direct OS path operations
        # Go up one level from src/ to the project root, then into prompts/baseline
        prompts_dir = os.path.join(script_dir, "..", "prompts", "baseline")  # Direct path joining
        prompts_dir = os.path.normpath(prompts_dir)  # Direct path normalization
    
    logger.info(f"Loading prompts from directory: {prompts_dir}")  # Direct logging call
    
    prompt_files = {  # Direct dictionary definition
        "planner": "planner_system_prompt.txt",
        "critic_planner": "critic_planner_system_prompt.txt",
        "researcher": "researcher_system_prompt.txt",
        "critic_researcher": "critic_researcher_system_prompt.txt",
        "expert": "expert_system_prompt.txt",
        "critic_expert": "critic_expert_system_prompt.txt",
        "finalizer": "finalizer_system_prompt.txt"
    }
    
    prompts = {}  # Direct dictionary initialization
    for agent_name, filename in prompt_files.items():  # Direct dictionary iteration
        file_path = os.path.join(prompts_dir, filename)  # Direct path joining
        logger.debug(f"Loading prompt for {agent_name} from: {file_path}")  # Direct logging call
        prompts[agent_name] = load_prompt_from_file(file_path)  # Direct function call and assignment
    
    logger.info(f"Successfully loaded {len(prompts)} prompts")  # Direct logging call with f-string formatting
    return prompts  # Direct return statement
```

**Assertion Specifications:**
- Verify OS path operations are called with correct parameters
- Verify all prompt files are loaded with correct paths
- Verify dictionary contains all expected agent names as keys
- Verify logging calls are made with correct messages
- Verify error conditions are properly handled and propagated
- Verify auto-discovery works when prompts_dir is None
- Verify all seven prompt files are processed

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from main import load_baseline_prompts

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_valid_directory(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with valid directory path."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "You are a planner agent.",
        "critic_planner": "You are a critic for planner.",
        "researcher": "You are a researcher agent.",
        "critic_researcher": "You are a critic for researcher.",
        "expert": "You are an expert agent.",
        "critic_expert": "You are a critic for expert.",
        "finalizer": "You are a finalizer agent."
    }
    mock_load_prompt.side_effect = lambda path: mock_prompts.get(path.split("/")[-1].replace(".txt", ""), "")
    
    # Test parameters
    test_prompts_dir = "/path/to/prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result
    assert result == mock_prompts
    assert len(result) == 7
    assert "planner" in result
    assert "critic_planner" in result
    assert "researcher" in result
    assert "critic_researcher" in result
    assert "expert" in result
    assert "critic_expert" in result
    assert "finalizer" in result
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Loading prompts from directory: {test_prompts_dir}")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")
    
    # Verify all prompt files were loaded
    assert mock_load_prompt.call_count == 7

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_auto_discovery(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with auto-discovery of directory."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock OS path operations for auto-discovery
    mock_os_path.dirname.return_value = "/path/to/script"
    mock_os_path.abspath.return_value = "/absolute/path/to/script"
    mock_os_path.join.return_value = "/path/to/prompts/baseline"
    mock_os_path.normpath.return_value = "/normalized/path/to/prompts/baseline"
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt content",
        "critic_planner": "Critic planner prompt content",
        "researcher": "Researcher prompt content",
        "critic_researcher": "Critic researcher prompt content",
        "expert": "Expert prompt content",
        "critic_expert": "Critic expert prompt content",
        "finalizer": "Finalizer prompt content"
    }
    mock_load_prompt.side_effect = lambda path: mock_prompts.get(path.split("/")[-1].replace(".txt", ""), "")
    
    # Execute function with None prompts_dir
    result = load_baseline_prompts(None)
    
    # Verify result
    assert result == mock_prompts
    assert len(result) == 7
    
    # Verify OS path operations were called for auto-discovery
    mock_os_path.dirname.assert_called_once()
    mock_os_path.abspath.assert_called_once()
    mock_os_path.join.assert_called()
    mock_os_path.normpath.assert_called_once()
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Loading prompts from directory: /normalized/path/to/prompts/baseline")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_missing_files(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with some missing files."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading with some files missing
    mock_load_prompt.side_effect = FileNotFoundError("File not found")
    
    # Test parameters
    test_prompts_dir = "/path/to/prompts/baseline"
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError):
        load_baseline_prompts(test_prompts_dir)
    
    # Verify logging was called
    mock_logger.info.assert_called_once_with(f"Loading prompts from directory: {test_prompts_dir}")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_empty_directory(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts from empty directory."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading returning empty strings
    mock_load_prompt.return_value = ""
    
    # Test parameters
    test_prompts_dir = "/path/to/empty/prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result contains all keys with empty values
    assert len(result) == 7
    assert all(value == "" for value in result.values())
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Loading prompts from directory: {test_prompts_dir}")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_large_content(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with large content."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading with large content
    large_content = "x" * 10000
    mock_prompts = {
        "planner": large_content,
        "critic_planner": large_content,
        "researcher": large_content,
        "critic_researcher": large_content,
        "expert": large_content,
        "critic_expert": large_content,
        "finalizer": large_content
    }
    mock_load_prompt.side_effect = lambda path: large_content
    
    # Test parameters
    test_prompts_dir = "/path/to/prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result
    assert len(result) == 7
    assert all(len(value) == 10000 for value in result.values())
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Loading prompts from directory: {test_prompts_dir}")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_special_characters_path(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with special characters in path."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt",
        "critic_planner": "Critic planner prompt",
        "researcher": "Researcher prompt",
        "critic_researcher": "Critic researcher prompt",
        "expert": "Expert prompt",
        "critic_expert": "Critic expert prompt",
        "finalizer": "Finalizer prompt"
    }
    mock_load_prompt.side_effect = lambda path: mock_prompts.get(path.split("/")[-1].replace(".txt", ""), "")
    
    # Test parameters with special characters
    test_prompts_dir = "/path/with/spaces and special chars/prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result
    assert result == mock_prompts
    assert len(result) == 7
    
    # Verify logging was called with special characters
    mock_logger.info.assert_any_call(f"Loading prompts from directory: {test_prompts_dir}")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_path_resolution_failure(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts when path resolution fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock OS path operations to fail
    mock_os_path.dirname.side_effect = Exception("Path resolution error")
    
    # Execute function with None prompts_dir and expect exception
    with pytest.raises(Exception) as exc_info:
        load_baseline_prompts(None)
    
    assert "Path resolution error" in str(exc_info.value)
    
    # Verify OS path operations were called
    mock_os_path.dirname.assert_called_once()

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_empty_directory_path(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with empty directory path."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt",
        "critic_planner": "Critic planner prompt",
        "researcher": "Researcher prompt",
        "critic_researcher": "Critic researcher prompt",
        "expert": "Expert prompt",
        "critic_expert": "Critic expert prompt",
        "finalizer": "Finalizer prompt"
    }
    mock_load_prompt.side_effect = lambda path: mock_prompts.get(path.split("/")[-1].replace(".txt", ""), "")
    
    # Test parameters with empty path
    test_prompts_dir = ""
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result
    assert result == mock_prompts
    assert len(result) == 7
    
    # Verify logging was called with empty path
    mock_logger.info.assert_any_call("Loading prompts from directory: ")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_relative_path(mock_os_path, mock_logger, mock_load_prompt):
    """Test loading baseline prompts with relative path."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt",
        "critic_planner": "Critic planner prompt",
        "researcher": "Researcher prompt",
        "critic_researcher": "Critic researcher prompt",
        "expert": "Expert prompt",
        "critic_expert": "Critic expert prompt",
        "finalizer": "Finalizer prompt"
    }
    mock_load_prompt.side_effect = lambda path: mock_prompts.get(path.split("/")[-1].replace(".txt", ""), "")
    
    # Test parameters with relative path
    test_prompts_dir = "./prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify result
    assert result == mock_prompts
    assert len(result) == 7
    
    # Verify logging was called with relative path
    mock_logger.info.assert_any_call("Loading prompts from directory: ./prompts/baseline")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")

@patch('main.load_prompt_from_file')
@patch('main.logger')
@patch('main.os.path')
def test_load_baseline_prompts_prompt_files_structure(mock_os_path, mock_logger, mock_load_prompt):
    """Test that all expected prompt files are processed."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.debug.return_value = None
    
    # Mock prompt loading
    mock_load_prompt.return_value = "Test prompt content"
    
    # Test parameters
    test_prompts_dir = "/path/to/prompts/baseline"
    
    # Execute function
    result = load_baseline_prompts(test_prompts_dir)
    
    # Verify all expected files are processed
    expected_files = [
        "planner_system_prompt.txt",
        "critic_planner_system_prompt.txt",
        "researcher_system_prompt.txt",
        "critic_researcher_system_prompt.txt",
        "expert_system_prompt.txt",
        "critic_expert_system_prompt.txt",
        "finalizer_system_prompt.txt"
    ]
    
    # Verify result contains all expected keys
    assert len(result) == 7
    assert "planner" in result
    assert "critic_planner" in result
    assert "researcher" in result
    assert "critic_researcher" in result
    assert "expert" in result
    assert "critic_expert" in result
    assert "finalizer" in result
    
    # Verify all files were loaded
    assert mock_load_prompt.call_count == 7
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Loading prompts from directory: {test_prompts_dir}")
    mock_logger.info.assert_any_call("Successfully loaded 7 prompts")
```

#### 3.10.3 JSONL File Reader

**Function Information:**
- **Function Name**: `read_jsonl_file`
- **Location**: `main.py:87`
- **Purpose**: Reads a JSONL file line by line and yields each parsed JSON object, handling empty lines and JSON parsing errors gracefully

**Function Signature and Parameters:**
- **Input**: `file_path: str` - Path to the JSONL file to read
- **Return**: Generator that yields `dict` - Parsed JSON object from each non-empty line
- **Side Effects**: Logs debug and error messages, opens and reads file

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.debug()` - Logging dependency for debug messages
  - `logger.error()` - Logging dependency for error messages
  - `builtins.open()` - File system dependency for reading files
  - `json.loads()` - JSON parsing dependency
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, generator yield, with statement, for loop)
  - String operations (`strip()`, f-string formatting)
  - Conditional logic (`if line:`, `try/except`)
  - Exception handling (`json.JSONDecodeError`)
  - Generator functionality (`yield`)
- **Mock Configuration**: Mock file operations, JSON parsing, and logging calls

**Test Cases:**

**Happy Path:**
- **Valid JSONL File**: File with multiple valid JSON objects
- **Single JSON Object**: File with one JSON object
- **Mixed Data Types**: JSON objects with various data types (strings, numbers, booleans, arrays, nested objects)
- **Normal Whitespace**: Lines with normal whitespace that gets stripped
- **All Lines Valid**: Every line contains valid JSON

**Edge Cases:**
- **Empty File**: File with no content
- **Empty Lines**: File with empty lines that should be skipped
- **Whitespace Only Lines**: Lines with only spaces/tabs/newlines
- **Large JSON Objects**: Very large JSON objects on single lines
- **Many Lines**: File with many lines (performance testing)
- **Special Characters**: JSON with special characters and Unicode
- **Nested Structures**: Deeply nested JSON objects
- **Mixed Empty/Valid**: File with mix of empty lines and valid JSON

**Error Conditions:**
- **File Not Found**: File path doesn't exist
- **Permission Denied**: No read permission for file
- **Invalid JSON Lines**: Lines with malformed JSON
- **Mixed Valid/Invalid**: File with mix of valid and invalid JSON lines
- **Encoding Issues**: File with encoding problems
- **Corrupted File**: File with corrupted content
- **Empty File Path**: Empty string as file path
- **None File Path**: None as file path

**State Changes:**
- **No State Changes**: Function is a generator and doesn't modify external state
- **File Handle**: File is opened and closed properly
- **Logging**: Debug and error messages may be logged
- **Generator State**: Generator yields values and can be resumed

**Mock Configurations:**
```python
@patch('main.json')
@patch('main.logger')
@patch('builtins.open')
def test_read_jsonl_file_with_mocked_dependencies(mock_open, mock_logger, mock_json):
    """Mock file operations, JSON parsing, and logging for testing."""
    # Mock logger calls
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    mock_file_content = [
        '{"name": "John", "age": 30}',
        '',
        '{"name": "Jane", "age": 25}',
        '   ',
        '{"name": "Bob", "age": 35}'
    ]
    mock_file = Mock()
    mock_file.__iter__ = Mock(return_value=iter(mock_file_content))
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Mock JSON parsing
    mock_json.loads.side_effect = [
        {"name": "John", "age": 30},
        {"name": "Jane", "age": 25},
        {"name": "Bob", "age": 35}
    ]
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result
    assert len(result) == 3
    assert result[0] == {"name": "John", "age": 30}
    assert result[1] == {"name": "Jane", "age": 25}
    assert result[2] == {"name": "Bob", "age": 35}
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and file handling in the implementation:
def read_jsonl_file(file_path: str) -> dict:  # Direct type annotations
    """Read a JSONL file line by line and yield each parsed JSON object."""
    
    logger.debug(f"Reading JSONL file: {file_path}")  # Direct logging call with f-string formatting
    
    with open(file_path, "r") as f:  # Direct file opening with context manager
        for line in f:  # Direct file iteration
            line = line.strip()  # Direct string operation to remove whitespace
            if line:  # Direct conditional logic to skip empty lines
                try:
                    yield json.loads(line)  # Direct JSON parsing and generator yield
                except json.JSONDecodeError as e:  # Direct exception handling
                    logger.error(f"Error parsing JSON line: {e}")  # Direct logging call with f-string formatting
                    continue  # Direct control flow
```

**Assertion Specifications:**
- Verify file is opened with correct path and mode
- Verify all valid JSON lines are parsed and yielded
- Verify empty lines and whitespace-only lines are skipped
- Verify JSON parsing errors are logged and skipped
- Verify debug logging is called with correct file path
- Verify error logging is called for invalid JSON lines
- Verify generator yields correct number of objects
- Verify file handle is properly closed
- Verify error conditions are properly handled and logged

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, mock_open
from main import read_jsonl_file

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_valid_content(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with valid JSON content."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    mock_file_content = [
        '{"name": "John", "age": 30, "city": "New York"}',
        '{"name": "Jane", "age": 25, "city": "Los Angeles"}',
        '{"name": "Bob", "age": 35, "city": "Chicago"}'
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing
    mock_json.loads.side_effect = [
        {"name": "John", "age": 30, "city": "New York"},
        {"name": "Jane", "age": 25, "city": "Los Angeles"},
        {"name": "Bob", "age": 35, "city": "Chicago"}
    ]
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result
    assert len(result) == 3
    assert result[0] == {"name": "John", "age": 30, "city": "New York"}
    assert result[1] == {"name": "Jane", "age": 25, "city": "Los Angeles"}
    assert result[2] == {"name": "Bob", "age": 35, "city": "Chicago"}
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_file_path, "r")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_empty_lines(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with empty lines that should be skipped."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content with empty lines
    mock_file_content = [
        '{"name": "John", "age": 30}',
        '',
        '   ',
        '\t',
        '\n',
        '{"name": "Jane", "age": 25}',
        '',
        '{"name": "Bob", "age": 35}'
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing
    mock_json.loads.side_effect = [
        {"name": "John", "age": 30},
        {"name": "Jane", "age": 25},
        {"name": "Bob", "age": 35}
    ]
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result - only 3 valid JSON objects
    assert len(result) == 3
    assert result[0] == {"name": "John", "age": 30}
    assert result[1] == {"name": "Jane", "age": 25}
    assert result[2] == {"name": "Bob", "age": 35}
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_invalid_json_lines(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with invalid JSON lines that should be skipped."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content with invalid JSON
    mock_file_content = [
        '{"name": "John", "age": 30}',
        'invalid json line',
        '{"name": "Jane", "age": 25}',
        '{incomplete json',
        '{"name": "Bob", "age": 35}',
        'null',
        '{"name": "Alice", "age": 28}'
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing with some errors
    def mock_json_loads(line):
        if line in ['invalid json line', '{incomplete json']:
            raise json.JSONDecodeError("Invalid JSON", line, 0)
        elif line == 'null':
            return None
        else:
            return json.loads(line)
    
    mock_json.loads.side_effect = mock_json_loads
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result - only valid JSON objects
    assert len(result) == 4
    assert result[0] == {"name": "John", "age": 30}
    assert result[1] == {"name": "Jane", "age": 25}
    assert result[2] == {"name": "Bob", "age": 35}
    assert result[3] == {"name": "Alice", "age": 28}
    
    # Verify error logging was called for invalid lines
    assert mock_logger.error.call_count == 2
    mock_logger.error.assert_any_call("Error parsing JSON line: Invalid JSON")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_empty_file(mock_file, mock_logger, mock_json):
    """Test reading empty JSONL file."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock empty file content
    mock_file_content = []
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Test parameters
    test_file_path = "/path/to/empty.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result is empty
    assert len(result) == 0
    assert result == []
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_mixed_data_types(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with various data types."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content with various data types
    mock_file_content = [
        '{"string": "hello", "number": 42, "boolean": true, "null": null}',
        '{"array": [1, 2, 3], "nested": {"key": "value"}}',
        '{"float": 3.14, "negative": -10, "zero": 0}',
        '{"unicode": "cafÃ©", "special": "\\n\\t\\"quoted\\""}'
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing
    mock_json.loads.side_effect = [
        {"string": "hello", "number": 42, "boolean": True, "null": None},
        {"array": [1, 2, 3], "nested": {"key": "value"}},
        {"float": 3.14, "negative": -10, "zero": 0},
        {"unicode": "cafÃ©", "special": "\n\t\"quoted\""}
    ]
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result
    assert len(result) == 4
    assert result[0] == {"string": "hello", "number": 42, "boolean": True, "null": None}
    assert result[1] == {"array": [1, 2, 3], "nested": {"key": "value"}}
    assert result[2] == {"float": 3.14, "negative": -10, "zero": 0}
    assert result[3] == {"unicode": "cafÃ©", "special": "\n\t\"quoted\""}
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', side_effect=FileNotFoundError("File not found"))
def test_read_jsonl_file_file_not_found(mock_open, mock_logger, mock_json):
    """Test reading JSONL file that doesn't exist."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Test parameters
    test_file_path = "/path/to/nonexistent.jsonl"
    
    # Execute function and expect exception
    with pytest.raises(FileNotFoundError):
        list(read_jsonl_file(test_file_path))
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with(test_file_path, "r")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', side_effect=PermissionError("Permission denied"))
def test_read_jsonl_file_permission_denied(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with permission denied."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Test parameters
    test_file_path = "/path/to/protected.jsonl"
    
    # Execute function and expect exception
    with pytest.raises(PermissionError):
        list(read_jsonl_file(test_file_path))
    
    # Verify file was attempted to be opened
    mock_file.assert_called_once_with(test_file_path, "r")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_large_content(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with large JSON objects."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock large file content
    large_json = '{"data": "' + "x" * 10000 + '", "id": 1}'
    mock_file_content = [large_json] * 10
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing
    large_object = {"data": "x" * 10000, "id": 1}
    mock_json.loads.return_value = large_object
    
    # Test parameters
    test_file_path = "/path/to/large.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result
    assert len(result) == 10
    assert all(obj == large_object for obj in result)
    assert all(len(obj["data"]) == 10000 for obj in result)
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_generator_behavior(mock_file, mock_logger, mock_json):
    """Test that the function behaves as a proper generator."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content
    mock_file_content = [
        '{"id": 1, "name": "First"}',
        '{"id": 2, "name": "Second"}',
        '{"id": 3, "name": "Third"}'
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing
    mock_json.loads.side_effect = [
        {"id": 1, "name": "First"},
        {"id": 2, "name": "Second"},
        {"id": 3, "name": "Third"}
    ]
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function as generator
    generator = read_jsonl_file(test_file_path)
    
    # Verify it's a generator
    assert hasattr(generator, '__iter__')
    assert hasattr(generator, '__next__')
    
    # Verify first item
    first_item = next(generator)
    assert first_item == {"id": 1, "name": "First"}
    
    # Verify second item
    second_item = next(generator)
    assert second_item == {"id": 2, "name": "Second"}
    
    # Verify third item
    third_item = next(generator)
    assert third_item == {"id": 3, "name": "Third"}
    
    # Verify generator is exhausted
    with pytest.raises(StopIteration):
        next(generator)
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_whitespace_handling(mock_file, mock_logger, mock_json):
    """Test proper handling of whitespace in JSONL file."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content with various whitespace
    mock_file_content = [
        '  {"name": "John", "age": 30}  ',
        '\t{"name": "Jane", "age": 25}\t',
        '\n{"name": "Bob", "age": 35}\n',
        '   \t\n{"name": "Alice", "age": 28}\n\t   '
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing
    mock_json.loads.side_effect = [
        {"name": "John", "age": 30},
        {"name": "Jane", "age": 25},
        {"name": "Bob", "age": 35},
        {"name": "Alice", "age": 28}
    ]
    
    # Test parameters
    test_file_path = "/path/to/test.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result
    assert len(result) == 4
    assert result[0] == {"name": "John", "age": 30}
    assert result[1] == {"name": "Jane", "age": 25}
    assert result[2] == {"name": "Bob", "age": 35}
    assert result[3] == {"name": "Alice", "age": 28}
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_read_jsonl_file_mixed_valid_invalid(mock_file, mock_logger, mock_json):
    """Test reading JSONL file with mix of valid and invalid lines."""
    # Setup mocks
    mock_logger.debug.return_value = None
    mock_logger.error.return_value = None
    
    # Mock file content with mix of valid and invalid
    mock_file_content = [
        '{"valid": "json1"}',
        'invalid json line',
        '',
        '{"valid": "json2"}',
        '   ',
        '{incomplete',
        '{"valid": "json3"}',
        'null',
        '{"valid": "json4"}'
    ]
    mock_file.return_value.__iter__.return_value = iter(mock_file_content)
    
    # Mock JSON parsing with errors for invalid lines
    def mock_json_loads(line):
        if line in ['invalid json line', '{incomplete']:
            raise json.JSONDecodeError("Invalid JSON", line, 0)
        elif line == 'null':
            return None
        else:
            return json.loads(line)
    
    mock_json.loads.side_effect = mock_json_loads
    
    # Test parameters
    test_file_path = "/path/to/mixed.jsonl"
    
    # Execute function
    result = list(read_jsonl_file(test_file_path))
    
    # Verify result - only valid JSON objects
    assert len(result) == 5
    assert result[0] == {"valid": "json1"}
    assert result[1] == {"valid": "json2"}
    assert result[2] == {"valid": "json3"}
    assert result[3] is None  # null value
    assert result[4] == {"valid": "json4"}
    
    # Verify error logging was called for invalid lines
    assert mock_logger.error.call_count == 2
    mock_logger.error.assert_any_call("Error parsing JSON line: Invalid JSON")
    
    # Verify logging was called
    mock_logger.debug.assert_called_once_with(f"Reading JSONL file: {test_file_path}")
```
#### 3.10.4 JSONL File Writer

**Function Information:**
- **Function Name**: `write_jsonl_file`
- **Location**: `main.py:109`
- **Purpose**: Writes a list of data to a JSONL file, with each element as a separate line, handling JSON serialization and file writing

**Function Signature and Parameters:**
- **Input**: `data_list: list[dict]` - List of data to write (can be dicts, strings, etc.)
- **Input**: `output_file_path: str` - Path to the output JSONL file
- **Return**: `None` - No return value
- **Side Effects**: Creates or overwrites file, logs info messages

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for info messages
  - `builtins.open()` - File system dependency for writing files
  - `json.dumps()` - JSON serialization dependency
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, with statement, for loop)
  - List operations (`len()`, iteration)
  - String operations (f-string formatting, concatenation)
  - Variable assignment and control flow
- **Mock Configuration**: Mock file operations, JSON serialization, and logging calls

**Test Cases:**

**Happy Path:**
- **Valid Data List**: List with multiple valid JSON-serializable objects
- **Single Item**: List with one item
- **Mixed Data Types**: List with various data types (dicts, strings, numbers, booleans, arrays, nested objects)
- **Empty List**: List with no items (should create empty file)
- **All Items Serializable**: Every item can be serialized to JSON

**Edge Cases:**
- **Large Data List**: List with many items
- **Large Individual Items**: Items with very large content
- **Special Characters**: Data with special characters and Unicode
- **Nested Structures**: Deeply nested data structures
- **Mixed Data Types**: List with mix of different data types
- **Empty Strings**: Items with empty string values
- **Null Values**: Items with None/null values
- **Complex Objects**: Items with complex object structures

**Error Conditions:**
- **File Path Issues**: Invalid file path, permission denied, disk full
- **Non-Serializable Data**: Items that cannot be serialized to JSON
- **Empty Data List**: None or empty list as input
- **Invalid File Path**: Empty string, None, or invalid path
- **Directory Not Exists**: Output directory doesn't exist
- **Encoding Issues**: Data with encoding problems
- **File System Errors**: Various file system related errors
- **JSON Serialization Errors**: Items that cause JSON serialization to fail

**State Changes:**
- **File Creation**: New file is created or existing file is overwritten
- **File Content**: File contains JSON lines separated by newlines
- **Logging**: Info messages are logged for start and completion
- **No Return Value**: Function returns None

**Mock Configurations:**
```python
@patch('main.json')
@patch('main.logger')
@patch('builtins.open')
def test_write_jsonl_file_with_mocked_dependencies(mock_open, mock_logger, mock_json):
    """Mock file operations, JSON serialization, and logging for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock file object
    mock_file = Mock()
    mock_file.write = Mock()
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Mock JSON serialization
    mock_json.dumps.side_effect = [
        '{"name": "John", "age": 30}',
        '{"name": "Jane", "age": 25}',
        '{"name": "Bob", "age": 35}'
    ]
    
    # Test parameters
    test_data_list = [
        {"name": "John", "age": 30},
        {"name": "Jane", "age": 25},
        {"name": "Bob", "age": 35}
    ]
    test_output_path = "/path/to/output.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 3
    
    # Verify file writes were called
    assert mock_file.write.call_count == 3
    mock_file.write.assert_any_call('{"name": "John", "age": 30}\n')
    mock_file.write.assert_any_call('{"name": "Jane", "age": 25}\n')
    mock_file.write.assert_any_call('{"name": "Bob", "age": 35}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and file handling in the implementation:
def write_jsonl_file(data_list: list[dict], output_file_path: str) -> None:  # Direct type annotations
    """Write a list of data to a JSONL file, with each element as a separate line."""
    
    logger.info(f"Writing {len(data_list)} items to: {output_file_path}")  # Direct logging call with f-string formatting
    
    with open(output_file_path, "w") as f:  # Direct file opening with context manager
        for item in data_list:  # Direct list iteration
            json_line = json.dumps(item)  # Direct JSON serialization
            f.write(json_line + "\n")  # Direct file writing with string concatenation
    
    logger.info(f"Successfully wrote {len(data_list)} items to: {output_file_path}")  # Direct logging call with f-string formatting
```

**Assertion Specifications:**
- Verify file is opened with correct path and write mode
- Verify JSON serialization is called for each item in the list
- Verify file write operations are called with correct content
- Verify each line ends with newline character
- Verify logging calls are made with correct messages and counts
- Verify file handle is properly closed
- Verify error conditions are properly handled and propagated
- Verify empty lists create empty files
- Verify all items are processed in order

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, mock_open
from main import write_jsonl_file

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_valid_data(mock_file, mock_logger, mock_json):
    """Test writing valid data to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization
    mock_json.dumps.side_effect = [
        '{"name": "John", "age": 30, "city": "New York"}',
        '{"name": "Jane", "age": 25, "city": "Los Angeles"}',
        '{"name": "Bob", "age": 35, "city": "Chicago"}'
    ]
    
    # Test parameters
    test_data_list = [
        {"name": "John", "age": 30, "city": "New York"},
        {"name": "Jane", "age": 25, "city": "Los Angeles"},
        {"name": "Bob", "age": 35, "city": "Chicago"}
    ]
    test_output_path = "/path/to/output.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 3
    mock_json.dumps.assert_any_call({"name": "John", "age": 30, "city": "New York"})
    mock_json.dumps.assert_any_call({"name": "Jane", "age": 25, "city": "Los Angeles"})
    mock_json.dumps.assert_any_call({"name": "Bob", "age": 35, "city": "Chicago"})
    
    # Verify file writes were called with correct content
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 3
    mock_file_handle.write.assert_any_call('{"name": "John", "age": 30, "city": "New York"}\n')
    mock_file_handle.write.assert_any_call('{"name": "Jane", "age": 25, "city": "Los Angeles"}\n')
    mock_file_handle.write.assert_any_call('{"name": "Bob", "age": 35, "city": "Chicago"}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_empty_list(mock_file, mock_logger, mock_json):
    """Test writing empty list to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Test parameters
    test_data_list = []
    test_output_path = "/path/to/empty.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was not called
    mock_json.dumps.assert_not_called()
    
    # Verify no file writes were called
    mock_file_handle = mock_file()
    mock_file_handle.write.assert_not_called()
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_single_item(mock_file, mock_logger, mock_json):
    """Test writing single item to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization
    mock_json.dumps.return_value = '{"id": 1, "name": "Single Item"}'
    
    # Test parameters
    test_data_list = [{"id": 1, "name": "Single Item"}]
    test_output_path = "/path/to/single.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called once
    mock_json.dumps.assert_called_once_with({"id": 1, "name": "Single Item"})
    
    # Verify file write was called once
    mock_file_handle = mock_file()
    mock_file_handle.write.assert_called_once_with('{"id": 1, "name": "Single Item"}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_mixed_data_types(mock_file, mock_logger, mock_json):
    """Test writing list with mixed data types to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization
    mock_json.dumps.side_effect = [
        '{"string": "hello", "number": 42, "boolean": true, "null": null}',
        '{"array": [1, 2, 3], "nested": {"key": "value"}}',
        '{"float": 3.14, "negative": -10, "zero": 0}',
        '{"unicode": "cafÃ©", "special": "\\n\\t\\"quoted\\""}'
    ]
    
    # Test parameters
    test_data_list = [
        {"string": "hello", "number": 42, "boolean": True, "null": None},
        {"array": [1, 2, 3], "nested": {"key": "value"}},
        {"float": 3.14, "negative": -10, "zero": 0},
        {"unicode": "cafÃ©", "special": "\n\t\"quoted\""}
    ]
    test_output_path = "/path/to/mixed.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 4
    
    # Verify file writes were called with correct content
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 4
    mock_file_handle.write.assert_any_call('{"string": "hello", "number": 42, "boolean": true, "null": null}\n')
    mock_file_handle.write.assert_any_call('{"array": [1, 2, 3], "nested": {"key": "value"}}\n')
    mock_file_handle.write.assert_any_call('{"float": 3.14, "negative": -10, "zero": 0}\n')
    mock_file_handle.write.assert_any_call('{"unicode": "cafÃ©", "special": "\\n\\t\\"quoted\\""}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', side_effect=PermissionError("Permission denied"))
def test_write_jsonl_file_permission_denied(mock_open, mock_logger, mock_json):
    """Test writing JSONL file with permission denied."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Test parameters
    test_data_list = [{"name": "test"}]
    test_output_path = "/path/to/protected.jsonl"
    
    # Execute function and expect exception
    with pytest.raises(PermissionError):
        write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was attempted to be opened
    mock_open.assert_called_once_with(test_output_path, "w")
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with(f"Writing {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_json_serialization_error(mock_file, mock_logger, mock_json):
    """Test writing JSONL file with non-serializable data."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization to fail
    mock_json.dumps.side_effect = TypeError("Object of type function is not JSON serializable")
    
    # Test parameters with non-serializable data
    test_data_list = [{"name": "test", "func": lambda x: x}]
    test_output_path = "/path/to/error.jsonl"
    
    # Execute function and expect exception
    with pytest.raises(TypeError):
        write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was attempted
    mock_json.dumps.assert_called_once_with({"name": "test", "func": lambda x: x})
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with(f"Writing {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_large_data(mock_file, mock_logger, mock_json):
    """Test writing large data to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock large JSON content
    large_json = '{"data": "' + "x" * 10000 + '", "id": 1}'
    mock_json.dumps.return_value = large_json
    
    # Test parameters with large data
    large_data = {"data": "x" * 10000, "id": 1}
    test_data_list = [large_data] * 10
    test_output_path = "/path/to/large.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 10
    
    # Verify file writes were called
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 10
    mock_file_handle.write.assert_any_call(large_json + '\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_special_characters(mock_file, mock_logger, mock_json):
    """Test writing data with special characters to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization with special characters
    mock_json.dumps.side_effect = [
        '{"name": "JosÃ©", "city": "SÃ£o Paulo"}',
        '{"text": "Line 1\\nLine 2\\tTabbed", "quotes": "\\"quoted\\""}',
        '{"emoji": "ðŸš€", "unicode": "cafÃ©"}'
    ]
    
    # Test parameters with special characters
    test_data_list = [
        {"name": "JosÃ©", "city": "SÃ£o Paulo"},
        {"text": "Line 1\nLine 2\tTabbed", "quotes": "\"quoted\""},
        {"emoji": "ðŸš€", "unicode": "cafÃ©"}
    ]
    test_output_path = "/path/to/special.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 3
    
    # Verify file writes were called with correct content
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 3
    mock_file_handle.write.assert_any_call('{"name": "JosÃ©", "city": "SÃ£o Paulo"}\n')
    mock_file_handle.write.assert_any_call('{"text": "Line 1\\nLine 2\\tTabbed", "quotes": "\\"quoted\\""}\n')
    mock_file_handle.write.assert_any_call('{"emoji": "ðŸš€", "unicode": "cafÃ©"}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_nested_structures(mock_file, mock_logger, mock_json):
    """Test writing nested data structures to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization for nested structures
    mock_json.dumps.side_effect = [
        '{"user": {"name": "John", "address": {"city": "NYC", "zip": "10001"}}, "tags": ["admin", "user"]}',
        '{"config": {"settings": {"theme": "dark", "language": "en"}, "version": "1.0.0"}}'
    ]
    
    # Test parameters with nested structures
    test_data_list = [
        {
            "user": {
                "name": "John",
                "address": {
                    "city": "NYC",
                    "zip": "10001"
                }
            },
            "tags": ["admin", "user"]
        },
        {
            "config": {
                "settings": {
                    "theme": "dark",
                    "language": "en"
                },
                "version": "1.0.0"
            }
        }
    ]
    test_output_path = "/path/to/nested.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 2
    
    # Verify file writes were called with correct content
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 2
    mock_file_handle.write.assert_any_call('{"user": {"name": "John", "address": {"city": "NYC", "zip": "10001"}}, "tags": ["admin", "user"]}\n')
    mock_file_handle.write.assert_any_call('{"config": {"settings": {"theme": "dark", "language": "en"}, "version": "1.0.0"}}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_empty_strings_and_nulls(mock_file, mock_logger, mock_json):
    """Test writing data with empty strings and null values to JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization
    mock_json.dumps.side_effect = [
        '{"name": "", "description": null, "tags": []}',
        '{"id": 0, "value": null, "empty": ""}'
    ]
    
    # Test parameters with empty strings and nulls
    test_data_list = [
        {"name": "", "description": None, "tags": []},
        {"id": 0, "value": None, "empty": ""}
    ]
    test_output_path = "/path/to/empty_values.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called for each item
    assert mock_json.dumps.call_count == 2
    
    # Verify file writes were called with correct content
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 2
    mock_file_handle.write.assert_any_call('{"name": "", "description": null, "tags": []}\n')
    mock_file_handle.write.assert_any_call('{"id": 0, "value": null, "empty": ""}\n')
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_file_write_error(mock_file, mock_logger, mock_json):
    """Test writing JSONL file when file write fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization
    mock_json.dumps.return_value = '{"name": "test"}'
    
    # Mock file write to fail
    mock_file_handle = mock_file()
    mock_file_handle.write.side_effect = IOError("Disk full")
    
    # Test parameters
    test_data_list = [{"name": "test"}]
    test_output_path = "/path/to/error.jsonl"
    
    # Execute function and expect exception
    with pytest.raises(IOError):
        write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called
    mock_json.dumps.assert_called_once_with({"name": "test"})
    
    # Verify file write was attempted
    mock_file_handle.write.assert_called_once_with('{"name": "test"}\n')
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with(f"Writing {len(test_data_list)} items to: {test_output_path}")

@patch('main.json')
@patch('main.logger')
@patch('builtins.open', new_callable=mock_open)
def test_write_jsonl_file_order_preservation(mock_file, mock_logger, mock_json):
    """Test that items are written in the correct order."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock JSON serialization
    mock_json.dumps.side_effect = [
        '{"id": 1, "name": "First"}',
        '{"id": 2, "name": "Second"}',
        '{"id": 3, "name": "Third"}'
    ]
    
    # Test parameters
    test_data_list = [
        {"id": 1, "name": "First"},
        {"id": 2, "name": "Second"},
        {"id": 3, "name": "Third"}
    ]
    test_output_path = "/path/to/order.jsonl"
    
    # Execute function
    write_jsonl_file(test_data_list, test_output_path)
    
    # Verify file was opened correctly
    mock_file.assert_called_once_with(test_output_path, "w")
    
    # Verify JSON serialization was called in order
    assert mock_json.dumps.call_count == 3
    mock_json.dumps.assert_any_call({"id": 1, "name": "First"})
    mock_json.dumps.assert_any_call({"id": 2, "name": "Second"})
    mock_json.dumps.assert_any_call({"id": 3, "name": "Third"})
    
    # Verify file writes were called in order
    mock_file_handle = mock_file()
    assert mock_file_handle.write.call_count == 3
    
    # Get all write calls and verify order
    write_calls = mock_file_handle.write.call_args_list
    assert write_calls[0][0][0] == '{"id": 1, "name": "First"}\n'
    assert write_calls[1][0][0] == '{"id": 2, "name": "Second"}\n'
    assert write_calls[2][0][0] == '{"id": 3, "name": "Third"}\n'
    
    # Verify logging was called
    mock_logger.info.assert_any_call(f"Writing {len(test_data_list)} items to: {test_output_path}")
    mock_logger.info.assert_any_call(f"Successfully wrote {len(test_data_list)} items to: {test_output_path}")
```
#### 3.10.5 Agent Configuration Factory

**Function Information:**
- **Function Name**: `make_agent_configs`
- **Location**: `main.py:129`
- **Purpose**: Creates a dictionary of AgentConfig objects from a dictionary of prompts, configuring each agent with specific parameters and system prompts

**Function Signature and Parameters:**
- **Input**: `prompts: dict` - Dictionary of prompts for each agent
- **Return**: `dict[str, AgentConfig]` - Dictionary of agent configurations with agent names as keys
- **Side Effects**: Logs info messages for start and completion

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for info messages
  - `AgentConfig` - Class for creating agent configuration objects
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, dictionary operations, variable assignment)
  - Dictionary definition and key access
  - String operations and f-string formatting
  - List and type annotations
  - Control flow and return statement
- **Mock Configuration**: Mock logging calls and AgentConfig class instantiation

**Test Cases:**

**Happy Path:**
- **Complete Prompts Dictionary**: Dictionary with all required agent prompts
- **All Agent Types**: All five agent types (planner, researcher, expert, critic, finalizer) are configured
- **Valid Prompt Content**: Prompts contain valid string content
- **Correct Configuration Parameters**: Each agent has correct model, temperature, output schema, and retry limits
- **Proper Dictionary Structure**: Return dictionary has correct keys and AgentConfig values

**Edge Cases:**
- **Empty Prompts Dictionary**: Dictionary with no prompts
- **Missing Some Prompts**: Dictionary missing some agent prompts
- **Empty Prompt Strings**: Prompts with empty string content
- **Large Prompt Content**: Prompts with very large content
- **Special Characters in Prompts**: Prompts with special characters and Unicode
- **Duplicate Agent Names**: Dictionary with duplicate agent names
- **Extra Prompts**: Dictionary with extra prompts not used by the function
- **None Values in Prompts**: Dictionary with None values for some prompts

**Error Conditions:**
- **Missing Required Prompts**: Dictionary missing required agent prompts
- **Invalid Prompt Types**: Prompts with non-string values
- **AgentConfig Creation Failure**: AgentConfig constructor fails
- **KeyError on Prompt Access**: Accessing non-existent prompt keys
- **None Prompts Dictionary**: None as input instead of dictionary
- **Empty String Prompts**: Prompts with empty strings
- **Invalid Dictionary Structure**: Input is not a dictionary

**State Changes:**
- **No State Changes**: Function is pure and doesn't modify external state
- **Return Value**: Dictionary with all agent configurations is returned
- **Logging**: Info messages are logged for start and completion

**Mock Configurations:**
```python
@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_with_mocked_dependencies(mock_logger, mock_agent_config):
    """Mock AgentConfig class and logging for testing."""
    # Mock logger calls
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_planner_config = Mock()
    mock_researcher_config = Mock()
    mock_expert_config = Mock()
    mock_critic_config = Mock()
    mock_finalizer_config = Mock()
    
    mock_agent_config.side_effect = [
        mock_planner_config,
        mock_researcher_config,
        mock_expert_config,
        mock_critic_config,
        mock_finalizer_config
    ]
    
    # Test parameters
    test_prompts = {
        "planner": "You are a planner agent.",
        "critic_planner": "You are a critic for planner.",
        "researcher": "You are a researcher agent.",
        "critic_researcher": "You are a critic for researcher.",
        "expert": "You are an expert agent.",
        "critic_expert": "You are a critic for expert.",
        "finalizer": "You are a finalizer agent."
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and dictionary operations in the implementation:
def make_agent_configs(prompts: dict) -> dict[str, AgentConfig]:  # Direct type annotations
    """Make a dictionary of agent configs from the prompts."""
    
    logger.info("Creating agent configurations...")  # Direct logging call
    
    configs = {  # Direct dictionary definition
        "planner": AgentConfig(  # Direct AgentConfig instantiation
            name="planner", 
            provider="openai", 
            model="gpt-4o-mini", 
            temperature=0.0, 
            output_schema={"research_steps": list[str], "expert_steps": list[str]},  # Direct type annotations
            system_prompt=prompts["planner"],  # Direct dictionary key access
            retry_limit=3
        ),
        "researcher": AgentConfig(  # Direct AgentConfig instantiation
            name="researcher", 
            provider="openai", 
            model="gpt-4o-mini", 
            temperature=0.0, 
            output_schema={"result": str},  # Direct dictionary definition
            system_prompt=prompts["researcher"],  # Direct dictionary key access
            retry_limit=5
        ),
        "expert": AgentConfig(  # Direct AgentConfig instantiation
            name="expert", 
            provider="openai", 
            model="gpt-4o-mini", 
            temperature=0.0, 
            output_schema={"expert_answer": str, "reasoning_trace": str},  # Direct dictionary definition
            system_prompt=prompts["expert"],  # Direct dictionary key access
            retry_limit=5
        ),
        "critic": AgentConfig(  # Direct AgentConfig instantiation
            name="critic", 
            provider="openai", 
            model="gpt-4o-mini", 
            temperature=0.0, 
            output_schema={"decision": Literal["approve", "reject"], "feedback": str},  # Direct type annotations
            system_prompt={  # Direct nested dictionary definition
                "critic_planner": prompts["critic_planner"],  # Direct dictionary key access
                "critic_researcher": prompts["critic_researcher"],  # Direct dictionary key access
                "critic_expert": prompts["critic_expert"]  # Direct dictionary key access
            }, 
            retry_limit=None
        ),
        "finalizer": AgentConfig(  # Direct AgentConfig instantiation
            name="finalizer", 
            provider="openai", 
            model="gpt-4o-mini", 
            temperature=0.0, 
            output_schema={"final_answer": str, "final_reasoning_trace": str},  # Direct dictionary definition
            system_prompt=prompts["finalizer"],  # Direct dictionary key access
            retry_limit=None
        ),
    }
    
    logger.info(f"Created {len(configs)} agent configurations")  # Direct logging call with f-string formatting
    return configs  # Direct return statement
```

**Assertion Specifications:**
- Verify AgentConfig is called with correct parameters for each agent
- Verify all five agent types are created (planner, researcher, expert, critic, finalizer)
- Verify correct prompt content is passed to each agent
- Verify correct output schemas are defined for each agent
- Verify correct retry limits are set for each agent
- Verify logging calls are made with correct messages
- Verify return dictionary has correct structure and content
- Verify error conditions are properly handled and propagated
- Verify all required prompts are accessed from input dictionary

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from main import make_agent_configs

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_complete_prompts(mock_logger, mock_agent_config):
    """Test creating agent configs with complete prompts dictionary."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters
    test_prompts = {
        "planner": "You are a planner agent that creates research and expert steps.",
        "critic_planner": "You are a critic that reviews planner outputs.",
        "researcher": "You are a researcher agent that performs research tasks.",
        "critic_researcher": "You are a critic that reviews researcher outputs.",
        "expert": "You are an expert agent that provides expert analysis.",
        "critic_expert": "You are a critic that reviews expert outputs.",
        "finalizer": "You are a finalizer agent that creates final answers."
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify AgentConfig calls with correct parameters
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["name"] == "planner"
    assert planner_call[1]["provider"] == "openai"
    assert planner_call[1]["model"] == "gpt-4o-mini"
    assert planner_call[1]["temperature"] == 0.0
    assert planner_call[1]["system_prompt"] == test_prompts["planner"]
    assert planner_call[1]["retry_limit"] == 3
    
    researcher_call = mock_agent_config.call_args_list[1]
    assert researcher_call[1]["name"] == "researcher"
    assert researcher_call[1]["system_prompt"] == test_prompts["researcher"]
    assert researcher_call[1]["retry_limit"] == 5
    
    expert_call = mock_agent_config.call_args_list[2]
    assert expert_call[1]["name"] == "expert"
    assert expert_call[1]["system_prompt"] == test_prompts["expert"]
    assert expert_call[1]["retry_limit"] == 5
    
    critic_call = mock_agent_config.call_args_list[3]
    assert critic_call[1]["name"] == "critic"
    assert critic_call[1]["system_prompt"] == {
        "critic_planner": test_prompts["critic_planner"],
        "critic_researcher": test_prompts["critic_researcher"],
        "critic_expert": test_prompts["critic_expert"]
    }
    assert critic_call[1]["retry_limit"] is None
    
    finalizer_call = mock_agent_config.call_args_list[4]
    assert finalizer_call[1]["name"] == "finalizer"
    assert finalizer_call[1]["system_prompt"] == test_prompts["finalizer"]
    assert finalizer_call[1]["retry_limit"] is None
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_empty_prompts(mock_logger, mock_agent_config):
    """Test creating agent configs with empty prompts dictionary."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters with empty prompts
    test_prompts = {}
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        make_agent_configs(test_prompts)
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with("Creating agent configurations...")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_missing_prompts(mock_logger, mock_agent_config):
    """Test creating agent configs with missing prompts."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Test parameters with missing prompts
    test_prompts = {
        "planner": "You are a planner agent.",
        "researcher": "You are a researcher agent."
        # Missing other required prompts
    }
    
    # Execute function and expect KeyError
    with pytest.raises(KeyError):
        make_agent_configs(test_prompts)
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with("Creating agent configurations...")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_empty_string_prompts(mock_logger, mock_agent_config):
    """Test creating agent configs with empty string prompts."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters with empty string prompts
    test_prompts = {
        "planner": "",
        "critic_planner": "",
        "researcher": "",
        "critic_researcher": "",
        "expert": "",
        "critic_expert": "",
        "finalizer": ""
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify empty prompts are passed correctly
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["system_prompt"] == ""
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_large_prompt_content(mock_logger, mock_agent_config):
    """Test creating agent configs with large prompt content."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters with large prompt content
    large_prompt = "x" * 10000
    test_prompts = {
        "planner": large_prompt,
        "critic_planner": large_prompt,
        "researcher": large_prompt,
        "critic_researcher": large_prompt,
        "expert": large_prompt,
        "critic_expert": large_prompt,
        "finalizer": large_prompt
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify large prompts are passed correctly
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["system_prompt"] == large_prompt
    assert len(planner_call[1]["system_prompt"]) == 10000
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_special_characters(mock_logger, mock_agent_config):
    """Test creating agent configs with special characters in prompts."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters with special characters
    test_prompts = {
        "planner": "You are a planner agent with special chars: cafÃ©, ðŸš€, \"quoted\"",
        "critic_planner": "You are a critic with special chars: SÃ£o Paulo, \n\t",
        "researcher": "You are a researcher with special chars: ä½ å¥½, ã“ã‚“ã«ã¡ã¯",
        "critic_researcher": "You are a critic with special chars: JosÃ©, MarÃ­a",
        "expert": "You are an expert with special chars: Î±Î²Î³, Î”Î•Î–",
        "critic_expert": "You are a critic with special chars: 123, 456",
        "finalizer": "You are a finalizer with special chars: !@#, $%^"
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify special character prompts are passed correctly
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["system_prompt"] == test_prompts["planner"]
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_extra_prompts(mock_logger, mock_agent_config):
    """Test creating agent configs with extra prompts not used by the function."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters with extra prompts
    test_prompts = {
        "planner": "You are a planner agent.",
        "critic_planner": "You are a critic for planner.",
        "researcher": "You are a researcher agent.",
        "critic_researcher": "You are a critic for researcher.",
        "expert": "You are an expert agent.",
        "critic_expert": "You are a critic for expert.",
        "finalizer": "You are a finalizer agent.",
        "extra_agent": "This prompt is not used.",
        "another_extra": "This is also not used."
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify extra prompts are not used
    assert "extra_agent" not in result
    assert "another_extra" not in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_none_values(mock_logger, mock_agent_config):
    """Test creating agent configs with None values in prompts."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters with None values
    test_prompts = {
        "planner": None,
        "critic_planner": None,
        "researcher": None,
        "critic_researcher": None,
        "expert": None,
        "critic_expert": None,
        "finalizer": None
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    assert "planner" in result
    assert "researcher" in result
    assert "expert" in result
    assert "critic" in result
    assert "finalizer" in result
    
    # Verify AgentConfig was called for each agent
    assert mock_agent_config.call_count == 5
    
    # Verify None prompts are passed correctly
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["system_prompt"] is None
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_agentconfig_creation_failure(mock_logger, mock_agent_config):
    """Test creating agent configs when AgentConfig creation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig to fail
    mock_agent_config.side_effect = Exception("AgentConfig creation failed")
    
    # Test parameters
    test_prompts = {
        "planner": "You are a planner agent.",
        "critic_planner": "You are a critic for planner.",
        "researcher": "You are a researcher agent.",
        "critic_researcher": "You are a critic for researcher.",
        "expert": "You are an expert agent.",
        "critic_expert": "You are a critic for expert.",
        "finalizer": "You are a finalizer agent."
    }
    
    # Execute function and expect exception
    with pytest.raises(Exception) as exc_info:
        make_agent_configs(test_prompts)
    
    assert "AgentConfig creation failed" in str(exc_info.value)
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with("Creating agent configurations...")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_output_schemas(mock_logger, mock_agent_config):
    """Test that output schemas are correctly defined for each agent."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters
    test_prompts = {
        "planner": "You are a planner agent.",
        "critic_planner": "You are a critic for planner.",
        "researcher": "You are a researcher agent.",
        "critic_researcher": "You are a critic for researcher.",
        "expert": "You are an expert agent.",
        "critic_expert": "You are a critic for expert.",
        "finalizer": "You are a finalizer agent."
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    
    # Verify output schemas are correctly defined
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["output_schema"] == {"research_steps": list[str], "expert_steps": list[str]}
    
    researcher_call = mock_agent_config.call_args_list[1]
    assert researcher_call[1]["output_schema"] == {"result": str}
    
    expert_call = mock_agent_config.call_args_list[2]
    assert expert_call[1]["output_schema"] == {"expert_answer": str, "reasoning_trace": str}
    
    critic_call = mock_agent_config.call_args_list[3]
    assert critic_call[1]["output_schema"] == {"decision": Literal["approve", "reject"], "feedback": str}
    
    finalizer_call = mock_agent_config.call_args_list[4]
    assert finalizer_call[1]["output_schema"] == {"final_answer": str, "final_reasoning_trace": str}
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_retry_limits(mock_logger, mock_agent_config):
    """Test that retry limits are correctly set for each agent."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Mock AgentConfig instances
    mock_configs = {
        "planner": Mock(),
        "researcher": Mock(),
        "expert": Mock(),
        "critic": Mock(),
        "finalizer": Mock()
    }
    
    mock_agent_config.side_effect = list(mock_configs.values())
    
    # Test parameters
    test_prompts = {
        "planner": "You are a planner agent.",
        "critic_planner": "You are a critic for planner.",
        "researcher": "You are a researcher agent.",
        "critic_researcher": "You are a critic for researcher.",
        "expert": "You are an expert agent.",
        "critic_expert": "You are a critic for expert.",
        "finalizer": "You are a finalizer agent."
    }
    
    # Execute function
    result = make_agent_configs(test_prompts)
    
    # Verify result structure
    assert len(result) == 5
    
    # Verify retry limits are correctly set
    planner_call = mock_agent_config.call_args_list[0]
    assert planner_call[1]["retry_limit"] == 3
    
    researcher_call = mock_agent_config.call_args_list[1]
    assert researcher_call[1]["retry_limit"] == 5
    
    expert_call = mock_agent_config.call_args_list[2]
    assert expert_call[1]["retry_limit"] == 5
    
    critic_call = mock_agent_config.call_args_list[3]
    assert critic_call[1]["retry_limit"] is None
    
    finalizer_call = mock_agent_config.call_args_list[4]
    assert finalizer_call[1]["retry_limit"] is None
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Creating agent configurations...")
    mock_logger.info.assert_any_call("Created 5 agent configurations")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_none_input(mock_logger, mock_agent_config):
    """Test creating agent configs with None input."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Execute function and expect TypeError
    with pytest.raises(TypeError):
        make_agent_configs(None)
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with("Creating agent configurations...")

@patch('main.AgentConfig')
@patch('main.logger')
def test_make_agent_configs_invalid_input_type(mock_logger, mock_agent_config):
    """Test creating agent configs with invalid input type."""
    # Setup mocks
    mock_logger.info.return_value = None
    
    # Execute function and expect TypeError
    with pytest.raises(TypeError):
        make_agent_configs("not a dictionary")
    
    # Verify logging was called for start
    mock_logger.info.assert_called_once_with("Creating agent configurations...")
```
#### 3.10.6 Main Application Entry Point

**Function Information:**
- **Function Name**: `main`
- **Location**: `main.py:151`
- **Purpose**: Main application entry point that orchestrates the entire multi-agent system workflow, including prompt loading, agent configuration, graph creation, JSONL processing, and response writing

**Function Signature and Parameters:**
- **Input**: None - No parameters
- **Return**: `None` - No return value
- **Side Effects**: Logs info/error messages, creates files, processes JSONL data, invokes multi-agent graph, writes output files, may exit with error code

**Dependencies Analysis:**
- **Dependencies to Mock**: 
  - `logger.info()` - Logging dependency for info messages
  - `logger.error()` - Logging dependency for error messages
  - `load_baseline_prompts()` - Internal function for loading prompts
  - `make_agent_configs()` - Internal function for creating agent configurations
  - `create_multi_agent_graph()` - Internal function for creating the multi-agent graph
  - `read_jsonl_file()` - Internal function for reading JSONL file
  - `write_jsonl_file()` - Internal function for writing JSONL file
  - `time.sleep()` - Time delay dependency
  - `print()` - Console output dependency
  - `sys.exit()` - System exit dependency
- **Dependencies to Use Directly**: 
  - Built-in Python features (function definition, try/except, for loop, if/else)
  - List operations (append, iteration)
  - Dictionary operations (key access, value assignment)
  - String operations (f-string formatting, concatenation)
  - Variable assignment and control flow
- **Mock Configuration**: Mock all internal function calls, logging, time delays, console output, and system exit

**Test Cases:**

**Happy Path:**
- **Complete Workflow Success**: All steps complete successfully from start to finish
- **Valid JSONL Data**: JSONL file contains valid Level 1 questions with file names
- **Successful Graph Invocation**: Multi-agent graph processes questions successfully
- **Response Generation**: Responses are generated and collected correctly
- **File Writing Success**: Output file is written successfully
- **All Logging**: All expected logging messages are generated

**Edge Cases:**
- **Empty JSONL File**: JSONL file contains no data
- **No Level 1 Questions**: JSONL file contains no Level 1 questions
- **Empty File Names**: Some questions have empty file_name fields
- **Mixed Question Levels**: JSONL file contains questions of different levels
- **Single Question**: JSONL file contains only one Level 1 question
- **Many Questions**: JSONL file contains many Level 1 questions
- **Large Response Data**: Generated responses are very large
- **Special Characters**: Questions and file names contain special characters

**Error Conditions:**
- **Prompt Loading Failure**: `load_baseline_prompts()` fails
- **Agent Config Creation Failure**: `make_agent_configs()` fails
- **Graph Creation Failure**: `create_multi_agent_graph()` fails
- **JSONL File Reading Failure**: `read_jsonl_file()` fails
- **Graph Invocation Failure**: `graph.invoke()` fails
- **Response Writing Failure**: `write_jsonl_file()` fails
- **Missing Required Fields**: JSONL items missing required fields
- **Invalid JSONL Data**: Malformed JSONL data
- **File System Errors**: File system related errors
- **Network Errors**: Network-related errors during processing

**State Changes:**
- **Application State**: Application progresses through different states
- **Response Collection**: Responses list is built up during processing
- **File System**: Output files are created
- **Logging**: Multiple log messages are generated
- **System Exit**: Application may exit with error code on failure

**Mock Configurations:**
```python
@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_with_mocked_dependencies(mock_logger, mock_load_prompts, mock_make_configs, 
                                      mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                      mock_time, mock_sys):
    """Mock all dependencies for testing the main function."""
    # Mock logger calls
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt",
        "critic_planner": "Critic planner prompt",
        "researcher": "Researcher prompt",
        "critic_researcher": "Critic researcher prompt",
        "expert": "Expert prompt",
        "critic_expert": "Critic expert prompt",
        "finalizer": "Finalizer prompt"
    }
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading
    mock_jsonl_data = [
        {"Level": 1, "Question": "Test question 1", "file_name": "test1.txt", "task_id": "task1"},
        {"Level": 1, "Question": "Test question 2", "file_name": "", "task_id": "task2"}
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock graph invocation
    mock_graph.invoke.return_value = {
        "final_answer": "Test answer",
        "reasoning_trace": "Test reasoning"
    }
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock write JSONL
    mock_write_jsonl.return_value = None
    
    # Mock print and sys.exit
    mock_print = Mock()
    mock_sys.exit.return_value = None
    
    # Execute function
    main()
    
    # Verify all dependencies were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once_with(mock_prompts)
    mock_create_graph.assert_called_once_with(mock_agent_configs)
    mock_read_jsonl.assert_called_once()
    assert mock_graph.invoke.call_count == 2
    assert mock_time.sleep.call_count == 2
    mock_write_jsonl.assert_called_once()
    
    # Verify logging was called
    mock_logger.info.assert_any_call("Application started")
    mock_logger.info.assert_any_call("Application finished successfully")
```

**Direct Usage Examples:**
```python
# Direct usage of built-in functions and control flow in the implementation:
def main() -> None:  # Direct type annotations
    """Main function to run the application."""
    
    try:  # Direct exception handling
        logger.info("Application started")  # Direct logging call
        
        # Load baseline prompts
        logger.info("Loading baseline prompts...")  # Direct logging call
        prompts = load_baseline_prompts()  # Direct function call
        logger.info(f"Loaded {len(prompts)} prompts: {list(prompts.keys())}")  # Direct logging call with f-string formatting
        agent_configs = make_agent_configs(prompts)  # Direct function call
        
        # Create the multi-agent graph using the factory function
        logger.info("Creating multi-agent graph...")  # Direct logging call
        graph = create_multi_agent_graph(agent_configs)  # Direct function call
        logger.info("Graph created successfully!")  # Direct logging call
        
        jsonl_file_path = "/home/joe/python-proj/hf-agents-course/src/metadata.jsonl"  # Direct variable assignment
        
        responses = []  # Direct list initialization
        logger.info("Starting to process JSONL file...")  # Direct logging call

        for item in read_jsonl_file(jsonl_file_path):  # Direct function call and iteration
            if item["Level"] == 1:  # Direct conditional logic and dictionary key access
                logger.info(f"Processing question: {item["Question"]}")  # Direct logging call with f-string formatting
                if item["file_name"] != "":  # Direct conditional logic and string comparison
                    file_path = f"/home/joe/datum/gaia_lvl1/{item['file_name']}"  # Direct f-string formatting and dictionary key access
                else:
                    file_path = ""  # Direct variable assignment
                result = graph.invoke({"question": item["Question"], "file_path": file_path})  # Direct method call and dictionary creation
                response = {  # Direct dictionary creation
                    "task_id": item["task_id"],  # Direct dictionary key access
                    "model_answer": result["final_answer"],  # Direct dictionary key access
                    "reasoning_trace": result["reasoning_trace"]  # Direct dictionary key access
                }

                responses.append(response)  # Direct list operation
                logger.info(f"Completed question: {item["Question"]}")  # Direct logging call with f-string formatting
                time.sleep(5)  # Direct function call

        logger.info("Writing responses to output file...")  # Direct logging call
        write_jsonl_file(responses, "/home/joe/python-proj/hf-agents-course/src/gaia_lvl1_responses.jsonl")  # Direct function call
        logger.info("Application finished successfully")  # Direct logging call
    except Exception as e:  # Direct exception handling
        logger.error(f"Application failed: {str(e)}")  # Direct logging call with f-string formatting
        print(f"Application failed: {str(e)}")  # Direct function call with f-string formatting
        sys.exit(1)  # Direct function call
```

**Assertion Specifications:**
- Verify all internal functions are called with correct parameters
- Verify logging calls are made in correct sequence with correct messages
- Verify JSONL data is processed correctly (only Level 1 questions)
- Verify responses are collected and formatted correctly
- Verify graph invocation happens for each Level 1 question
- Verify time delays occur between questions
- Verify output file is written with collected responses
- Verify error handling works correctly and logs errors
- Verify system exit is called on failure
- Verify application completes successfully without errors

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from main import main

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_complete_workflow_success(mock_logger, mock_load_prompts, mock_make_configs, 
                                       mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                       mock_time, mock_sys):
    """Test complete successful workflow of the main function."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {
        "planner": "Planner prompt",
        "critic_planner": "Critic planner prompt",
        "researcher": "Researcher prompt",
        "critic_researcher": "Critic researcher prompt",
        "expert": "Expert prompt",
        "critic_expert": "Critic expert prompt",
        "finalizer": "Finalizer prompt"
    }
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading with Level 1 questions
    mock_jsonl_data = [
        {"Level": 1, "Question": "What is AI?", "file_name": "ai_doc.txt", "task_id": "task1"},
        {"Level": 1, "Question": "How does ML work?", "file_name": "", "task_id": "task2"},
        {"Level": 2, "Question": "Advanced question", "file_name": "advanced.txt", "task_id": "task3"}
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock graph invocation
    mock_graph.invoke.return_value = {
        "final_answer": "Test answer",
        "reasoning_trace": "Test reasoning"
    }
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock write JSONL
    mock_write_jsonl.return_value = None
    
    # Execute function
    main()
    
    # Verify prompt loading
    mock_load_prompts.assert_called_once()
    
    # Verify agent config creation
    mock_make_configs.assert_called_once_with(mock_prompts)
    
    # Verify graph creation
    mock_create_graph.assert_called_once_with(mock_agent_configs)
    
    # Verify JSONL reading
    mock_read_jsonl.assert_called_once()
    
    # Verify graph invocation only for Level 1 questions (2 calls)
    assert mock_graph.invoke.call_count == 2
    mock_graph.invoke.assert_any_call({"question": "What is AI?", "file_path": "/home/joe/datum/gaia_lvl1/ai_doc.txt"})
    mock_graph.invoke.assert_any_call({"question": "How does ML work?", "file_path": ""})
    
    # Verify time delays
    assert mock_time.sleep.call_count == 2
    
    # Verify response writing
    expected_responses = [
        {"task_id": "task1", "model_answer": "Test answer", "reasoning_trace": "Test reasoning"},
        {"task_id": "task2", "model_answer": "Test answer", "reasoning_trace": "Test reasoning"}
    ]
    mock_write_jsonl.assert_called_once_with(expected_responses, "/home/joe/python-proj/hf-agents-course/src/gaia_lvl1_responses.jsonl")
    
    # Verify logging sequence
    mock_logger.info.assert_any_call("Application started")
    mock_logger.info.assert_any_call("Loading baseline prompts...")
    mock_logger.info.assert_any_call("Loaded 7 prompts: ['planner', 'critic_planner', 'researcher', 'critic_researcher', 'expert', 'critic_expert', 'finalizer']")
    mock_logger.info.assert_any_call("Creating multi-agent graph...")
    mock_logger.info.assert_any_call("Graph created successfully!")
    mock_logger.info.assert_any_call("Starting to process JSONL file...")
    mock_logger.info.assert_any_call("Processing question: What is AI?")
    mock_logger.info.assert_any_call("Completed question: What is AI?")
    mock_logger.info.assert_any_call("Processing question: How does ML work?")
    mock_logger.info.assert_any_call("Completed question: How does ML work?")
    mock_logger.info.assert_any_call("Writing responses to output file...")
    mock_logger.info.assert_any_call("Application finished successfully")

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_empty_jsonl_file(mock_logger, mock_load_prompts, mock_make_configs, 
                              mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                              mock_time, mock_sys):
    """Test main function with empty JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock empty JSONL reading
    mock_read_jsonl.return_value = []
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock write JSONL
    mock_write_jsonl.return_value = None
    
    # Execute function
    main()
    
    # Verify all setup functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once()
    mock_read_jsonl.assert_called_once()
    
    # Verify no graph invocations (no Level 1 questions)
    mock_graph.invoke.assert_not_called()
    
    # Verify no time delays
    mock_time.sleep.assert_not_called()
    
    # Verify empty response writing
    mock_write_jsonl.assert_called_once_with([], "/home/joe/python-proj/hf-agents-course/src/gaia_lvl1_responses.jsonl")
    
    # Verify logging
    mock_logger.info.assert_any_call("Application started")
    mock_logger.info.assert_any_call("Application finished successfully")

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_no_level1_questions(mock_logger, mock_load_prompts, mock_make_configs, 
                                 mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                 mock_time, mock_sys):
    """Test main function with no Level 1 questions in JSONL file."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading with no Level 1 questions
    mock_jsonl_data = [
        {"Level": 2, "Question": "Advanced question 1", "file_name": "advanced1.txt", "task_id": "task1"},
        {"Level": 3, "Question": "Expert question 1", "file_name": "expert1.txt", "task_id": "task2"}
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock write JSONL
    mock_write_jsonl.return_value = None
    
    # Execute function
    main()
    
    # Verify all setup functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once()
    mock_read_jsonl.assert_called_once()
    
    # Verify no graph invocations (no Level 1 questions)
    mock_graph.invoke.assert_not_called()
    
    # Verify no time delays
    mock_time.sleep.assert_not_called()
    
    # Verify empty response writing
    mock_write_jsonl.assert_called_once_with([], "/home/joe/python-proj/hf-agents-course/src/gaia_lvl1_responses.jsonl")
    
    # Verify logging
    mock_logger.info.assert_any_call("Application started")
    mock_logger.info.assert_any_call("Application finished successfully")

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_prompt_loading_failure(mock_logger, mock_load_prompts, mock_make_configs, 
                                    mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                    mock_time, mock_sys):
    """Test main function when prompt loading fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading to fail
    mock_load_prompts.side_effect = Exception("Prompt loading failed")
    
    # Mock print and sys.exit
    mock_print = Mock()
    mock_sys.exit.return_value = None
    
    # Execute function
    main()
    
    # Verify prompt loading was attempted
    mock_load_prompts.assert_called_once()
    
    # Verify error logging
    mock_logger.error.assert_called_once_with("Application failed: Prompt loading failed")
    
    # Verify print was called
    # Note: print is built-in, so we can't easily mock it in this context
    
    # Verify system exit was called
    mock_sys.exit.assert_called_once_with(1)

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_graph_creation_failure(mock_logger, mock_load_prompts, mock_make_configs, 
                                    mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                    mock_time, mock_sys):
    """Test main function when graph creation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation to fail
    mock_create_graph.side_effect = Exception("Graph creation failed")
    
    # Mock print and sys.exit
    mock_print = Mock()
    mock_sys.exit.return_value = None
    
    # Execute function
    main()
    
    # Verify setup functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once_with(mock_agent_configs)
    
    # Verify error logging
    mock_logger.error.assert_called_once_with("Application failed: Graph creation failed")
    
    # Verify system exit was called
    mock_sys.exit.assert_called_once_with(1)

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_graph_invocation_failure(mock_logger, mock_load_prompts, mock_make_configs, 
                                      mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                      mock_time, mock_sys):
    """Test main function when graph invocation fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading
    mock_jsonl_data = [
        {"Level": 1, "Question": "Test question", "file_name": "test.txt", "task_id": "task1"}
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock graph invocation to fail
    mock_graph.invoke.side_effect = Exception("Graph invocation failed")
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock print and sys.exit
    mock_print = Mock()
    mock_sys.exit.return_value = None
    
    # Execute function
    main()
    
    # Verify setup functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once()
    mock_read_jsonl.assert_called_once()
    
    # Verify graph invocation was attempted
    mock_graph.invoke.assert_called_once()
    
    # Verify error logging
    mock_logger.error.assert_called_once_with("Application failed: Graph invocation failed")
    
    # Verify system exit was called
    mock_sys.exit.assert_called_once_with(1)

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_response_writing_failure(mock_logger, mock_load_prompts, mock_make_configs, 
                                      mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                      mock_time, mock_sys):
    """Test main function when response writing fails."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading
    mock_jsonl_data = [
        {"Level": 1, "Question": "Test question", "file_name": "test.txt", "task_id": "task1"}
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock graph invocation
    mock_graph.invoke.return_value = {
        "final_answer": "Test answer",
        "reasoning_trace": "Test reasoning"
    }
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock write JSONL to fail
    mock_write_jsonl.side_effect = Exception("Write failed")
    
    # Mock print and sys.exit
    mock_print = Mock()
    mock_sys.exit.return_value = None
    
    # Execute function
    main()
    
    # Verify setup functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once()
    mock_read_jsonl.assert_called_once()
    mock_graph.invoke.assert_called_once()
    mock_time.sleep.assert_called_once()
    
    # Verify write was attempted
    expected_responses = [
        {"task_id": "task1", "model_answer": "Test answer", "reasoning_trace": "Test reasoning"}
    ]
    mock_write_jsonl.assert_called_once_with(expected_responses, "/home/joe/python-proj/hf-agents-course/src/gaia_lvl1_responses.jsonl")
    
    # Verify error logging
    mock_logger.error.assert_called_once_with("Application failed: Write failed")
    
    # Verify system exit was called
    mock_sys.exit.assert_called_once_with(1)

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_missing_required_fields(mock_logger, mock_load_prompts, mock_make_configs, 
                                     mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                     mock_time, mock_sys):
    """Test main function with JSONL items missing required fields."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading with missing fields
    mock_jsonl_data = [
        {"Level": 1, "Question": "Test question"},  # Missing file_name and task_id
        {"Level": 1, "file_name": "test.txt", "task_id": "task1"}  # Missing Question
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock print and sys.exit
    mock_print = Mock()
    mock_sys.exit.return_value = None
    
    # Execute function and expect KeyError
    main()
    
    # Verify setup functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once()
    mock_read_jsonl.assert_called_once()
    
    # Verify error logging
    mock_logger.error.assert_called_once()
    assert "KeyError" in mock_logger.error.call_args[0][0]
    
    # Verify system exit was called
    mock_sys.exit.assert_called_once_with(1)

@patch('main.sys')
@patch('main.time')
@patch('main.write_jsonl_file')
@patch('main.read_jsonl_file')
@patch('main.create_multi_agent_graph')
@patch('main.make_agent_configs')
@patch('main.load_baseline_prompts')
@patch('main.logger')
def test_main_special_characters_in_data(mock_logger, mock_load_prompts, mock_make_configs, 
                                        mock_create_graph, mock_read_jsonl, mock_write_jsonl, 
                                        mock_time, mock_sys):
    """Test main function with special characters in questions and file names."""
    # Setup mocks
    mock_logger.info.return_value = None
    mock_logger.error.return_value = None
    
    # Mock prompt loading
    mock_prompts = {"planner": "test", "critic_planner": "test", "researcher": "test", 
                   "critic_researcher": "test", "expert": "test", "critic_expert": "test", "finalizer": "test"}
    mock_load_prompts.return_value = mock_prompts
    
    # Mock agent configs
    mock_agent_configs = {"planner": Mock(), "researcher": Mock(), "expert": Mock(), "critic": Mock(), "finalizer": Mock()}
    mock_make_configs.return_value = mock_agent_configs
    
    # Mock graph creation
    mock_graph = Mock()
    mock_create_graph.return_value = mock_graph
    
    # Mock JSONL reading with special characters
    mock_jsonl_data = [
        {"Level": 1, "Question": "What is AI? ðŸš€", "file_name": "ai_doc_ðŸš€.txt", "task_id": "task1"},
        {"Level": 1, "Question": "How does ML work? ä½ å¥½", "file_name": "ml_doc_ä½ å¥½.txt", "task_id": "task2"}
    ]
    mock_read_jsonl.return_value = mock_jsonl_data
    
    # Mock graph invocation
    mock_graph.invoke.return_value = {
        "final_answer": "Test answer with special chars: cafÃ©",
        "reasoning_trace": "Test reasoning with special chars: SÃ£o Paulo"
    }
    
    # Mock time sleep
    mock_time.sleep.return_value = None
    
    # Mock write JSONL
    mock_write_jsonl.return_value = None
    
    # Execute function
    main()
    
    # Verify all functions were called
    mock_load_prompts.assert_called_once()
    mock_make_configs.assert_called_once()
    mock_create_graph.assert_called_once()
    mock_read_jsonl.assert_called_once()
    
    # Verify graph invocation with special characters
    assert mock_graph.invoke.call_count == 2
    mock_graph.invoke.assert_any_call({"question": "What is AI? ðŸš€", "file_path": "/home/joe/datum/gaia_lvl1/ai_doc_ðŸš€.txt"})
    mock_graph.invoke.assert_any_call({"question": "How does ML work? ä½ å¥½", "file_path": "/home/joe/datum/gaia_lvl1/ml_doc_ä½ å¥½.txt"})
    
    # Verify time delays
    assert mock_time.sleep.call_count == 2
    
    # Verify response writing with special characters
    expected_responses = [
        {"task_id": "task1", "model_answer": "Test answer with special chars: cafÃ©", "reasoning_trace": "Test reasoning with special chars: SÃ£o Paulo"},
        {"task_id": "task2", "model_answer": "Test answer with special chars: cafÃ©", "reasoning_trace": "Test reasoning with special chars: SÃ£o Paulo"}
    ]
    mock_write_jsonl.assert_called_once_with(expected_responses, "/home/joe/python-proj/hf-agents-course/src/gaia_lvl1_responses.jsonl")
    
    # Verify logging
    mock_logger.info.assert_any_call("Application started")
    mock_logger.info.assert_any_call("Application finished successfully")
```

## 4. Integration Tests
### 4.1 Agent Communication Integration
#### 4.1.1 Message Flow Between Agents

**Integration Test Information:**
- **Test Name**: Message Flow Between Agents
- **Location**: Section 4.1.1
- **Purpose**: Tests the complete message flow patterns between orchestrator and different agent types (planner, researcher, expert, critic, finalizer), including instruction delivery, response handling, and feedback loops

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `execute_next_step()` - Orchestrator's message sending function
  - `compose_agent_message()` - Message composition function
  - `send_message()` - Message sending function
  - `get_agent_conversation()` - Message retrieval function
  - `create_planner_agent()` - Planner agent factory
  - `create_researcher_agent()` - Researcher agent factory
  - `create_expert_agent()` - Expert agent factory
  - `create_critic_agent()` - Critic agent factory
  - `create_finalizer_agent()` - Finalizer agent factory
  - `GraphState` - Shared state management
  - LLM factories and configurations
- **Integration Points**: 
  - Orchestrator's execute_next_step() calling compose_agent_message() and send_message()
  - Agent factories creating callable functions that process messages
  - Message flow between orchestrator and each agent type
  - State updates during message processing
  - LLM integration for agent responses
- **Data Flow**: 
  - Orchestrator determines next step and composes appropriate message
  - Message is sent and stored in GraphState
  - Target agent retrieves conversation history
  - Agent processes message through LLM
  - Agent sends response back through message system
  - State is updated with agent's output
- **State Management**: 
  - GraphState maintains conversation history and agent outputs
  - Current_step and next_step guide message flow
  - Agent-specific state fields (research_steps, expert_answer, etc.)
  - Retry counts and limits for error handling
- **Message Passing**: 
  - Orchestrator sends instruction messages to agents
  - Agents send response messages back to orchestrator
  - Critic agents send feedback messages
  - Messages include step_id for research/expert workflows
  - Message types: instruction, response, feedback

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses to avoid actual API calls
  - Mock datetime for consistent timestamps
  - Use real message handling functions
  - Mock external tools (research tools, expert tools)
  - Use real agent factory functions
- **Test Data**: 
  - Sample questions for different complexity levels
  - Mock LLM responses for each agent type
  - Test GraphState configurations
  - Agent configurations with different parameters
- **Environment Setup**: 
  - Initialize GraphState with proper structure
  - Set up mock LLM instances
  - Configure agent factories with mock LLMs
  - Prepare test message data
- **Dependencies**: 
  - Mock ChatOpenAI for LLM responses
  - Mock datetime for timestamps
  - Mock external tools (research, expert)
  - Real message handling functions
- **Graph State Initialization**: 
  - Initialize GraphState with empty agent_messages
  - Set up question and current_step
  - Initialize retry counts and limits
  - Set up agent-specific state fields

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete message flow from orchestrator to planner and back
  - Multi-step research workflow with step-specific messages
  - Expert analysis with research results integration
  - Critic feedback and approval flows
  - Finalizer response generation
- **Component Interaction Patterns**: 
  - Orchestrator-to-planner instruction flow
  - Orchestrator-to-researcher with step_id linking
  - Orchestrator-to-expert with research context
  - Orchestrator-to-critic with decision context
  - Orchestrator-to-finalizer with complete context
- **Data Transformation**: 
  - Question transformation into agent-specific instructions
  - Research steps into researcher instructions
  - Research results into expert context
  - Agent outputs into state updates
  - State data into finalizer context
- **Error Propagation**: 
  - LLM failure handling in agent responses
  - Invalid message composition scenarios
  - State corruption during message flow
  - Retry mechanism integration
- **State Synchronization**: 
  - State updates during message processing
  - Conversation history maintenance
  - Agent output storage and retrieval
  - Step progression and state transitions
- **Message Flow**: 
  - Instruction message composition and delivery
  - Response message processing and storage
  - Feedback message integration
  - Step-specific message linking
  - Multi-agent conversation flows
- **Retry Logic Integration**: 
  - Retry count incrementation during failures
  - Retry limit checking and enforcement
  - Feedback integration in retry scenarios
  - State preservation during retries
- **Critic Decision Integration**: 
  - Critic decision message flow
  - Decision impact on subsequent agent calls
  - Feedback integration in agent instructions
  - Approval/rejection flow handling

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock ChatOpenAI.invoke() for LLM responses
  - Mock datetime for timestamps
  - Use real message handling functions
  - Mock external tools
- **Integration Boundaries**: 
  - Test orchestrator message sending with real message functions
  - Test agent factories with mock LLMs
  - Mock external dependencies (LLM, tools, datetime)
- **Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Sample research and expert tool responses
  - Test GraphState configurations
  - Agent configuration objects
- **Mock Behavior**: 
  - Consistent LLM response patterns
  - Proper error simulation
  - Realistic tool responses
  - Timestamp consistency
- **LLM Mocking**: 
  - Mock ChatOpenAI constructor and invoke method
  - Simulate different response patterns for each agent
  - Mock structured output validation
  - Simulate LLM failures for error testing
- **Tool Mocking**: 
  - Mock research tools (web search, file tools)
  - Mock expert tools (calculator, unit converter)
  - Simulate tool failures and responses

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify orchestrator correctly sends messages to agents
  - Verify agents properly process messages and respond
  - Verify message flow follows expected patterns
  - Verify state updates occur correctly
- **Data Flow Validation**: 
  - Validate message content transformation
  - Verify conversation history maintenance
  - Check agent output storage
  - Validate step progression
- **State Consistency**: 
  - Verify GraphState integrity during message flow
  - Check conversation history consistency
  - Validate agent output storage
  - Verify retry count management
- **Error Handling**: 
  - Verify error propagation through message system
  - Check retry mechanism integration
  - Validate graceful degradation
  - Verify error message composition
- **Message Validation**: 
  - Verify message composition for each agent type
  - Check message content appropriateness
  - Validate step_id linking for research/expert
  - Verify message type selection
- **Graph State Validation**: 
  - Verify state updates during message processing
  - Check conversation history updates
  - Validate agent output storage
  - Verify step progression
- **Performance Metrics**: 
  - Measure message flow performance
  - Check agent response time
  - Validate state update efficiency
  - Measure conversation history performance

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from multi_agent_system import (
    execute_next_step, compose_agent_message, send_message, get_agent_conversation,
    create_planner_agent, create_researcher_agent, create_expert_agent,
    create_critic_agent, create_finalizer_agent, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_message_flow_orchestrator_to_planner(mock_compose_message, mock_send_message, mock_datetime):
    """Test complete message flow from orchestrator to planner agent."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "planner", "type": "instruction",
        "content": "Develop a plan to answer: What is AI?", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (orchestrator sends message to planner)
    result_state = execute_next_step(state)
    
    # Verify orchestrator sent message to planner
    assert result_state["current_step"] == "planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="planner", type="instruction",
        content="Develop a logical plan to answer the following question:\nWhat is AI?"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_message_flow_planner_response_integration(mock_system_message, mock_get_conversation, 
                                                  mock_convert, mock_validate, mock_compose_message, 
                                                  mock_send_message, mock_datetime):
    """Test message flow with planner agent response processing."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "Plan this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"research_steps": ["Research AI basics", "Research AI applications"], "expert_steps": ["Analyze AI definition", "Analyze AI impact"]}
    
    # Mock response message
    mock_response_message = {
        "sender": "planner", "receiver": "orchestrator", "type": "response",
        "content": "Plan created", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create planner agent
    mock_config = AgentConfig("planner", "openai", "gpt-4", 0.7, 
                             {"research_steps": "list", "expert_steps": "list"}, 
                             "You are a planner agent", 3)
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Initialize test state with instruction message
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "planner", "type": "instruction",
            "content": "Develop a plan to answer: What is AI?", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute planner agent
    result_state = planner_agent(state)
    
    # Verify planner processed message and responded
    assert result_state["research_steps"] == ["Research AI basics", "Research AI applications"]
    assert result_state["expert_steps"] == ["Analyze AI definition", "Analyze AI impact"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_message_flow_researcher_step_specific_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test message flow with researcher agent and step-specific messaging."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock step-specific instruction message
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
        "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with research steps
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="researcher",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,  # First research step
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (orchestrator sends step-specific message to researcher)
    result_state = execute_next_step(state)
    
    # Verify orchestrator sent step-specific message to researcher
    assert result_state["current_step"] == "researcher"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="researcher", type="instruction",
        content="Research AI basics"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_message_flow_critic_feedback_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test message flow with critic agent feedback and retry scenarios."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock critic feedback message
    mock_feedback_message = {
        "sender": "critic_planner", "receiver": "orchestrator", "type": "feedback",
        "content": "Plan rejected: Too vague, need specific steps", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_feedback_message
    mock_send_message.return_value = None
    
    # Initialize test state with critic decision
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="critic_planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="reject",
        critic_planner_feedback="Plan is too vague, need more specific steps",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=1,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (orchestrator sends message to critic)
    result_state = execute_next_step(state)
    
    # Verify orchestrator sent message to critic
    assert result_state["current_step"] == "critic_planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="critic_planner", type="instruction",
        content="Review the following plan:\n\nPlan: Research AI basics, analyze applications, provide definition\n\nProvide feedback and decide whether to approve or reject."
    )
    mock_send_message.assert_called_once_with(state, mock_feedback_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_message_flow_finalizer_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test message flow with finalizer agent and complete context."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock finalizer instruction message
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "finalizer", "type": "instruction",
        "content": "Finalize the answer", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with complete context
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="finalizer",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=["AI is machine intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="AI is artificial intelligence that mimics human cognitive functions",
        expert_reasoning="Based on research and analysis of AI concepts",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="approve",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (orchestrator sends message to finalizer)
    result_state = execute_next_step(state)
    
    # Verify orchestrator sent complete context to finalizer
    assert result_state["current_step"] == "finalizer"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="finalizer", type="instruction",
        content="Based on the research and expert analysis, provide a final answer to: What is AI?\n\nResearch Steps: ['Research AI basics', 'Research AI applications']\nResearch Results: ['AI is machine intelligence', 'AI has various applications']\nExpert Answer: AI is artificial intelligence that mimics human cognitive functions\nExpert Reasoning: Based on research and analysis of AI concepts"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_message_flow_error_propagation_integration(mock_datetime):
    """Test error propagation through message flow system."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state with high retry count
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=3,  # At retry limit
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test retry limit checking
    from multi_agent_system import check_retry_limit
    result_state = check_retry_limit(state)
    
    # Verify retry limit exceeded and flow redirected to finalizer
    assert result_state["next_step"] == "finalizer"
    assert result_state["final_answer"] == "The question could not be answered."
    assert "Retry limit exceeded" in result_state["final_reasoning_trace"]

@patch('multi_agent_system.datetime')
def test_message_flow_performance_integration(mock_datetime):
    """Test performance aspects of message flow between agents."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test message flow performance
    import time
    start_time = time.time()
    
    # Simulate multiple message flows
    for i in range(10):
        # Mock execute_next_step for performance testing
        with patch('multi_agent_system.send_message'), patch('multi_agent_system.compose_agent_message'):
            state["next_step"] = "planner"
            state["current_step"] = "planner"
    
    flow_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert flow_time < 1.0  # Should complete in less than 1 second
```
#### 4.1.2 Agent State Synchronization

**Integration Test Information:**
- **Test Name**: Agent State Synchronization
- **Location**: Section 4.1.2
- **Purpose**: Tests how state is synchronized and maintained across different agent types (planner, researcher, expert, critic, finalizer) during multi-step workflows, ensuring consistency and proper state transitions

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `GraphState` - Main state container for the entire system
  - `ResearcherState` - State for researcher subgraph operations
  - `ExpertState` - State for expert subgraph operations
  - `execute_next_step()` - Orchestrator's state transition function
  - `create_researcher_subgraph()` - Researcher subgraph factory
  - `create_expert_subgraph()` - Expert subgraph factory
  - `create_researcher_agent()` - Researcher agent factory
  - `create_expert_agent()` - Expert agent factory
  - `determine_next_step()` - Next step determination logic
  - `check_retry_limit()` - Retry limit checking and state updates
  - Agent message handling functions
- **Integration Points**: 
  - GraphState updates during agent transitions
  - ResearcherState creation and management within GraphState
  - ExpertState creation and management within GraphState
  - State synchronization between main graph and subgraphs
  - Retry count management across state transitions
  - Agent output storage and retrieval from state
- **Data Flow**: 
  - Initial state setup with question and configuration
  - State updates during agent transitions
  - Subgraph state creation and population
  - Agent output storage in main state
  - State consistency validation across transitions
  - Retry count updates and limit enforcement
- **State Management**: 
  - GraphState maintains global system state
  - ResearcherState manages research-specific operations
  - ExpertState manages expert-specific operations
  - State transitions update current_step and next_step
  - Agent outputs stored in appropriate state fields
  - Retry counts tracked per agent type
- **Message Passing**: 
  - State updates trigger message composition
  - Agent responses update state fields
  - State changes propagate through message system
  - Conversation history maintained in state

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses to avoid API calls
  - Mock datetime for consistent timestamps
  - Use real state management functions
  - Mock external tools and services
  - Use real subgraph factory functions
- **Test Data**: 
  - Sample questions requiring multi-step workflows
  - Mock LLM responses for each agent type
  - Test state configurations for different scenarios
  - Agent configurations with various parameters
  - Retry limit configurations
- **Environment Setup**: 
  - Initialize GraphState with proper structure
  - Set up mock LLM instances for each agent
  - Configure subgraph factories with mock components
  - Prepare test state transition scenarios
- **Dependencies**: 
  - Mock ChatOpenAI for LLM responses
  - Mock datetime for timestamps
  - Mock external tools and services
  - Real state management functions
- **Graph State Initialization**: 
  - Initialize GraphState with empty agent_messages
  - Set up question and initial step configuration
  - Initialize retry counts and limits
  - Set up empty agent-specific state fields

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete state synchronization through full workflow
  - Multi-step research workflow with state preservation
  - Expert analysis with research state integration
  - Critic review with state validation
  - Finalizer completion with full state context
- **Component Interaction Patterns**: 
  - State transitions between different agent types
  - Subgraph state creation and management
  - Agent output storage and retrieval
  - State consistency across agent boundaries
  - Retry mechanism state management
- **Data Transformation**: 
  - Question transformation into agent-specific state
  - Research results integration into expert state
  - Agent outputs transformation into main state
  - State data transformation for finalizer context
- **Error Propagation**: 
  - State corruption during agent transitions
  - Retry limit exceeded state handling
  - Invalid state transition scenarios
  - State recovery mechanisms
- **State Synchronization**: 
  - State updates during agent processing
  - Subgraph state synchronization with main state
  - Agent output consistency across state fields
  - State transition validation and enforcement
- **Message Flow**: 
  - State-driven message composition
  - Message-based state updates
  - Conversation history state maintenance
  - State-aware message routing
- **Retry Logic Integration**: 
  - Retry count state management
  - Retry limit state enforcement
  - State preservation during retries
  - Retry failure state handling
- **Critic Decision Integration**: 
  - Critic decision state updates
  - Decision impact on subsequent state transitions
  - State validation during critic review
  - Approval/rejection state handling

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock ChatOpenAI.invoke() for LLM responses
  - Mock datetime for timestamps
  - Use real state management functions
  - Mock external tools and services
- **Integration Boundaries**: 
  - Test state management with real state functions
  - Test subgraph factories with mock LLMs
  - Mock external dependencies (LLM, tools, datetime)
- **Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Sample research and expert tool responses
  - Test state configurations for different scenarios
  - Agent configuration objects
- **Mock Behavior**: 
  - Consistent LLM response patterns
  - Proper error simulation
  - Realistic tool responses
  - Timestamp consistency
- **LLM Mocking**: 
  - Mock ChatOpenAI constructor and invoke method
  - Simulate different response patterns for each agent
  - Mock structured output validation
  - Simulate LLM failures for error testing
- **Tool Mocking**: 
  - Mock research tools (web search, file tools)
  - Mock expert tools (calculator, unit converter)
  - Simulate tool failures and responses

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify state transitions occur correctly
  - Verify subgraph state creation and management
  - Verify agent output storage and retrieval
  - Verify state consistency across components
- **Data Flow Validation**: 
  - Validate state data transformation
  - Verify agent output integration
  - Check state field updates
  - Validate state transition logic
- **State Consistency**: 
  - Verify GraphState integrity during transitions
  - Check subgraph state consistency
  - Validate agent output storage
  - Verify retry count management
- **Error Handling**: 
  - Verify error state propagation
  - Check retry mechanism integration
  - Validate state recovery
  - Verify error state handling
- **Message Validation**: 
  - Verify state-driven message composition
  - Check message-based state updates
  - Validate conversation history maintenance
  - Verify state-aware message routing
- **Graph State Validation**: 
  - Verify state updates during transitions
  - Check subgraph state synchronization
  - Validate agent output storage
  - Verify state transition enforcement
- **Performance Metrics**: 
  - Measure state transition performance
  - Check subgraph state management efficiency
  - Validate state update performance
  - Measure state consistency validation

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from multi_agent_system import (
    execute_next_step, determine_next_step, check_retry_limit,
    create_researcher_subgraph, create_expert_subgraph,
    create_researcher_agent, create_expert_agent,
    GraphState, ResearcherState, ExpertState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_state_synchronization_planner_to_researcher(mock_compose_message, mock_send_message, mock_datetime):
    """Test state synchronization during transition from planner to researcher."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
        "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with planner output
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="researcher",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=0,  # First research step
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (transition to researcher)
    result_state = execute_next_step(state)
    
    # Verify state synchronization
    assert result_state["current_step"] == "researcher"
    assert result_state["research_steps"] == ["Research AI basics", "Research AI applications"]
    assert result_state["current_research_index"] == 0
    assert result_state["critic_planner_decision"] == "approve"
    assert len(result_state["agent_messages"]) == 1  # New instruction message
    
    # Verify message composition with state context
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="researcher", type="instruction",
        content="Research AI basics"
    )

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_state_synchronization_researcher_subgraph_integration(mock_system_message, mock_get_conversation, 
                                                              mock_convert, mock_validate, mock_compose_message, 
                                                              mock_send_message, mock_datetime):
    """Test state synchronization with researcher subgraph creation and management."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "AI is artificial intelligence that mimics human cognitive functions"}
    
    # Mock response message
    mock_response_message = {
        "sender": "researcher", "receiver": "orchestrator", "type": "response",
        "content": "Research completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create researcher agent
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    researcher_agent = create_researcher_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
            "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute researcher agent
    result_state = researcher_agent(state)
    
    # Verify state synchronization
    assert result_state["research_results"] == ["AI is artificial intelligence that mimics human cognitive functions"]
    assert result_state["current_research_index"] == 1  # Moved to next research step
    assert len(result_state["agent_messages"]) == 2  # Original + response
    assert result_state["researcher_states"] == {}  # Subgraph state managed internally

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_state_synchronization_expert_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test state synchronization with expert agent and research results integration."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock expert instruction message
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "expert", "type": "instruction",
        "content": "Analyze AI definition", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with research results
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="expert",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (transition to expert)
    result_state = execute_next_step(state)
    
    # Verify state synchronization with research context
    assert result_state["current_step"] == "expert"
    assert result_state["research_results"] == ["AI is artificial intelligence", "AI has various applications"]
    assert result_state["expert_steps"] == ["Analyze AI definition", "Analyze AI impact"]
    assert result_state["critic_researcher_decision"] == "approve"
    
    # Verify message composition with research context
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="expert", type="instruction",
        content="Analyze AI definition"
    )

@patch('multi_agent_system.datetime')
def test_state_synchronization_retry_logic_integration(mock_datetime):
    """Test state synchronization with retry logic and limit enforcement."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state with high retry count
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=3,  # At retry limit
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test retry limit checking
    result_state = check_retry_limit(state)
    
    # Verify retry limit state synchronization
    assert result_state["next_step"] == "finalizer"
    assert result_state["final_answer"] == "The question could not be answered."
    assert "Retry limit exceeded" in result_state["final_reasoning_trace"]
    assert result_state["planner_retry_count"] == 3  # Unchanged

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_state_synchronization_critic_decision_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test state synchronization with critic decision integration."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock critic instruction message
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "critic_planner", "type": "instruction",
        "content": "Review the plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with plan for critic review
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="critic_planner",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (transition to critic)
    result_state = execute_next_step(state)
    
    # Verify state synchronization for critic review
    assert result_state["current_step"] == "critic_planner"
    assert result_state["research_steps"] == ["Research AI basics", "Research AI applications"]
    assert result_state["expert_steps"] == ["Analyze AI definition", "Analyze AI impact"]
    
    # Verify message composition with plan context
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="critic_planner", type="instruction",
        content="Review the following plan:\n\nPlan: Research AI basics, Research AI applications, Analyze AI definition, Analyze AI impact\n\nProvide feedback and decide whether to approve or reject."
    )

@patch('multi_agent_system.datetime')
def test_state_synchronization_performance_integration(mock_datetime):
    """Test performance aspects of state synchronization across agents."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test state synchronization performance
    import time
    start_time = time.time()
    
    # Simulate multiple state transitions
    for i in range(10):
        # Mock execute_next_step for performance testing
        with patch('multi_agent_system.send_message'), patch('multi_agent_system.compose_agent_message'):
            state["next_step"] = "planner"
            state["current_step"] = "planner"
            state["research_steps"] = ["Research step " + str(i)]
            state["expert_steps"] = ["Expert step " + str(i)]
    
    sync_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert sync_time < 1.0  # Should complete in less than 1 second
```
#### 4.1.3 Communication Protocol Validation

**Integration Test Information:**
- **Test Name**: Communication Protocol Validation
- **Location**: Section 4.1.3
- **Purpose**: Tests the AgentMessage protocol implementation across all agent types, validating message composition, transmission, retrieval, and conversion between different message formats in the multi-agent system

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `AgentMessage` - TypedDict for inter-agent communication
  - `compose_agent_message()` - Message composition function
  - `send_message()` - Message sending and storage function
  - `get_agent_conversation()` - Message retrieval function
  - `convert_agent_messages_to_langchain()` - Message format conversion
  - `execute_next_step()` - Orchestrator's message sending
  - `create_planner_agent()` - Planner agent factory
  - `create_researcher_agent()` - Researcher agent factory
  - `create_expert_agent()` - Expert agent factory
  - `create_critic_agent()` - Critic agent factory
  - `create_finalizer_agent()` - Finalizer agent factory
  - `GraphState` - State container for message storage
- **Integration Points**: 
  - Message composition and validation across all agent types
  - Message storage and retrieval from GraphState
  - Message format conversion between AgentMessage and LangChain formats
  - Message flow between orchestrator and different agent types
  - Conversation history management and filtering
  - Step-specific message linking and retrieval
- **Data Flow**: 
  - Orchestrator composes messages for different agent types
  - Messages are stored in GraphState agent_messages list
  - Agents retrieve conversation history using filters
  - Messages are converted to LangChain format for LLM processing
  - Agent responses are stored back in GraphState
  - Conversation history is maintained and updated
- **State Management**: 
  - GraphState maintains agent_messages list
  - Message timestamps and metadata are preserved
  - Conversation history is indexed and searchable
  - Step-specific messages are linked via step_id
  - Message types (instruction, response, feedback) are tracked
- **Message Passing**: 
  - Orchestrator sends instruction messages to agents
  - Agents send response messages back to orchestrator
  - Critic agents send feedback messages
  - Messages include sender, receiver, type, content, and timestamp
  - Step-specific messages include step_id for research/expert workflows

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock datetime for consistent timestamps
  - Mock LLM responses to avoid API calls
  - Use real message handling functions
  - Mock external tools and services
  - Use real agent factory functions
- **Test Data**: 
  - Sample messages for each agent type and message type
  - Test conversation histories with various patterns
  - Step-specific message data for research/expert workflows
  - Agent configuration objects
  - Mock LLM responses for each agent
- **Environment Setup**: 
  - Initialize GraphState with proper structure
  - Set up mock LLM instances for each agent
  - Configure agent factories with mock components
  - Prepare test message scenarios
- **Dependencies**: 
  - Mock datetime for timestamps
  - Mock ChatOpenAI for LLM responses
  - Mock external tools and services
  - Real message handling functions
- **Graph State Initialization**: 
  - Initialize GraphState with empty agent_messages
  - Set up question and step configuration
  - Initialize retry counts and limits
  - Set up agent-specific state fields

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete message protocol flow for all agent types
  - Multi-step conversation with step-specific messages
  - Message format conversion and validation
  - Conversation history management and retrieval
  - Message composition for different scenarios
- **Component Interaction Patterns**: 
  - Orchestrator-to-agent message composition patterns
  - Agent-to-orchestrator response patterns
  - Critic feedback message patterns
  - Step-specific message linking patterns
  - Conversation filtering and retrieval patterns
- **Data Transformation**: 
  - AgentMessage to LangChain message conversion
  - Message content transformation for different agents
  - Timestamp and metadata preservation
  - Step_id linking and retrieval
  - Message type validation and routing
- **Error Propagation**: 
  - Invalid message composition scenarios
  - Message format conversion errors
  - Conversation retrieval errors
  - Step-specific message linking errors
  - Message validation failures
- **State Synchronization**: 
  - Message storage and retrieval consistency
  - Conversation history updates
  - Step-specific message management
  - Message metadata preservation
  - State-driven message composition
- **Message Flow**: 
  - Instruction message flow patterns
  - Response message flow patterns
  - Feedback message flow patterns
  - Step-specific message flow patterns
  - Multi-agent conversation flows
- **Retry Logic Integration**: 
  - Message composition during retry scenarios
  - Conversation history during retries
  - Step-specific message handling during retries
  - Message validation during retry flows
- **Critic Decision Integration**: 
  - Critic feedback message composition
  - Decision message flow patterns
  - Feedback integration in subsequent messages
  - Approval/rejection message handling

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock datetime for timestamps
  - Mock ChatOpenAI.invoke() for LLM responses
  - Use real message handling functions
  - Mock external tools and services
- **Integration Boundaries**: 
  - Test message protocol with real message functions
  - Test agent factories with mock LLMs
  - Mock external dependencies (LLM, tools, datetime)
- **Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Sample conversation histories
  - Test message data for different scenarios
  - Agent configuration objects
- **Mock Behavior**: 
  - Consistent timestamp generation
  - Proper LLM response patterns
  - Realistic tool responses
  - Message validation behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI constructor and invoke method
  - Simulate different response patterns for each agent
  - Mock structured output validation
  - Simulate LLM failures for error testing
- **Tool Mocking**: 
  - Mock research tools (web search, file tools)
  - Mock expert tools (calculator, unit converter)
  - Simulate tool failures and responses

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify message composition for all agent types
  - Verify message storage and retrieval
  - Verify message format conversion
  - Verify conversation history management
- **Data Flow Validation**: 
  - Validate message content transformation
  - Verify timestamp and metadata preservation
  - Check step-specific message linking
  - Validate message type routing
- **State Consistency**: 
  - Verify message storage consistency
  - Check conversation history integrity
  - Validate step-specific message management
  - Verify message metadata consistency
- **Error Handling**: 
  - Verify message validation errors
  - Check format conversion errors
  - Validate conversation retrieval errors
  - Verify error message composition
- **Message Validation**: 
  - Verify message structure compliance
  - Check message type validation
  - Validate sender/receiver relationships
  - Verify step_id linking for research/expert
- **Graph State Validation**: 
  - Verify message storage in GraphState
  - Check conversation history updates
  - Validate step-specific message management
  - Verify message-driven state updates
- **Performance Metrics**: 
  - Measure message composition performance
  - Check conversation retrieval efficiency
  - Validate format conversion performance
  - Measure message storage efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from multi_agent_system import (
    compose_agent_message, send_message, get_agent_conversation,
    convert_agent_messages_to_langchain, execute_next_step,
    create_planner_agent, create_researcher_agent, create_expert_agent,
    create_critic_agent, create_finalizer_agent, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

@patch('multi_agent_system.datetime')
def test_communication_protocol_message_composition_integration(mock_datetime):
    """Test message composition protocol across all agent types."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test message composition for each agent type
    agent_types = ["planner", "researcher", "expert", "critic_planner", "critic_researcher", "critic_expert", "finalizer"]
    message_types = ["instruction", "response", "feedback"]
    
    for agent_type in agent_types:
        for message_type in message_types:
            # Compose message
            message = compose_agent_message(
                sender="orchestrator" if message_type == "instruction" else agent_type,
                receiver=agent_type if message_type == "instruction" else "orchestrator",
                message_type=message_type,
                content=f"Test {message_type} for {agent_type}",
                step_id=1 if agent_type in ["researcher", "expert"] else None
            )
            
            # Verify message structure
            assert message["timestamp"] == "2024-01-01T12:00:00"
            assert message["sender"] in ["orchestrator", agent_type]
            assert message["receiver"] in ["orchestrator", agent_type]
            assert message["type"] == message_type
            assert message["content"] == f"Test {message_type} for {agent_type}"
            
            # Verify step_id for research/expert agents
            if agent_type in ["researcher", "expert"]:
                assert message["step_id"] == 1
            else:
                assert message["step_id"] is None

@patch('multi_agent_system.datetime')
def test_communication_protocol_message_storage_integration(mock_datetime):
    """Test message storage and retrieval protocol in GraphState."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Create and send multiple messages
    messages = [
        compose_agent_message("orchestrator", "planner", "instruction", "Plan this", None),
        compose_agent_message("planner", "orchestrator", "response", "Plan created", None),
        compose_agent_message("orchestrator", "researcher", "instruction", "Research this", 1),
        compose_agent_message("researcher", "orchestrator", "response", "Research done", 1)
    ]
    
    # Send messages to state
    for message in messages:
        state = send_message(state, message)
    
    # Verify message storage
    assert len(state["agent_messages"]) == 4
    assert state["agent_messages"][0]["sender"] == "orchestrator"
    assert state["agent_messages"][0]["receiver"] == "planner"
    assert state["agent_messages"][1]["sender"] == "planner"
    assert state["agent_messages"][1]["receiver"] == "orchestrator"
    
    # Test message retrieval
    planner_conversation = get_agent_conversation(state, "planner")
    assert len(planner_conversation) == 2
    
    researcher_conversation = get_agent_conversation(state, "researcher")
    assert len(researcher_conversation) == 2
    
    # Test step-specific retrieval
    step_1_messages = get_agent_conversation(state, "researcher", step_id=1)
    assert len(step_1_messages) == 2
    assert all(msg["step_id"] == 1 for msg in step_1_messages)

@patch('multi_agent_system.datetime')
def test_communication_protocol_format_conversion_integration(mock_datetime):
    """Test message format conversion between AgentMessage and LangChain formats."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Create test AgentMessages
    agent_messages = [
        compose_agent_message("orchestrator", "planner", "instruction", "Develop a plan", None),
        compose_agent_message("planner", "orchestrator", "response", "Plan: Research, analyze, conclude", None),
        compose_agent_message("orchestrator", "researcher", "instruction", "Research AI basics", 1),
        compose_agent_message("researcher", "orchestrator", "response", "AI is artificial intelligence", 1)
    ]
    
    # Convert to LangChain format
    langchain_messages = convert_agent_messages_to_langchain(agent_messages)
    
    # Verify conversion
    assert len(langchain_messages) == 4
    assert isinstance(langchain_messages[0], HumanMessage)
    assert isinstance(langchain_messages[1], AIMessage)
    assert isinstance(langchain_messages[2], HumanMessage)
    assert isinstance(langchain_messages[3], AIMessage)
    
    # Verify content preservation
    assert "Develop a plan" in langchain_messages[0].content
    assert "Plan: Research, analyze, conclude" in langchain_messages[1].content
    assert "Research AI basics" in langchain_messages[2].content
    assert "AI is artificial intelligence" in langchain_messages[3].content

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_communication_protocol_orchestrator_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test orchestrator's message sending protocol integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "planner", "type": "instruction",
        "content": "Develop a plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (orchestrator sends message)
    result_state = execute_next_step(state)
    
    # Verify orchestrator message protocol
    assert result_state["current_step"] == "planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="planner", type="instruction",
        content="Develop a logical plan to answer the following question:\nWhat is AI?"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_communication_protocol_agent_response_integration(mock_system_message, mock_get_conversation, 
                                                          mock_convert, mock_validate, mock_compose_message, 
                                                          mock_send_message, mock_datetime):
    """Test agent response protocol integration."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "Plan this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"research_steps": ["Research AI basics"], "expert_steps": ["Analyze AI"]}
    
    # Mock response message
    mock_response_message = {
        "sender": "planner", "receiver": "orchestrator", "type": "response",
        "content": "Plan created", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create planner agent
    mock_config = AgentConfig("planner", "openai", "gpt-4", 0.7, 
                             {"research_steps": "list", "expert_steps": "list"}, 
                             "You are a planner agent", 3)
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Initialize test state with instruction message
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "planner", "type": "instruction",
            "content": "Develop a plan", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute planner agent
    result_state = planner_agent(state)
    
    # Verify agent response protocol
    assert result_state["research_steps"] == ["Research AI basics"]
    assert result_state["expert_steps"] == ["Analyze AI"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
def test_communication_protocol_error_handling_integration(mock_datetime):
    """Test communication protocol error handling and validation."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test invalid message composition
    with pytest.raises(KeyError):
        # Missing required fields
        invalid_message = {
            "sender": "orchestrator",
            "receiver": "planner"
            # Missing type, content, timestamp
        }
    
    # Test message retrieval with invalid agent
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test retrieval for non-existent agent
    empty_conversation = get_agent_conversation(state, "non_existent_agent")
    assert len(empty_conversation) == 0
    
    # Test step-specific retrieval with invalid step_id
    step_messages = get_agent_conversation(state, "researcher", step_id=999)
    assert len(step_messages) == 0

@patch('multi_agent_system.datetime')
def test_communication_protocol_performance_integration(mock_datetime):
    """Test performance aspects of communication protocol."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test communication protocol performance
    import time
    start_time = time.time()
    
    # Simulate multiple message operations
    for i in range(100):
        # Compose message
        message = compose_agent_message(
            sender="orchestrator", receiver="planner", 
            message_type="instruction", content=f"Message {i}", step_id=None
        )
        
        # Send message
        state = send_message(state, message)
        
        # Retrieve conversation
        conversation = get_agent_conversation(state, "planner")
        
        # Convert to LangChain format
        langchain_messages = convert_agent_messages_to_langchain(conversation)
    
    protocol_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert protocol_time < 5.0  # Should complete in less than 5 seconds
    assert len(state["agent_messages"]) == 100
```
### 4.2 Coordination Mechanism Integration
#### 4.2.1 Task Allocation and Routing

**Integration Test Information:**
- **Test Name**: Task Allocation and Routing
- **Location**: Section 4.2.1
- **Purpose**: Tests how the orchestrator allocates tasks to different agents and routes workflow execution through the multi-agent system, including step determination, agent selection, and workflow progression

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `determine_next_step()` - Next step determination logic
  - `execute_next_step()` - Task execution and routing function
  - `route_from_orchestrator()` - Orchestrator routing logic
  - `check_retry_limit()` - Retry limit checking and routing
  - `orchestrator()` - Main orchestrator function
  - `create_multi_agent_graph()` - Graph creation with routing
  - `create_input_interface()` - Input interface with routing
  - Agent factory functions for task allocation
  - `GraphState` - State container for routing decisions
- **Integration Points**: 
  - Orchestrator's task allocation decisions
  - Step determination based on current state
  - Agent selection and routing logic
  - Workflow progression and state transitions
  - Retry mechanism integration with routing
  - Critic decision impact on routing
  - Graph state updates during routing
- **Data Flow**: 
  - Input question triggers initial routing
  - State analysis determines next step
  - Task allocation to appropriate agent
  - Agent execution and response handling
  - State updates drive next routing decision
  - Workflow progression through different agents
  - Final routing to completion
- **State Management**: 
  - GraphState tracks current_step and next_step
  - Agent outputs stored for routing decisions
  - Retry counts influence routing logic
  - Critic decisions affect routing paths
  - Workflow state maintained across transitions
- **Message Passing**: 
  - Routing decisions trigger message composition
  - Task allocation sends instructions to agents
  - Agent responses influence next routing
  - Critic feedback affects routing decisions
  - Workflow state communicated through messages

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses to avoid API calls
  - Mock datetime for consistent timestamps
  - Use real routing and allocation functions
  - Mock external tools and services
  - Use real agent factory functions
- **Test Data**: 
  - Sample questions requiring different workflows
  - Mock LLM responses for each agent type
  - Test state configurations for routing scenarios
  - Agent configurations with various parameters
  - Retry limit configurations
- **Environment Setup**: 
  - Initialize GraphState with proper structure
  - Set up mock LLM instances for each agent
  - Configure routing functions with mock components
  - Prepare test routing scenarios
- **Dependencies**: 
  - Mock ChatOpenAI for LLM responses
  - Mock datetime for timestamps
  - Mock external tools and services
  - Real routing and allocation functions
- **Graph State Initialization**: 
  - Initialize GraphState with empty agent_messages
  - Set up question and initial step configuration
  - Initialize retry counts and limits
  - Set up agent-specific state fields

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete workflow routing from input to completion
  - Multi-step task allocation and execution
  - Agent selection based on workflow requirements
  - State-driven routing decisions
  - Workflow progression through all agent types
- **Component Interaction Patterns**: 
  - Orchestrator-to-agent task allocation patterns
  - Step determination based on agent outputs
  - Routing logic integration with state management
  - Agent response handling and next step routing
  - Critic decision integration with routing
- **Data Transformation**: 
  - Question transformation into routing decisions
  - Agent output transformation into next step logic
  - State data transformation for routing decisions
  - Workflow state transformation across agents
- **Error Propagation**: 
  - Retry limit exceeded routing scenarios
  - Agent failure routing and recovery
  - Invalid state routing scenarios
  - Error state propagation through routing
- **State Synchronization**: 
  - State updates during routing decisions
  - Agent output synchronization with routing
  - Workflow state consistency across routing
  - State-driven routing validation
- **Message Flow**: 
  - Routing-driven message composition
  - Task allocation message flow patterns
  - Agent response routing patterns
  - Workflow progression message flows
- **Retry Logic Integration**: 
  - Retry count routing decisions
  - Retry limit routing enforcement
  - Retry failure routing scenarios
  - Retry state routing management
- **Critic Decision Integration**: 
  - Critic decision routing impact
  - Approval/rejection routing scenarios
  - Feedback integration in routing decisions
  - Critic-driven workflow routing

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock ChatOpenAI.invoke() for LLM responses
  - Mock datetime for timestamps
  - Use real routing and allocation functions
  - Mock external tools and services
- **Integration Boundaries**: 
  - Test routing logic with real routing functions
  - Test agent factories with mock LLMs
  - Mock external dependencies (LLM, tools, datetime)
- **Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Sample routing scenarios and state configurations
  - Test agent configurations
  - Mock workflow progression data
- **Mock Behavior**: 
  - Consistent LLM response patterns
  - Proper error simulation
  - Realistic tool responses
  - Routing decision simulation
- **LLM Mocking**: 
  - Mock ChatOpenAI constructor and invoke method
  - Simulate different response patterns for each agent
  - Mock structured output validation
  - Simulate LLM failures for error testing
- **Tool Mocking**: 
  - Mock research tools (web search, file tools)
  - Mock expert tools (calculator, unit converter)
  - Simulate tool failures and responses

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify task allocation to correct agents
  - Verify routing decisions based on state
  - Verify workflow progression through agents
  - Verify state-driven routing logic
- **Data Flow Validation**: 
  - Validate routing decision data flow
  - Verify agent output integration with routing
  - Check workflow state transformation
  - Validate routing state updates
- **State Consistency**: 
  - Verify routing state consistency
  - Check agent output synchronization
  - Validate workflow state integrity
  - Verify routing decision consistency
- **Error Handling**: 
  - Verify error routing propagation
  - Check retry mechanism routing integration
  - Validate error state routing
  - Verify routing error recovery
- **Message Validation**: 
  - Verify routing-driven message composition
  - Check task allocation message flow
  - Validate agent response routing
  - Verify workflow message routing
- **Graph State Validation**: 
  - Verify routing state updates
  - Check agent output storage during routing
  - Validate workflow state management
  - Verify routing decision storage
- **Performance Metrics**: 
  - Measure routing decision performance
  - Check task allocation efficiency
  - Validate workflow progression performance
  - Measure routing state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from multi_agent_system import (
    determine_next_step, execute_next_step, route_from_orchestrator,
    check_retry_limit, orchestrator, create_multi_agent_graph,
    create_input_interface, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_task_allocation_initial_routing_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test initial task allocation and routing from input to planner."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "planner", "type": "instruction",
        "content": "Develop a plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with input question
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="input",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (initial routing to planner)
    result_state = execute_next_step(state)
    
    # Verify initial task allocation and routing
    assert result_state["current_step"] == "planner"
    assert result_state["next_step"] == "planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="planner", type="instruction",
        content="Develop a logical plan to answer the following question:\nWhat is AI?"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_task_allocation_step_determination_integration(mock_datetime):
    """Test step determination logic for task allocation and routing."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test step determination for different scenarios
    
    # Scenario 1: Planner completed, route to researcher
    state_1 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_1 = determine_next_step(state_1)
    assert result_1["next_step"] == "researcher"
    assert result_1["current_research_index"] == 0
    
    # Scenario 2: Research completed, route to expert
    state_2 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,  # All research steps completed
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_2 = determine_next_step(state_2)
    assert result_2["next_step"] == "expert"
    
    # Scenario 3: Expert completed, route to finalizer
    state_3 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="expert",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="AI is artificial intelligence that mimics human cognitive functions",
        expert_reasoning="Based on research and analysis",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="approve",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_3 = determine_next_step(state_3)
    assert result_3["next_step"] == "finalizer"

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_task_allocation_critic_routing_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test task allocation and routing with critic decision integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "critic_planner", "type": "instruction",
        "content": "Review the plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with plan for critic review
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="critic_planner",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (routing to critic)
    result_state = execute_next_step(state)
    
    # Verify critic routing and task allocation
    assert result_state["current_step"] == "critic_planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="critic_planner", type="instruction",
        content="Review the following plan:\n\nPlan: Research AI basics, Research AI applications, Analyze AI definition, Analyze AI impact\n\nProvide feedback and decide whether to approve or reject."
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_task_allocation_retry_routing_integration(mock_datetime):
    """Test task allocation and routing with retry logic integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test retry routing scenarios
    
    # Scenario 1: Retry limit not exceeded, continue routing
    state_1 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=2,  # Below retry limit
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_1 = check_retry_limit(state_1)
    assert result_1["next_step"] == "planner"  # Continue routing
    
    # Scenario 2: Retry limit exceeded, route to finalizer
    state_2 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=3,  # At retry limit
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_2 = check_retry_limit(state_2)
    assert result_2["next_step"] == "finalizer"  # Route to finalizer
    assert result_2["final_answer"] == "The question could not be answered."

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_task_allocation_workflow_progression_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test complete workflow progression through task allocation and routing."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
        "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with research workflow
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="researcher",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=0,  # First research step
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (routing to researcher)
    result_state = execute_next_step(state)
    
    # Verify workflow progression routing
    assert result_state["current_step"] == "researcher"
    assert result_state["current_research_index"] == 0
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="researcher", type="instruction",
        content="Research AI basics"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_task_allocation_performance_integration(mock_datetime):
    """Test performance aspects of task allocation and routing."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test routing performance
    import time
    start_time = time.time()
    
    # Simulate multiple routing decisions
    for i in range(100):
        # Mock determine_next_step for performance testing
        with patch('multi_agent_system.send_message'), patch('multi_agent_system.compose_agent_message'):
            state["next_step"] = "planner"
            state["current_step"] = "planner"
            state["research_steps"] = ["Research step " + str(i)]
            state["expert_steps"] = ["Expert step " + str(i)]
    
    routing_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert routing_time < 1.0  # Should complete in less than 1 second
```
#### 4.2.2 Agent Workflow Coordination

**Integration Test Information:**
- **Test Name**: Agent Workflow Coordination
- **Location**: Section 4.2.2
- **Purpose**: Tests how multiple agents coordinate and collaborate during complex workflows, including research-expert coordination, critic review integration, and multi-step workflow orchestration across the entire agent system

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `orchestrator()` - Main orchestrator function for workflow coordination
  - `create_multi_agent_graph()` - Graph creation with workflow coordination
  - `create_researcher_subgraph()` - Researcher subgraph for research workflows
  - `create_expert_subgraph()` - Expert subgraph for expert workflows
  - `create_planner_agent()` - Planner agent for workflow planning
  - `create_researcher_agent()` - Researcher agent for research coordination
  - `create_expert_agent()` - Expert agent for expert coordination
  - `create_critic_agent()` - Critic agent for review coordination
  - `create_finalizer_agent()` - Finalizer agent for completion coordination
  - `determine_next_step()` - Next step determination for workflow progression
  - `execute_next_step()` - Step execution for workflow coordination
  - `GraphState` - State container for workflow coordination
- **Integration Points**: 
  - Multi-agent workflow orchestration and coordination
  - Research-expert workflow coordination and handoff
  - Critic review integration in workflow progression
  - Subgraph coordination and state management
  - Workflow state transitions and progression
  - Agent output integration and coordination
  - Workflow completion and finalization
- **Data Flow**: 
  - Initial workflow planning and coordination
  - Research workflow execution and coordination
  - Research results handoff to expert workflow
  - Expert analysis coordination and integration
  - Critic review coordination and feedback integration
  - Workflow state coordination across agents
  - Final workflow completion and coordination
- **State Management**: 
  - GraphState coordinates workflow state across agents
  - Research state coordination and management
  - Expert state coordination and integration
  - Critic decision coordination and impact
  - Workflow progression state coordination
  - Agent output coordination and storage
- **Message Passing**: 
  - Workflow coordination through message passing
  - Agent-to-agent coordination messages
  - Workflow state coordination messages
  - Critic feedback coordination messages
  - Workflow completion coordination messages

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses to avoid API calls
  - Mock datetime for consistent timestamps
  - Use real workflow coordination functions
  - Mock external tools and services
  - Use real agent factory functions
- **Test Data**: 
  - Sample complex questions requiring multi-agent coordination
  - Mock LLM responses for each agent type
  - Test workflow coordination scenarios
  - Agent configurations with coordination parameters
  - Workflow state configurations
- **Environment Setup**: 
  - Initialize GraphState with proper workflow structure
  - Set up mock LLM instances for each agent
  - Configure workflow coordination functions
  - Prepare test coordination scenarios
- **Dependencies**: 
  - Mock ChatOpenAI for LLM responses
  - Mock datetime for timestamps
  - Mock external tools and services
  - Real workflow coordination functions
- **Graph State Initialization**: 
  - Initialize GraphState with workflow coordination structure
  - Set up question and workflow configuration
  - Initialize coordination state fields
  - Set up agent-specific coordination fields

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete multi-agent workflow coordination
  - Research-expert workflow handoff and coordination
  - Critic review workflow integration
  - Workflow completion and finalization
  - Multi-step coordination across all agents
- **Component Interaction Patterns**: 
  - Planner-researcher coordination patterns
  - Researcher-expert coordination patterns
  - Critic-agent coordination patterns
  - Finalizer coordination patterns
  - Subgraph coordination patterns
- **Data Transformation**: 
  - Workflow state transformation across agents
  - Research results transformation for expert coordination
  - Expert output transformation for critic coordination
  - Critic feedback transformation for workflow coordination
  - Final output transformation and coordination
- **Error Propagation**: 
  - Workflow coordination error scenarios
  - Agent failure coordination and recovery
  - Subgraph coordination error handling
  - Workflow state corruption coordination
  - Error propagation through coordination system
- **State Synchronization**: 
  - Workflow state synchronization across agents
  - Research-expert state coordination
  - Critic decision state coordination
  - Workflow progression state coordination
  - Final state coordination and synchronization
- **Message Flow**: 
  - Workflow coordination message flows
  - Agent-to-agent coordination messages
  - Workflow state coordination messages
  - Critic feedback coordination messages
  - Workflow completion coordination messages
- **Retry Logic Integration**: 
  - Workflow coordination during retry scenarios
  - Agent retry coordination and state management
  - Subgraph retry coordination
  - Workflow retry state coordination
  - Retry failure coordination handling
- **Critic Decision Integration**: 
  - Critic decision workflow coordination
  - Approval/rejection workflow coordination
  - Feedback integration in workflow coordination
  - Critic-driven workflow coordination
  - Decision impact on workflow coordination

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock ChatOpenAI.invoke() for LLM responses
  - Mock datetime for timestamps
  - Use real workflow coordination functions
  - Mock external tools and services
- **Integration Boundaries**: 
  - Test workflow coordination with real coordination functions
  - Test agent factories with mock LLMs
  - Mock external dependencies (LLM, tools, datetime)
- **Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Sample workflow coordination scenarios
  - Test agent configurations
  - Mock workflow progression data
- **Mock Behavior**: 
  - Consistent LLM response patterns
  - Proper error simulation
  - Realistic tool responses
  - Workflow coordination simulation
- **LLM Mocking**: 
  - Mock ChatOpenAI constructor and invoke method
  - Simulate different response patterns for each agent
  - Mock structured output validation
  - Simulate LLM failures for error testing
- **Tool Mocking**: 
  - Mock research tools (web search, file tools)
  - Mock expert tools (calculator, unit converter)
  - Simulate tool failures and responses

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify multi-agent workflow coordination
  - Verify research-expert workflow handoff
  - Verify critic review workflow integration
  - Verify workflow completion coordination
- **Data Flow Validation**: 
  - Validate workflow state coordination
  - Verify agent output coordination
  - Check workflow progression coordination
  - Validate coordination state updates
- **State Consistency**: 
  - Verify workflow state consistency
  - Check agent coordination state
  - Validate workflow progression consistency
  - Verify coordination state integrity
- **Error Handling**: 
  - Verify workflow coordination error propagation
  - Check agent failure coordination
  - Validate subgraph coordination error handling
  - Verify coordination error recovery
- **Message Validation**: 
  - Verify workflow coordination messages
  - Check agent-to-agent coordination messages
  - Validate workflow state coordination messages
  - Verify coordination message flows
- **Graph State Validation**: 
  - Verify workflow coordination state updates
  - Check agent coordination state storage
  - Validate workflow progression state management
  - Verify coordination state transitions
- **Performance Metrics**: 
  - Measure workflow coordination performance
  - Check agent coordination efficiency
  - Validate workflow progression performance
  - Measure coordination state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from multi_agent_system import (
    orchestrator, create_multi_agent_graph, create_researcher_subgraph,
    create_expert_subgraph, create_planner_agent, create_researcher_agent,
    create_expert_agent, create_critic_agent, create_finalizer_agent,
    determine_next_step, execute_next_step, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_workflow_coordination_planner_researcher_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test workflow coordination between planner and researcher agents."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
        "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with planner output
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="researcher",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=0,  # First research step
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (coordination to researcher)
    result_state = execute_next_step(state)
    
    # Verify planner-researcher workflow coordination
    assert result_state["current_step"] == "researcher"
    assert result_state["research_steps"] == ["Research AI basics", "Research AI applications"]
    assert result_state["current_research_index"] == 0
    assert result_state["critic_planner_decision"] == "approve"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="researcher", type="instruction",
        content="Research AI basics"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_workflow_coordination_researcher_expert_integration(mock_system_message, mock_get_conversation, 
                                                           mock_convert, mock_validate, mock_compose_message, 
                                                           mock_send_message, mock_datetime):
    """Test workflow coordination between researcher and expert agents."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "AI is artificial intelligence", "expert_reasoning": "Based on research analysis"}
    
    # Mock response message
    mock_response_message = {
        "sender": "expert", "receiver": "orchestrator", "type": "response",
        "content": "Analysis completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create expert agent
    mock_config = AgentConfig("expert", "openai", "gpt-4", 0.7, 
                             {"expert_answer": "string", "expert_reasoning": "string"}, 
                             "You are an expert agent", 5)
    expert_agent = create_expert_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state with research results
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "expert", "type": "instruction",
            "content": "Analyze AI definition", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="expert",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,  # All research completed
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute expert agent (researcher-expert coordination)
    result_state = expert_agent(state)
    
    # Verify researcher-expert workflow coordination
    assert result_state["expert_answer"] == "AI is artificial intelligence"
    assert result_state["expert_reasoning"] == "Based on research analysis"
    assert result_state["research_results"] == ["AI is artificial intelligence", "AI has various applications"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_workflow_coordination_critic_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test workflow coordination with critic agent integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "critic_planner", "type": "instruction",
        "content": "Review the plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with plan for critic review
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="critic_planner",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (coordination to critic)
    result_state = execute_next_step(state)
    
    # Verify critic workflow coordination
    assert result_state["current_step"] == "critic_planner"
    assert result_state["research_steps"] == ["Research AI basics", "Research AI applications"]
    assert result_state["expert_steps"] == ["Analyze AI definition", "Analyze AI impact"]
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="critic_planner", type="instruction",
        content="Review the following plan:\n\nPlan: Research AI basics, Research AI applications, Analyze AI definition, Analyze AI impact\n\nProvide feedback and decide whether to approve or reject."
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_workflow_coordination_finalizer_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test workflow coordination with finalizer agent for completion."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "finalizer", "type": "instruction",
        "content": "Finalize the answer", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with complete workflow context
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="finalizer",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="AI is artificial intelligence that mimics human cognitive functions",
        expert_reasoning="Based on research and analysis of AI concepts",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="approve",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (coordination to finalizer)
    result_state = execute_next_step(state)
    
    # Verify finalizer workflow coordination
    assert result_state["current_step"] == "finalizer"
    assert result_state["research_results"] == ["AI is artificial intelligence", "AI has various applications"]
    assert result_state["expert_answer"] == "AI is artificial intelligence that mimics human cognitive functions"
    assert result_state["critic_expert_decision"] == "approve"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="finalizer", type="instruction",
        content="Based on the research and expert analysis, provide a final answer to: What is AI?\n\nResearch Steps: ['Research AI basics', 'Research AI applications']\nResearch Results: ['AI is artificial intelligence', 'AI has various applications']\nExpert Answer: AI is artificial intelligence that mimics human cognitive functions\nExpert Reasoning: Based on research and analysis of AI concepts"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_workflow_coordination_subgraph_integration(mock_datetime):
    """Test workflow coordination with subgraph integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test subgraph coordination scenarios
    
    # Create researcher subgraph
    mock_researcher_llm = Mock()
    mock_research_tools = [Mock(), Mock()]
    researcher_subgraph = create_researcher_subgraph(mock_researcher_llm, mock_research_tools)
    
    # Create expert subgraph
    mock_expert_llm = Mock()
    mock_expert_tools = [Mock(), Mock()]
    expert_subgraph = create_expert_subgraph(mock_expert_llm, mock_expert_tools)
    
    # Verify subgraph coordination structure
    assert researcher_subgraph is not None
    assert expert_subgraph is not None
    
    # Test subgraph state coordination
    from multi_agent_system import ResearcherState, ExpertState
    
    researcher_state = ResearcherState(
        messages=[],
        step_index=0,
        result=None
    )
    
    expert_state = ExpertState(
        messages=[],
        question="What is AI?",
        research_steps=["Research AI basics"],
        research_results=["AI is artificial intelligence"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Verify subgraph state coordination
    assert researcher_state["step_index"] == 0
    assert expert_state["question"] == "What is AI?"
    assert expert_state["research_results"] == ["AI is artificial intelligence"]

@patch('multi_agent_system.datetime')
def test_workflow_coordination_error_handling_integration(mock_datetime):
    """Test workflow coordination error handling and recovery."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test workflow coordination error scenarios
    
    # Scenario 1: Agent failure during coordination
    state_1 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=2,  # High retry count
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test retry limit coordination
    from multi_agent_system import check_retry_limit
    result_1 = check_retry_limit(state_1)
    
    # Verify error coordination handling
    assert result_1["next_step"] == "finalizer"  # Route to finalizer on error
    assert "Retry limit exceeded" in result_1["final_reasoning_trace"]
    
    # Scenario 2: Invalid workflow state coordination
    state_2 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="invalid_step",  # Invalid step
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test invalid state coordination
    result_2 = determine_next_step(state_2)
    
    # Verify invalid state coordination handling
    assert result_2["next_step"] == "finalizer"  # Route to finalizer on invalid state

@patch('multi_agent_system.datetime')
def test_workflow_coordination_performance_integration(mock_datetime):
    """Test performance aspects of workflow coordination."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test workflow coordination performance
    import time
    start_time = time.time()
    
    # Simulate multiple workflow coordination operations
    for i in range(50):
        # Mock workflow coordination for performance testing
        with patch('multi_agent_system.send_message'), patch('multi_agent_system.compose_agent_message'):
            state["next_step"] = "planner"
            state["current_step"] = "planner"
            state["research_steps"] = ["Research step " + str(i)]
            state["expert_steps"] = ["Expert step " + str(i)]
            state["research_results"] = ["Result " + str(i)]
    
    coordination_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert coordination_time < 2.0  # Should complete in less than 2 seconds
```
#### 4.2.3 State Management Across Agents

**Integration Test Information:**
- **Test Name**: State Management Across Agents
- **Location**: Section 4.2.3
- **Purpose**: Tests how state is managed, synchronized, and maintained across all agent types during multi-agent workflows, ensuring consistency, integrity, and proper state transitions throughout the entire system

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `GraphState` - Main state container for the entire system
  - `ResearcherState` - State for researcher subgraph operations
  - `ExpertState` - State for expert subgraph operations
  - `orchestrator()` - Main orchestrator function for state management
  - `create_multi_agent_graph()` - Graph creation with state management
  - `create_researcher_subgraph()` - Researcher subgraph state management
  - `create_expert_subgraph()` - Expert subgraph state management
  - `create_planner_agent()` - Planner agent state management
  - `create_researcher_agent()` - Researcher agent state management
  - `create_expert_agent()` - Expert agent state management
  - `create_critic_agent()` - Critic agent state management
  - `create_finalizer_agent()` - Finalizer agent state management
  - `determine_next_step()` - Next step determination with state management
  - `execute_next_step()` - Step execution with state management
  - `check_retry_limit()` - Retry limit checking with state management
- **Integration Points**: 
  - Global state management across all agent types
  - Subgraph state integration with main state
  - Agent state updates and synchronization
  - State transitions and progression management
  - State consistency validation across agents
  - State recovery and error handling
  - State persistence and retrieval
- **Data Flow**: 
  - Initial state setup and initialization
  - State updates during agent transitions
  - Subgraph state integration and synchronization
  - Agent output storage and state updates
  - State validation and consistency checks
  - State recovery and error handling
  - Final state completion and validation
- **State Management**: 
  - GraphState maintains global system state
  - ResearcherState manages research-specific operations
  - ExpertState manages expert-specific operations
  - State transitions update current_step and next_step
  - Agent outputs stored in appropriate state fields
  - Retry counts and limits tracked per agent type
  - State consistency maintained across all agents
- **Message Passing**: 
  - State updates trigger message composition
  - Agent responses update state fields
  - State changes propagate through message system
  - Conversation history maintained in state
  - State-driven message routing and composition

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses to avoid API calls
  - Mock datetime for consistent timestamps
  - Use real state management functions
  - Mock external tools and services
  - Use real agent factory functions
- **Test Data**: 
  - Sample complex workflows requiring state management
  - Mock LLM responses for each agent type
  - Test state configurations for different scenarios
  - Agent configurations with state management parameters
  - State transition scenarios
- **Environment Setup**: 
  - Initialize GraphState with proper structure
  - Set up mock LLM instances for each agent
  - Configure state management functions
  - Prepare test state scenarios
- **Dependencies**: 
  - Mock ChatOpenAI for LLM responses
  - Mock datetime for timestamps
  - Mock external tools and services
  - Real state management functions
- **Graph State Initialization**: 
  - Initialize GraphState with comprehensive structure
  - Set up question and initial state configuration
  - Initialize all state fields and counters
  - Set up agent-specific state fields

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete state management through full workflow
  - Multi-agent state synchronization and consistency
  - State transitions and progression management
  - State completion and finalization
  - State integrity validation across all agents
- **Component Interaction Patterns**: 
  - Agent-to-agent state transfer patterns
  - Subgraph state integration patterns
  - State update propagation patterns
  - State validation and consistency patterns
  - State recovery and error handling patterns
- **Data Transformation**: 
  - State data transformation across agents
  - Agent output transformation into state updates
  - State field transformation and validation
  - State transition transformation and management
  - Final state transformation and completion
- **Error Propagation**: 
  - State corruption and recovery scenarios
  - Agent failure state handling
  - State inconsistency detection and resolution
  - State error propagation and handling
  - State recovery mechanisms
- **State Synchronization**: 
  - State synchronization across all agents
  - Subgraph state synchronization with main state
  - Agent state consistency validation
  - State transition synchronization
  - Final state synchronization and validation
- **Message Flow**: 
  - State-driven message composition
  - Message-based state updates
  - State-aware message routing
  - Conversation history state management
  - State message flow validation
- **Retry Logic Integration**: 
  - State management during retry scenarios
  - Retry count state tracking and management
  - State preservation during retries
  - Retry failure state handling
  - State recovery after retry failures
- **Critic Decision Integration**: 
  - Critic decision state management
  - Decision impact on state transitions
  - State validation during critic review
  - Approval/rejection state handling
  - Critic-driven state management

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock ChatOpenAI.invoke() for LLM responses
  - Mock datetime for timestamps
  - Use real state management functions
  - Mock external tools and services
- **Integration Boundaries**: 
  - Test state management with real state functions
  - Test agent factories with mock LLMs
  - Mock external dependencies (LLM, tools, datetime)
- **Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Sample state management scenarios
  - Test agent configurations
  - Mock state progression data
- **Mock Behavior**: 
  - Consistent LLM response patterns
  - Proper error simulation
  - Realistic tool responses
  - State management simulation
- **LLM Mocking**: 
  - Mock ChatOpenAI constructor and invoke method
  - Simulate different response patterns for each agent
  - Mock structured output validation
  - Simulate LLM failures for error testing
- **Tool Mocking**: 
  - Mock research tools (web search, file tools)
  - Mock expert tools (calculator, unit converter)
  - Simulate tool failures and responses

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify state management across all agents
  - Verify subgraph state integration
  - Verify state transitions and progression
  - Verify state completion and finalization
- **Data Flow Validation**: 
  - Validate state data transformation
  - Verify agent output state integration
  - Check state transition validation
  - Validate state update propagation
- **State Consistency**: 
  - Verify state consistency across all agents
  - Check subgraph state consistency
  - Validate state transition consistency
  - Verify state integrity maintenance
- **Error Handling**: 
  - Verify state error propagation
  - Check state recovery mechanisms
  - Validate state corruption handling
  - Verify state error recovery
- **Message Validation**: 
  - Verify state-driven message composition
  - Check message-based state updates
  - Validate state-aware message routing
  - Verify state message flow
- **Graph State Validation**: 
  - Verify state updates across all agents
  - Check subgraph state synchronization
  - Validate state transition management
  - Verify state completion validation
- **Performance Metrics**: 
  - Measure state management performance
  - Check state synchronization efficiency
  - Validate state transition performance
  - Measure state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime
from multi_agent_system import (
    orchestrator, create_multi_agent_graph, create_researcher_subgraph,
    create_expert_subgraph, create_planner_agent, create_researcher_agent,
    create_expert_agent, create_critic_agent, create_finalizer_agent,
    determine_next_step, execute_next_step, check_retry_limit,
    GraphState, ResearcherState, ExpertState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_state_management_global_state_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test global state management across all agent types."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "planner", "type": "instruction",
        "content": "Develop a plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize comprehensive test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="input",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (state management to planner)
    result_state = execute_next_step(state)
    
    # Verify global state management
    assert result_state["current_step"] == "planner"
    assert result_state["next_step"] == "planner"
    assert result_state["planner_retry_count"] == 0
    assert result_state["researcher_retry_count"] == 0
    assert result_state["expert_retry_count"] == 0
    assert len(result_state["agent_messages"]) == 1  # New instruction message
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="planner", type="instruction",
        content="Develop a logical plan to answer the following question:\nWhat is AI?"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_state_management_subgraph_integration(mock_system_message, mock_get_conversation, 
                                             mock_convert, mock_validate, mock_compose_message, 
                                             mock_send_message, mock_datetime):
    """Test subgraph state management and integration with main state."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "AI is artificial intelligence that mimics human cognitive functions"}
    
    # Mock response message
    mock_response_message = {
        "sender": "researcher", "receiver": "orchestrator", "type": "response",
        "content": "Research completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create researcher agent
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    researcher_agent = create_researcher_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state with research workflow
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
            "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute researcher agent (subgraph state management)
    result_state = researcher_agent(state)
    
    # Verify subgraph state management
    assert result_state["research_results"] == ["AI is artificial intelligence that mimics human cognitive functions"]
    assert result_state["current_research_index"] == 1  # Moved to next research step
    assert len(result_state["agent_messages"]) == 2  # Original + response
    assert result_state["researcher_states"] == {}  # Subgraph state managed internally
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
def test_state_management_transition_integration(mock_datetime):
    """Test state transitions and progression management across agents."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test state transitions for different scenarios
    
    # Scenario 1: Planner to researcher transition
    state_1 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_1 = determine_next_step(state_1)
    assert result_1["next_step"] == "researcher"
    assert result_1["current_research_index"] == 0
    
    # Scenario 2: Researcher to expert transition
    state_2 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,  # All research completed
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_2 = determine_next_step(state_2)
    assert result_2["next_step"] == "expert"
    
    # Scenario 3: Expert to finalizer transition
    state_3 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="expert",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="AI is artificial intelligence that mimics human cognitive functions",
        expert_reasoning="Based on research and analysis",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="approve",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_3 = determine_next_step(state_3)
    assert result_3["next_step"] == "finalizer"

@patch('multi_agent_system.datetime')
def test_state_management_retry_integration(mock_datetime):
    """Test state management with retry logic integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test retry state management scenarios
    
    # Scenario 1: Retry limit not exceeded, continue state management
    state_1 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=2,  # Below retry limit
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_1 = check_retry_limit(state_1)
    assert result_1["next_step"] == "planner"  # Continue state management
    assert result_1["planner_retry_count"] == 2  # Unchanged
    
    # Scenario 2: Retry limit exceeded, state management to finalizer
    state_2 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=3,  # At retry limit
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_2 = check_retry_limit(state_2)
    assert result_2["next_step"] == "finalizer"  # State management to finalizer
    assert result_2["final_answer"] == "The question could not be answered."
    assert "Retry limit exceeded" in result_2["final_reasoning_trace"]

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_state_management_critic_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test state management with critic decision integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "critic_planner", "type": "instruction",
        "content": "Review the plan", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with plan for critic review
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="critic_planner",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (state management to critic)
    result_state = execute_next_step(state)
    
    # Verify critic state management
    assert result_state["current_step"] == "critic_planner"
    assert result_state["research_steps"] == ["Research AI basics", "Research AI applications"]
    assert result_state["expert_steps"] == ["Analyze AI definition", "Analyze AI impact"]
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="critic_planner", type="instruction",
        content="Review the following plan:\n\nPlan: Research AI basics, Research AI applications, Analyze AI definition, Analyze AI impact\n\nProvide feedback and decide whether to approve or reject."
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_state_management_error_handling_integration(mock_datetime):
    """Test state management error handling and recovery."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test state management error scenarios
    
    # Scenario 1: Invalid state transition
    state_1 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="invalid_step",  # Invalid step
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test invalid state management
    result_1 = determine_next_step(state_1)
    
    # Verify error state management handling
    assert result_1["next_step"] == "finalizer"  # Route to finalizer on invalid state
    
    # Scenario 2: State corruption recovery
    state_2 = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="researcher",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze AI definition"],
        researcher_states={},
        current_research_index=5,  # Invalid index (beyond research steps)
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test state corruption management
    result_2 = determine_next_step(state_2)
    
    # Verify state corruption management handling
    assert result_2["next_step"] == "finalizer"  # Route to finalizer on state corruption

@patch('multi_agent_system.datetime')
def test_state_management_performance_integration(mock_datetime):
    """Test performance aspects of state management across agents."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=-1,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Test state management performance
    import time
    start_time = time.time()
    
    # Simulate multiple state management operations
    for i in range(100):
        # Mock state management for performance testing
        with patch('multi_agent_system.send_message'), patch('multi_agent_system.compose_agent_message'):
            state["next_step"] = "planner"
            state["current_step"] = "planner"
            state["research_steps"] = ["Research step " + str(i)]
            state["expert_steps"] = ["Expert step " + str(i)]
            state["research_results"] = ["Result " + str(i)]
            state["expert_answer"] = "Answer " + str(i)
            state["expert_reasoning"] = "Reasoning " + str(i)
    
    management_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert management_time < 2.0  # Should complete in less than 2 seconds
```
### 4.3 Tool Integration
#### 4.3.1 Research Tool Integration

**Integration Test Information:**
- **Test Name**: Research Tool Integration
- **Location**: Section 4.3.1
- **Purpose**: Tests how research tools (YouTube transcripts, file loaders, web search, Wikipedia, browser MCP) integrate with the researcher agent and subgraph, including tool invocation, data processing, and result integration into the multi-agent workflow

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `youtube_transcript_tool()` - YouTube video transcript extraction
  - `unstructured_excel_tool()` - Excel file processing
  - `unstructured_powerpoint_tool()` - PowerPoint file processing
  - `unstructured_pdf_tool()` - PDF file processing
  - `text_file_tool()` - Text file processing
  - `get_browser_mcp_tools()` - Browser MCP tool integration
  - `get_research_tools()` - Research tool aggregation
  - `create_researcher_subgraph()` - Researcher subgraph with tool integration
  - `create_researcher_agent()` - Researcher agent with tool access
  - `create_researcher_llm_node()` - Researcher LLM node with tool integration
  - `ResearcherState` - State for research operations
  - `GraphState` - Main state container for research results
- **Integration Points**: 
  - Research tool integration with researcher subgraph
  - Tool invocation and data processing workflows
  - Research results integration into GraphState
  - Tool error handling and recovery
  - Research tool coordination and aggregation
  - Tool output transformation and storage
  - Research workflow state management
- **Data Flow**: 
  - Research request triggers tool selection
  - Tool invocation processes input data
  - Tool output transforms into research results
  - Results integrate into researcher state
  - State updates propagate to GraphState
  - Research results flow to expert agent
  - Tool coordination manages multiple tools
- **State Management**: 
  - ResearcherState manages tool-specific operations
  - GraphState stores research results and metadata
  - Tool state tracks invocation and results
  - Research workflow state guides tool selection
  - State updates reflect tool processing outcomes
- **Message Passing**: 
  - Research requests trigger tool invocation
  - Tool results propagate through message system
  - Research state updates communicate tool outcomes
  - Tool coordination messages manage workflow
  - Error messages handle tool failures

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock external tool APIs (YouTube, web search, etc.)
  - Mock file system operations
  - Mock MCP client connections
  - Use real research tool functions
  - Mock LLM responses for researcher agent
  - Use real subgraph and agent factory functions
- **Test Data**: 
  - Sample YouTube URLs and transcripts
  - Test files (Excel, PowerPoint, PDF, text)
  - Mock web search results
  - Mock Wikipedia content
  - Mock browser MCP responses
  - Research workflow scenarios
- **Environment Setup**: 
  - Initialize GraphState with research workflow
  - Set up mock external tool services
  - Configure research tool functions
  - Prepare test file data
  - Set up mock MCP client
- **Dependencies**: 
  - Mock YouTube API responses
  - Mock file system operations
  - Mock web search APIs
  - Mock Wikipedia API
  - Mock MCP client and tools
  - Real research tool functions
- **Graph State Initialization**: 
  - Initialize GraphState with research workflow
  - Set up researcher state and tools
  - Initialize research results storage
  - Configure tool integration parameters

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete research tool workflow integration
  - Multi-tool research coordination
  - Tool result integration into workflow
  - Research state management and updates
  - Tool output transformation and storage
- **Component Interaction Patterns**: 
  - Tool-to-researcher agent integration patterns
  - Tool-to-subgraph integration patterns
  - Tool coordination and aggregation patterns
  - Tool result processing patterns
  - Tool error handling patterns
- **Data Transformation**: 
  - Tool output transformation into research results
  - File content transformation and processing
  - Web search result transformation
  - Transcript data transformation
  - Tool result aggregation and synthesis
- **Error Propagation**: 
  - Tool failure handling and recovery
  - File access error propagation
  - Network error handling in tools
  - Tool timeout and retry scenarios
  - Tool integration error recovery
- **State Synchronization**: 
  - Tool state synchronization with researcher state
  - Research result state management
  - Tool coordination state updates
  - State consistency across tool operations
  - Tool workflow state progression
- **Message Flow**: 
  - Research request message flows
  - Tool invocation message patterns
  - Tool result message propagation
  - Tool error message handling
  - Research completion message flows
- **Retry Logic Integration**: 
  - Tool retry mechanisms and state management
  - Tool failure recovery and retry
  - Tool timeout handling and retry
  - Tool integration retry logic
  - Research workflow retry coordination
- **Critic Decision Integration**: 
  - Tool result quality assessment
  - Research tool selection validation
  - Tool output validation and feedback
  - Tool integration decision impact
  - Research tool coordination decisions

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock external tool APIs and services
  - Mock file system operations
  - Use real research tool functions
  - Mock LLM responses for researcher
  - Use real subgraph and agent functions
- **Integration Boundaries**: 
  - Test research tool integration with real tool functions
  - Test researcher agent with mock LLMs
  - Mock external dependencies (APIs, file system, MCP)
- **Mock Data Setup**: 
  - Realistic tool API responses
  - Sample file content and metadata
  - Mock web search and Wikipedia results
  - Mock MCP tool responses
  - Test research workflow data
- **Mock Behavior**: 
  - Consistent tool API response patterns
  - Proper error simulation for tools
  - Realistic file processing behavior
  - Tool timeout and retry simulation
  - Tool coordination behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI for researcher agent
  - Simulate researcher response patterns
  - Mock tool selection and coordination
  - Simulate research result processing
- **Tool Mocking**: 
  - Mock YouTube transcript API
  - Mock file loader operations
  - Mock web search APIs
  - Mock Wikipedia API
  - Mock MCP client and tools

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify research tool integration with researcher agent
  - Verify tool coordination and aggregation
  - Verify tool result integration into workflow
  - Verify research state management
- **Data Flow Validation**: 
  - Validate tool output transformation
  - Verify research result data flow
  - Check tool coordination data flow
  - Validate tool state updates
- **State Consistency**: 
  - Verify research state consistency
  - Check tool state synchronization
  - Validate research result storage
  - Verify tool workflow state
- **Error Handling**: 
  - Verify tool error propagation
  - Check tool failure recovery
  - Validate tool retry mechanisms
  - Verify tool integration error handling
- **Message Validation**: 
  - Verify research request messages
  - Check tool invocation messages
  - Validate tool result messages
  - Verify tool error messages
- **Graph State Validation**: 
  - Verify research result storage in GraphState
  - Check tool state updates
  - Validate research workflow state
  - Verify tool integration state
- **Performance Metrics**: 
  - Measure tool integration performance
  - Check tool coordination efficiency
  - Validate research result processing
  - Measure tool state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    youtube_transcript_tool, unstructured_excel_tool, unstructured_powerpoint_tool,
    unstructured_pdf_tool, text_file_tool, get_browser_mcp_tools, get_research_tools,
    create_researcher_subgraph, create_researcher_agent, create_researcher_llm_node,
    ResearcherState, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document

@patch('multi_agent_system.YouTubeLoader')
@patch('multi_agent_system.youtube_transcript_api')
def test_research_tool_youtube_integration(mock_youtube_api, mock_youtube_loader):
    """Test YouTube transcript tool integration with researcher agent."""
    # Setup mock YouTube API
    mock_transcript = Mock()
    mock_transcript.fetch.return_value = [
        {"text": "AI is artificial intelligence", "start": 0.0, "duration": 2.0},
        {"text": "that mimics human cognitive functions", "start": 2.0, "duration": 3.0}
    ]
    mock_youtube_api.YouTubeTranscriptApi.get_transcript.return_value = mock_transcript.fetch.return_value
    
    # Setup mock YouTube loader
    mock_loader = Mock()
    mock_loader.load.return_value = [
        Document(page_content="AI is artificial intelligence that mimics human cognitive functions", metadata={"source": "youtube_video"})
    ]
    mock_youtube_loader.return_value = mock_loader
    
    # Test YouTube transcript tool integration
    test_url = "https://www.youtube.com/watch?v=test123"
    result = youtube_transcript_tool(test_url)
    
    # Verify YouTube tool integration
    assert "AI is artificial intelligence" in result
    assert "mimics human cognitive functions" in result
    mock_youtube_api.YouTubeTranscriptApi.get_transcript.assert_called_once_with("test123")
    mock_youtube_loader.assert_called_once_with(test_url)

@patch('multi_agent_system.UnstructuredExcelLoader')
@patch('builtins.open', create=True)
def test_research_tool_excel_integration(mock_open, mock_excel_loader):
    """Test Excel file tool integration with researcher agent."""
    # Setup mock file operations
    mock_file = Mock()
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Setup mock Excel loader
    mock_loader = Mock()
    mock_loader.load.return_value = [
        Document(page_content="Excel data: AI applications in healthcare", metadata={"source": "test.xlsx"}),
        Document(page_content="Excel data: AI applications in finance", metadata={"source": "test.xlsx"})
    ]
    mock_excel_loader.return_value = mock_loader
    
    # Test Excel tool integration
    test_file_path = "/path/to/test.xlsx"
    result = unstructured_excel_tool(test_file_path)
    
    # Verify Excel tool integration
    assert len(result) == 2
    assert "AI applications in healthcare" in result[0].page_content
    assert "AI applications in finance" in result[1].page_content
    assert result[0].metadata["source"] == "test.xlsx"
    mock_excel_loader.assert_called_once_with(test_file_path)

@patch('multi_agent_system.UnstructuredPowerPointLoader')
@patch('builtins.open', create=True)
def test_research_tool_powerpoint_integration(mock_open, mock_powerpoint_loader):
    """Test PowerPoint file tool integration with researcher agent."""
    # Setup mock file operations
    mock_file = Mock()
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Setup mock PowerPoint loader
    mock_loader = Mock()
    mock_loader.load.return_value = [
        Document(page_content="Slide 1: Introduction to AI", metadata={"source": "test.pptx"}),
        Document(page_content="Slide 2: AI Applications", metadata={"source": "test.pptx"})
    ]
    mock_powerpoint_loader.return_value = mock_loader
    
    # Test PowerPoint tool integration
    test_file_path = "/path/to/test.pptx"
    result = unstructured_powerpoint_tool(test_file_path)
    
    # Verify PowerPoint tool integration
    assert len(result) == 2
    assert "Introduction to AI" in result[0].page_content
    assert "AI Applications" in result[1].page_content
    assert result[0].metadata["source"] == "test.pptx"
    mock_powerpoint_loader.assert_called_once_with(test_file_path)

@patch('multi_agent_system.UnstructuredPDFLoader')
@patch('builtins.open', create=True)
def test_research_tool_pdf_integration(mock_open, mock_pdf_loader):
    """Test PDF file tool integration with researcher agent."""
    # Setup mock file operations
    mock_file = Mock()
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Setup mock PDF loader
    mock_loader = Mock()
    mock_loader.load.return_value = [
        Document(page_content="PDF content: AI research paper on machine learning", metadata={"source": "test.pdf"}),
        Document(page_content="PDF content: AI research paper on neural networks", metadata={"source": "test.pdf"})
    ]
    mock_pdf_loader.return_value = mock_loader
    
    # Test PDF tool integration
    test_file_path = "/path/to/test.pdf"
    result = unstructured_pdf_tool(test_file_path)
    
    # Verify PDF tool integration
    assert len(result) == 2
    assert "machine learning" in result[0].page_content
    assert "neural networks" in result[1].page_content
    assert result[0].metadata["source"] == "test.pdf"
    mock_pdf_loader.assert_called_once_with(test_file_path)

@patch('builtins.open', create=True)
def test_research_tool_text_integration(mock_open):
    """Test text file tool integration with researcher agent."""
    # Setup mock file operations
    mock_file = Mock()
    mock_file.read.return_value = "Text file content: AI is artificial intelligence that mimics human cognitive functions."
    mock_open.return_value.__enter__.return_value = mock_file
    
    # Test text tool integration
    test_file_path = "/path/to/test.txt"
    result = text_file_tool(test_file_path)
    
    # Verify text tool integration
    assert "AI is artificial intelligence" in result
    assert "mimics human cognitive functions" in result
    mock_open.assert_called_once_with(test_file_path, 'r', encoding='utf-8')

@patch('multi_agent_system.load_mcp_tools')
@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_research_tool_mcp_integration(mock_client, mock_session, mock_load_tools):
    """Test browser MCP tool integration with researcher agent."""
    # Setup mock MCP client
    mock_mcp_client = Mock()
    mock_client.return_value = mock_mcp_client
    
    # Setup mock session
    mock_mcp_session = Mock()
    mock_session.return_value.__aenter__.return_value = mock_mcp_session
    
    # Setup mock tools
    mock_tools = [
        Mock(name="web_search", description="Search the web"),
        Mock(name="browser_navigate", description="Navigate to URL")
    ]
    mock_load_tools.return_value = mock_tools
    
    # Test MCP tool integration
    test_mcp_url = "http://localhost:3000"
    result = await get_browser_mcp_tools(test_mcp_url)
    
    # Verify MCP tool integration
    assert len(result) == 2
    assert result[0].name == "web_search"
    assert result[1].name == "browser_navigate"
    mock_client.assert_called_once()
    mock_session.assert_called_once()
    mock_load_tools.assert_called_once()

@patch('multi_agent_system.get_browser_mcp_tools')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearch')
def test_research_tool_aggregation_integration(mock_tavily, mock_wikipedia, mock_mcp_tools):
    """Test research tool aggregation and integration."""
    # Setup mock tools
    mock_tavily_tool = Mock()
    mock_tavily.return_value = mock_tavily_tool
    
    mock_wikipedia_tool = Mock()
    mock_wikipedia.return_value = mock_wikipedia_tool
    
    mock_mcp_tools.return_value = [Mock(), Mock()]
    
    # Test research tool aggregation
    research_tools = get_research_tools()
    
    # Verify tool aggregation integration
    assert len(research_tools) > 0
    mock_tavily.assert_called_once()
    mock_wikipedia.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.get_research_tools')
def test_research_tool_researcher_agent_integration(mock_get_tools, mock_system_message, mock_get_conversation, 
                                                   mock_convert, mock_validate, mock_compose_message, 
                                                   mock_send_message, mock_datetime):
    """Test research tool integration with researcher agent."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock research tools
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research AI"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "AI is artificial intelligence based on research from multiple sources"}
    
    # Mock response message
    mock_response_message = {
        "sender": "researcher", "receiver": "orchestrator", "type": "response",
        "content": "Research completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create researcher agent with tool integration
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    researcher_agent = create_researcher_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state with research workflow
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
            "content": "Research AI basics", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute researcher agent with tool integration
    result_state = researcher_agent(state)
    
    # Verify research tool integration with agent
    assert result_state["research_results"] == ["AI is artificial intelligence based on research from multiple sources"]
    assert result_state["current_research_index"] == 1  # Moved to next research step
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_get_tools.assert_called_once()
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
def test_research_tool_error_handling_integration(mock_datetime):
    """Test research tool error handling and recovery integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test research tool error scenarios
    
    # Scenario 1: File not found error
    with patch('builtins.open', side_effect=FileNotFoundError("File not found")):
        with pytest.raises(FileNotFoundError):
            text_file_tool("/path/to/nonexistent.txt")
    
    # Scenario 2: YouTube API error
    with patch('multi_agent_system.youtube_transcript_api.YouTubeTranscriptApi.get_transcript', 
               side_effect=Exception("Video not available")):
        with pytest.raises(Exception):
            youtube_transcript_tool("https://www.youtube.com/watch?v=invalid")
    
    # Scenario 3: MCP connection error
    with patch('multi_agent_system.ClientSession', side_effect=Exception("Connection failed")):
        with pytest.raises(Exception):
            import asyncio
            asyncio.run(get_browser_mcp_tools("http://invalid-url"))

@patch('multi_agent_system.datetime')
def test_research_tool_performance_integration(mock_datetime):
    """Test performance aspects of research tool integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test research tool performance
    import time
    start_time = time.time()
    
    # Simulate multiple research tool operations
    with patch('builtins.open', create=True), patch('multi_agent_system.YouTubeLoader'), \
         patch('multi_agent_system.UnstructuredExcelLoader'), patch('multi_agent_system.UnstructuredPowerPointLoader'), \
         patch('multi_agent_system.UnstructuredPDFLoader'):
        
        for i in range(10):
            # Mock file operations for performance testing
            mock_file = Mock()
            mock_file.read.return_value = f"Test content {i}"
            
            # Test text file tool performance
            text_file_tool(f"/path/to/test{i}.txt")
            
            # Test other tool integrations would go here
    
    tool_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert tool_time < 5.0  # Should complete in less than 5 seconds
```
#### 4.3.2 Expert Tool Integration

**Integration Test Information:**
- **Test Name**: Expert Tool Integration
- **Location**: Section 4.3.2
- **Purpose**: Tests how expert tools (unit converter, calculator, Python REPL) integrate with the expert agent and subgraph, including tool invocation, data processing, and result integration into the multi-agent workflow for analysis and computation tasks

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `unit_converter()` - Unit conversion tool using pint library
  - `calculator()` - Mathematical calculation tool
  - `PythonREPLTool` - Python code execution tool
  - `get_expert_tools()` - Expert tool aggregation
  - `create_expert_subgraph()` - Expert subgraph with tool integration
  - `create_expert_agent()` - Expert agent with tool access
  - `create_expert_llm_node()` - Expert LLM node with tool integration
  - `ExpertState` - State for expert operations
  - `GraphState` - Main state container for expert results
- **Integration Points**: 
  - Expert tool integration with expert subgraph
  - Tool invocation and computation workflows
  - Expert results integration into GraphState
  - Tool error handling and recovery
  - Expert tool coordination and aggregation
  - Tool output transformation and storage
  - Expert workflow state management
- **Data Flow**: 
  - Expert request triggers tool selection
  - Tool invocation processes computation tasks
  - Tool output transforms into expert results
  - Results integrate into expert state
  - State updates propagate to GraphState
  - Expert results flow to finalizer agent
  - Tool coordination manages multiple tools
- **State Management**: 
  - ExpertState manages tool-specific operations
  - GraphState stores expert results and metadata
  - Tool state tracks invocation and results
  - Expert workflow state guides tool selection
  - State updates reflect tool processing outcomes
- **Message Passing**: 
  - Expert requests trigger tool invocation
  - Tool results propagate through message system
  - Expert state updates communicate tool outcomes
  - Tool coordination messages manage workflow
  - Error messages handle tool failures

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock external computation libraries (pint, math)
  - Mock Python REPL execution environment
  - Use real expert tool functions
  - Mock LLM responses for expert agent
  - Use real subgraph and agent factory functions
- **Test Data**: 
  - Sample unit conversion scenarios
  - Mathematical calculation expressions
  - Python code snippets for execution
  - Expert analysis scenarios
  - Computation workflow data
- **Environment Setup**: 
  - Initialize GraphState with expert workflow
  - Set up mock computation environments
  - Configure expert tool functions
  - Prepare test computation data
  - Set up mock Python REPL
- **Dependencies**: 
  - Mock pint unit conversion library
  - Mock mathematical computation functions
  - Mock Python REPL execution
  - Real expert tool functions
- **Graph State Initialization**: 
  - Initialize GraphState with expert workflow
  - Set up expert state and tools
  - Initialize expert results storage
  - Configure tool integration parameters

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete expert tool workflow integration
  - Multi-tool expert coordination
  - Tool result integration into workflow
  - Expert state management and updates
  - Tool output transformation and storage
- **Component Interaction Patterns**: 
  - Tool-to-expert agent integration patterns
  - Tool-to-subgraph integration patterns
  - Tool coordination and aggregation patterns
  - Tool result processing patterns
  - Tool error handling patterns
- **Data Transformation**: 
  - Tool output transformation into expert results
  - Unit conversion data transformation
  - Mathematical computation transformation
  - Python code execution transformation
  - Tool result aggregation and synthesis
- **Error Propagation**: 
  - Tool failure handling and recovery
  - Invalid input error propagation
  - Computation error handling
  - Tool timeout and retry scenarios
  - Tool integration error recovery
- **State Synchronization**: 
  - Tool state synchronization with expert state
  - Expert result state management
  - Tool coordination state updates
  - State consistency across tool operations
  - Tool workflow state progression
- **Message Flow**: 
  - Expert request message flows
  - Tool invocation message patterns
  - Tool result message propagation
  - Tool error message handling
  - Expert completion message flows
- **Retry Logic Integration**: 
  - Tool retry mechanisms and state management
  - Tool failure recovery and retry
  - Tool timeout handling and retry
  - Tool integration retry logic
  - Expert workflow retry coordination
- **Critic Decision Integration**: 
  - Tool result quality assessment
  - Expert tool selection validation
  - Tool output validation and feedback
  - Tool integration decision impact
  - Expert tool coordination decisions

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock external computation libraries
  - Mock Python REPL execution
  - Use real expert tool functions
  - Mock LLM responses for expert
  - Use real subgraph and agent functions
- **Integration Boundaries**: 
  - Test expert tool integration with real tool functions
  - Test expert agent with mock LLMs
  - Mock external dependencies (computation libraries, REPL)
- **Mock Data Setup**: 
  - Realistic computation scenarios
  - Sample unit conversion data
  - Mathematical expressions and results
  - Python code snippets and outputs
  - Test expert workflow data
- **Mock Behavior**: 
  - Consistent computation response patterns
  - Proper error simulation for tools
  - Realistic tool execution behavior
  - Tool timeout and retry simulation
  - Tool coordination behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI for expert agent
  - Simulate expert response patterns
  - Mock tool selection and coordination
  - Simulate expert result processing
- **Tool Mocking**: 
  - Mock pint unit conversion library
  - Mock mathematical computation functions
  - Mock Python REPL execution
  - Mock tool execution environments

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify expert tool integration with expert agent
  - Verify tool coordination and aggregation
  - Verify tool result integration into workflow
  - Verify expert state management
- **Data Flow Validation**: 
  - Validate tool output transformation
  - Verify expert result data flow
  - Check tool coordination data flow
  - Validate tool state updates
- **State Consistency**: 
  - Verify expert state consistency
  - Check tool state synchronization
  - Validate expert result storage
  - Verify tool workflow state
- **Error Handling**: 
  - Verify tool error propagation
  - Check tool failure recovery
  - Validate tool retry mechanisms
  - Verify tool integration error handling
- **Message Validation**: 
  - Verify expert request messages
  - Check tool invocation messages
  - Validate tool result messages
  - Verify tool error messages
- **Graph State Validation**: 
  - Verify expert result storage in GraphState
  - Check tool state updates
  - Validate expert workflow state
  - Verify tool integration state
- **Performance Metrics**: 
  - Measure tool integration performance
  - Check tool coordination efficiency
  - Validate expert result processing
  - Measure tool state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    unit_converter, calculator, get_expert_tools,
    create_expert_subgraph, create_expert_agent, create_expert_llm_node,
    ExpertState, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import PythonREPLTool

@patch('multi_agent_system.pint')
def test_expert_tool_unit_converter_integration(mock_pint):
    """Test unit converter tool integration with expert agent."""
    # Setup mock pint library
    mock_ureg = Mock()
    mock_quantity = Mock()
    mock_quantity.to.return_value = Mock()
    mock_quantity.to.return_value.magnitude = 1000.0
    mock_quantity.to.return_value.units = "meter"
    mock_ureg.Quantity.return_value = mock_quantity
    mock_pint.UnitRegistry.return_value = mock_ureg
    
    # Test unit converter tool integration
    test_quantity = "1 kilometer"
    test_to_unit = "meter"
    result = unit_converter(test_quantity, test_to_unit)
    
    # Verify unit converter tool integration
    assert "1000.0 meter" in result
    mock_ureg.Quantity.assert_called_once_with("1 kilometer")
    mock_quantity.to.assert_called_once_with("meter")

@patch('multi_agent_system.math')
def test_expert_tool_calculator_integration(mock_math):
    """Test calculator tool integration with expert agent."""
    # Setup mock math functions
    mock_math.eval = Mock(return_value=25.0)
    
    # Test calculator tool integration
    test_expression = "5 * 5"
    result = calculator(test_expression)
    
    # Verify calculator tool integration
    assert "25.0" in result
    mock_math.eval.assert_called_once_with(test_expression)

@patch('multi_agent_system.PythonREPLTool')
def test_expert_tool_python_repl_integration(mock_python_repl):
    """Test Python REPL tool integration with expert agent."""
    # Setup mock Python REPL
    mock_repl_tool = Mock()
    mock_repl_tool.invoke.return_value = "Result: 42"
    mock_python_repl.return_value = mock_repl_tool
    
    # Test Python REPL tool integration
    test_code = "x = 40 + 2\nprint(f'Result: {x}')"
    
    # Create Python REPL tool instance
    repl_tool = PythonREPLTool()
    result = repl_tool.invoke(test_code)
    
    # Verify Python REPL tool integration
    assert "Result: 42" in result
    mock_python_repl.assert_called_once()

def test_expert_tool_aggregation_integration():
    """Test expert tool aggregation and integration."""
    # Test expert tool aggregation
    expert_tools = get_expert_tools()
    
    # Verify tool aggregation integration
    assert len(expert_tools) > 0
    
    # Check for specific tool types
    tool_names = [tool.name for tool in expert_tools]
    assert "unit_converter" in tool_names
    assert "calculator" in tool_names

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.get_expert_tools')
def test_expert_tool_expert_agent_integration(mock_get_tools, mock_system_message, mock_get_conversation, 
                                             mock_convert, mock_validate, mock_compose_message, 
                                             mock_send_message, mock_datetime):
    """Test expert tool integration with expert agent."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock expert tools
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "Analysis complete", "expert_reasoning": "Based on computation and analysis"}
    
    # Mock response message
    mock_response_message = {
        "sender": "expert", "receiver": "orchestrator", "type": "response",
        "content": "Analysis completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create expert agent with tool integration
    mock_config = AgentConfig("expert", "openai", "gpt-4", 0.7, 
                             {"expert_answer": "string", "expert_reasoning": "string"}, 
                             "You are an expert agent", 5)
    expert_agent = create_expert_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state with expert workflow
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "expert", "type": "instruction",
            "content": "Analyze AI definition", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="expert",
        next_step="",
        research_steps=["Research AI basics", "Research AI applications"],
        expert_steps=["Analyze AI definition", "Analyze AI impact"],
        researcher_states={},
        current_research_index=2,  # All research completed
        research_results=["AI is artificial intelligence", "AI has various applications"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute expert agent with tool integration
    result_state = expert_agent(state)
    
    # Verify expert tool integration with agent
    assert result_state["expert_answer"] == "Analysis complete"
    assert result_state["expert_reasoning"] == "Based on computation and analysis"
    assert result_state["research_results"] == ["AI is artificial intelligence", "AI has various applications"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_get_tools.assert_called_once()
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_expert_tool_computation_workflow_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test expert tool computation workflow integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "expert", "type": "instruction",
        "content": "Calculate and analyze", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with computation workflow
    state = GraphState(
        agent_messages=[],
        question="Calculate the area of a circle with radius 5",
        current_step="",
        next_step="expert",
        research_steps=["Research circle area formula"],
        expert_steps=["Calculate area using Ï€rÂ²", "Analyze the result"],
        researcher_states={},
        current_research_index=2,
        research_results=["Circle area formula: A = Ï€rÂ²"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (computation workflow to expert)
    from multi_agent_system import execute_next_step
    result_state = execute_next_step(state)
    
    # Verify computation workflow integration
    assert result_state["current_step"] == "expert"
    assert result_state["research_results"] == ["Circle area formula: A = Ï€rÂ²"]
    assert result_state["expert_steps"] == ["Calculate area using Ï€rÂ²", "Analyze the result"]
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="expert", type="instruction",
        content="Calculate area using Ï€rÂ²"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_expert_tool_error_handling_integration(mock_datetime):
    """Test expert tool error handling and recovery integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test expert tool error scenarios
    
    # Scenario 1: Invalid unit conversion
    with patch('multi_agent_system.pint.UnitRegistry') as mock_pint:
        mock_ureg = Mock()
        mock_ureg.Quantity.side_effect = Exception("Invalid unit")
        mock_pint.return_value = mock_ureg
        
        with pytest.raises(Exception):
            unit_converter("invalid", "meter")
    
    # Scenario 2: Invalid mathematical expression
    with patch('multi_agent_system.math.eval', side_effect=Exception("Invalid expression")):
        with pytest.raises(Exception):
            calculator("invalid expression")
    
    # Scenario 3: Python REPL execution error
    with patch('multi_agent_system.PythonREPLTool') as mock_repl:
        mock_repl_tool = Mock()
        mock_repl_tool.invoke.side_effect = Exception("Execution error")
        mock_repl.return_value = mock_repl_tool
        
        with pytest.raises(Exception):
            repl_tool = PythonREPLTool()
            repl_tool.invoke("invalid python code")

@patch('multi_agent_system.datetime')
def test_expert_tool_performance_integration(mock_datetime):
    """Test performance aspects of expert tool integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test expert tool performance
    import time
    start_time = time.time()
    
    # Simulate multiple expert tool operations
    with patch('multi_agent_system.pint'), patch('multi_agent_system.math'), patch('multi_agent_system.PythonREPLTool'):
        
        for i in range(50):
            # Mock computation operations for performance testing
            mock_pint_ureg = Mock()
            mock_pint_ureg.Quantity.return_value.to.return_value.magnitude = i
            mock_pint_ureg.Quantity.return_value.to.return_value.units = "meter"
            
            # Test unit converter performance
            unit_converter(f"{i} kilometer", "meter")
            
            # Test calculator performance
            calculator(f"{i} + {i}")
            
            # Test Python REPL performance
            mock_repl_tool = Mock()
            mock_repl_tool.invoke.return_value = f"Result: {i * 2}"
    
    tool_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert tool_time < 3.0  # Should complete in less than 3 seconds

@patch('multi_agent_system.datetime')
def test_expert_tool_state_management_integration(mock_datetime):
    """Test expert tool state management and integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test expert tool state management scenarios
    
    # Scenario 1: Expert state with tool results
    expert_state = ExpertState(
        messages=[],
        question="Calculate the area of a circle",
        research_steps=["Research circle area formula"],
        research_results=["Circle area formula: A = Ï€rÂ²"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Verify expert state management
    assert expert_state["question"] == "Calculate the area of a circle"
    assert expert_state["research_results"] == ["Circle area formula: A = Ï€rÂ²"]
    
    # Scenario 2: GraphState with expert tool integration
    state = GraphState(
        agent_messages=[],
        question="What is the area of a circle with radius 5?",
        current_step="expert",
        next_step="",
        research_steps=["Research circle area formula"],
        expert_steps=["Calculate area", "Analyze result"],
        researcher_states={},
        current_research_index=2,
        research_results=["Circle area formula: A = Ï€rÂ²"],
        expert_state=expert_state,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify GraphState expert tool integration
    assert state["current_step"] == "expert"
    assert state["expert_state"] == expert_state
    assert state["research_results"] == ["Circle area formula: A = Ï€rÂ²"]
```
#### 4.3.3 External Service Integration

**Integration Test Information:**
- **Test Name**: External Service Integration
- **Location**: Section 4.3.3
- **Purpose**: Tests how external services (YouTube, Wikipedia, web search, browser MCP, file loaders) integrate with the multi-agent system, including service connectivity, data retrieval, error handling, and result integration into the agent workflow

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `youtube_transcript_tool()` - YouTube transcript retrieval service
  - `WikipediaQueryRun` - Wikipedia search and retrieval service
  - `TavilySearch` - Web search service
  - `get_browser_mcp_tools()` - Browser MCP service integration
  - `UnstructuredExcelLoader` - Excel file loading service
  - `UnstructuredPowerPointLoader` - PowerPoint file loading service
  - `UnstructuredPDFLoader` - PDF file loading service
  - `TextLoader` - Text file loading service
  - `get_research_tools()` - Research tool aggregation
  - `create_researcher_subgraph()` - Researcher subgraph with external services
  - `create_researcher_agent()` - Researcher agent with external service access
  - `ResearcherState` - State for external service operations
  - `GraphState` - Main state container for external service results
- **Integration Points**: 
  - External service integration with researcher subgraph
  - Service connectivity and data retrieval workflows
  - External service results integration into GraphState
  - Service error handling and recovery mechanisms
  - External service coordination and aggregation
  - Service output transformation and storage
  - External service workflow state management
- **Data Flow**: 
  - Research request triggers external service selection
  - Service invocation retrieves external data
  - Service output transforms into research results
  - Results integrate into researcher state
  - State updates propagate to GraphState
  - External service results flow to expert agent
  - Service coordination manages multiple external services
- **State Management**: 
  - ResearcherState manages external service operations
  - GraphState stores external service results and metadata
  - Service state tracks connectivity and results
  - External service workflow state guides service selection
  - State updates reflect external service processing outcomes
- **Message Passing**: 
  - Research requests trigger external service invocation
  - Service results propagate through message system
  - External service state updates communicate outcomes
  - Service coordination messages manage workflow
  - Error messages handle external service failures

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock external service APIs (YouTube, Wikipedia, web search)
  - Mock browser MCP client and session
  - Mock file system operations for loaders
  - Use real external service tool functions
  - Mock LLM responses for researcher agent
  - Use real subgraph and agent factory functions
- **Test Data**: 
  - Sample YouTube video URLs and transcripts
  - Wikipedia search queries and results
  - Web search queries and results
  - Browser MCP session data
  - Sample file paths and content
  - External service response data
- **Environment Setup**: 
  - Initialize GraphState with external service workflow
  - Set up mock external service environments
  - Configure external service tool functions
  - Prepare test external service data
  - Set up mock file system operations
- **Dependencies**: 
  - Mock YouTube API client
  - Mock Wikipedia API client
  - Mock web search API client
  - Mock browser MCP client
  - Mock file system operations
  - Real external service tool functions
- **Graph State Initialization**: 
  - Initialize GraphState with external service workflow
  - Set up researcher state and external services
  - Initialize external service results storage
  - Configure service integration parameters

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete external service workflow integration
  - Multi-service researcher coordination
  - External service result integration into workflow
  - Researcher state management and updates
  - External service output transformation and storage
- **Component Interaction Patterns**: 
  - Service-to-researcher agent integration patterns
  - Service-to-subgraph integration patterns
  - External service coordination and aggregation patterns
  - Service result processing patterns
  - External service error handling patterns
- **Data Transformation**: 
  - External service output transformation into research results
  - YouTube transcript data transformation
  - Wikipedia search result transformation
  - Web search result transformation
  - File content transformation
  - External service result aggregation and synthesis
- **Error Propagation**: 
  - External service failure handling and recovery
  - Network connectivity error propagation
  - Service timeout and retry scenarios
  - Invalid service response handling
  - External service integration error recovery
- **State Synchronization**: 
  - External service state synchronization with researcher state
  - Researcher result state management
  - External service coordination state updates
  - State consistency across external service operations
  - External service workflow state progression
- **Message Flow**: 
  - Research request message flows
  - External service invocation message patterns
  - Service result message propagation
  - External service error message handling
  - Research completion message flows
- **Retry Logic Integration**: 
  - External service retry mechanisms and state management
  - Service failure recovery and retry
  - Service timeout handling and retry
  - External service integration retry logic
  - Researcher workflow retry coordination
- **Critic Decision Integration**: 
  - External service result quality assessment
  - Researcher service selection validation
  - External service output validation and feedback
  - Service integration decision impact
  - Researcher external service coordination decisions

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock external service APIs and clients
  - Mock file system operations
  - Use real external service tool functions
  - Mock LLM responses for researcher
  - Use real subgraph and agent functions
- **Integration Boundaries**: 
  - Test external service integration with real tool functions
  - Test researcher agent with mock LLMs
  - Mock external dependencies (APIs, file system, network)
- **Mock Data Setup**: 
  - Realistic external service response scenarios
  - Sample YouTube transcript data
  - Wikipedia search results and content
  - Web search results and snippets
  - Browser MCP session data
  - Test file content and metadata
- **Mock Behavior**: 
  - Consistent external service response patterns
  - Proper error simulation for services
  - Realistic service execution behavior
  - Service timeout and retry simulation
  - External service coordination behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI for researcher agent
  - Simulate researcher response patterns
  - Mock external service selection and coordination
  - Simulate researcher result processing
- **Tool Mocking**: 
  - Mock YouTube API client and transcript retrieval
  - Mock Wikipedia API client and search
  - Mock web search API client
  - Mock browser MCP client and session
  - Mock file system operations for loaders

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify external service integration with researcher agent
  - Verify service coordination and aggregation
  - Verify external service result integration into workflow
  - Verify researcher state management
- **Data Flow Validation**: 
  - Validate external service output transformation
  - Verify researcher result data flow
  - Check external service coordination data flow
  - Validate external service state updates
- **State Consistency**: 
  - Verify researcher state consistency
  - Check external service state synchronization
  - Validate researcher result storage
  - Verify external service workflow state
- **Error Handling**: 
  - Verify external service error propagation
  - Check external service failure recovery
  - Validate external service retry mechanisms
  - Verify external service integration error handling
- **Message Validation**: 
  - Verify research request messages
  - Check external service invocation messages
  - Validate external service result messages
  - Verify external service error messages
- **Graph State Validation**: 
  - Verify researcher result storage in GraphState
  - Check external service state updates
  - Validate researcher workflow state
  - Verify external service integration state
- **Performance Metrics**: 
  - Measure external service integration performance
  - Check external service coordination efficiency
  - Validate researcher result processing
  - Measure external service state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    youtube_transcript_tool, get_browser_mcp_tools, get_research_tools,
    create_researcher_subgraph, create_researcher_agent, create_researcher_llm_node,
    ResearcherState, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.tools import WikipediaQueryRun
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.document_loaders import UnstructuredExcelLoader, UnstructuredPowerPointLoader, UnstructuredPDFLoader, TextLoader

@patch('multi_agent_system.YouTubeTranscriptApi')
def test_external_service_youtube_integration(mock_youtube_api):
    """Test YouTube transcript service integration with researcher agent."""
    # Setup mock YouTube API
    mock_transcript = [{"text": "This is a test transcript", "start": 0, "duration": 5}]
    mock_youtube_api.get_transcript.return_value = mock_transcript
    
    # Test YouTube transcript service integration
    test_url = "https://www.youtube.com/watch?v=test123"
    result = youtube_transcript_tool(test_url)
    
    # Verify YouTube service integration
    assert "This is a test transcript" in result
    mock_youtube_api.get_transcript.assert_called_once()

@patch('multi_agent_system.WikipediaQueryRun')
def test_external_service_wikipedia_integration(mock_wikipedia):
    """Test Wikipedia service integration with researcher agent."""
    # Setup mock Wikipedia service
    mock_wikipedia_instance = Mock()
    mock_wikipedia_instance.run.return_value = "Wikipedia search result content"
    mock_wikipedia.return_value = mock_wikipedia_instance
    
    # Test Wikipedia service integration
    test_query = "Artificial Intelligence"
    wikipedia_tool = WikipediaQueryRun()
    result = wikipedia_tool.run(test_query)
    
    # Verify Wikipedia service integration
    assert "Wikipedia search result content" in result
    mock_wikipedia_instance.run.assert_called_once_with(test_query)

@patch('multi_agent_system.TavilySearchResults')
def test_external_service_web_search_integration(mock_tavily):
    """Test web search service integration with researcher agent."""
    # Setup mock Tavily search service
    mock_tavily_instance = Mock()
    mock_tavily_instance.invoke.return_value = [{"content": "Web search result", "url": "https://example.com"}]
    mock_tavily.return_value = mock_tavily_instance
    
    # Test web search service integration
    test_query = "AI research papers"
    tavily_tool = TavilySearchResults()
    result = tavily_tool.invoke(test_query)
    
    # Verify web search service integration
    assert len(result) > 0
    assert "Web search result" in result[0]["content"]
    mock_tavily_instance.invoke.assert_called_once_with(test_query)

@patch('multi_agent_system.ClientSession')
@patch('multi_agent_system.streamablehttp_client')
async def test_external_service_browser_mcp_integration(mock_client, mock_session):
    """Test browser MCP service integration with researcher agent."""
    # Setup mock browser MCP client and session
    mock_session_instance = Mock()
    mock_session.return_value = mock_session_instance
    
    mock_client_instance = Mock()
    mock_client_instance.get_tools.return_value = [{"name": "browser_tool", "description": "Browser tool"}]
    mock_client.return_value = mock_client_instance
    
    # Test browser MCP service integration
    test_mcp_url = "http://localhost:3000"
    result = await get_browser_mcp_tools(test_mcp_url)
    
    # Verify browser MCP service integration
    assert len(result) > 0
    assert "browser_tool" in result[0]["name"]
    mock_session.assert_called_once()
    mock_client.assert_called_once()

@patch('multi_agent_system.UnstructuredExcelLoader')
def test_external_service_excel_loader_integration(mock_excel_loader):
    """Test Excel file loader service integration with researcher agent."""
    # Setup mock Excel loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [Mock(page_content="Excel content", metadata={"source": "test.xlsx"})]
    mock_excel_loader.return_value = mock_loader_instance
    
    # Test Excel loader service integration
    test_file_path = "test_data.xlsx"
    loader = UnstructuredExcelLoader(test_file_path)
    result = loader.load()
    
    # Verify Excel loader service integration
    assert len(result) > 0
    assert "Excel content" in result[0].page_content
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredPowerPointLoader')
def test_external_service_powerpoint_loader_integration(mock_powerpoint_loader):
    """Test PowerPoint file loader service integration with researcher agent."""
    # Setup mock PowerPoint loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [Mock(page_content="PowerPoint content", metadata={"source": "test.pptx"})]
    mock_powerpoint_loader.return_value = mock_loader_instance
    
    # Test PowerPoint loader service integration
    test_file_path = "test_presentation.pptx"
    loader = UnstructuredPowerPointLoader(test_file_path)
    result = loader.load()
    
    # Verify PowerPoint loader service integration
    assert len(result) > 0
    assert "PowerPoint content" in result[0].page_content
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_external_service_pdf_loader_integration(mock_pdf_loader):
    """Test PDF file loader service integration with researcher agent."""
    # Setup mock PDF loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [Mock(page_content="PDF content", metadata={"source": "test.pdf"})]
    mock_pdf_loader.return_value = mock_loader_instance
    
    # Test PDF loader service integration
    test_file_path = "test_document.pdf"
    loader = UnstructuredPDFLoader(test_file_path)
    result = loader.load()
    
    # Verify PDF loader service integration
    assert len(result) > 0
    assert "PDF content" in result[0].page_content
    mock_loader_instance.load.assert_called_once()

@patch('multi_agent_system.TextLoader')
def test_external_service_text_loader_integration(mock_text_loader):
    """Test text file loader service integration with researcher agent."""
    # Setup mock text loader
    mock_loader_instance = Mock()
    mock_loader_instance.load.return_value = [Mock(page_content="Text content", metadata={"source": "test.txt"})]
    mock_text_loader.return_value = mock_loader_instance
    
    # Test text loader service integration
    test_file_path = "test_file.txt"
    loader = TextLoader(test_file_path)
    result = loader.load()
    
    # Verify text loader service integration
    assert len(result) > 0
    assert "Text content" in result[0].page_content
    mock_loader_instance.load.assert_called_once()

def test_external_service_research_tools_aggregation_integration():
    """Test external service research tools aggregation and integration."""
    # Test research tools aggregation
    research_tools = get_research_tools()
    
    # Verify research tools aggregation integration
    assert len(research_tools) > 0
    
    # Check for specific tool types
    tool_names = [tool.name for tool in research_tools]
    assert "youtube_transcript_tool" in tool_names
    assert "unstructured_excel_tool" in tool_names
    assert "unstructured_powerpoint_tool" in tool_names
    assert "unstructured_pdf_tool" in tool_names
    assert "text_file_tool" in tool_names

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.get_research_tools')
def test_external_service_researcher_agent_integration(mock_get_tools, mock_system_message, mock_get_conversation, 
                                                      mock_convert, mock_validate, mock_compose_message, 
                                                      mock_send_message, mock_datetime):
    """Test external service integration with researcher agent."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock research tools (external services)
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "Research completed using external services"}
    
    # Mock response message
    mock_response_message = {
        "sender": "researcher", "receiver": "orchestrator", "type": "response",
        "content": "Research completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create researcher agent with external service integration
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    researcher_agent = create_researcher_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state with external service workflow
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
            "content": "Research AI definition", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Search Wikipedia", "Search web", "Check YouTube"],
        expert_steps=["Analyze research results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute researcher agent with external service integration
    result_state = researcher_agent(state)
    
    # Verify external service integration with researcher agent
    assert result_state["research_results"] == ["Research completed using external services"]
    assert result_state["current_research_index"] == 1
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_get_tools.assert_called_once()
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_external_service_research_workflow_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test external service research workflow integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
        "content": "Search Wikipedia for AI definition", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with external service research workflow
    state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="",
        next_step="researcher",
        research_steps=["Search Wikipedia", "Search web", "Check YouTube videos"],
        expert_steps=["Analyze research results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (research workflow to researcher)
    from multi_agent_system import execute_next_step
    result_state = execute_next_step(state)
    
    # Verify external service research workflow integration
    assert result_state["current_step"] == "researcher"
    assert result_state["research_steps"] == ["Search Wikipedia", "Search web", "Check YouTube videos"]
    assert result_state["current_research_index"] == 0
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="researcher", type="instruction",
        content="Search Wikipedia"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_external_service_error_handling_integration(mock_datetime):
    """Test external service error handling and recovery integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test external service error scenarios
    
    # Scenario 1: YouTube API error
    with patch('multi_agent_system.YouTubeTranscriptApi') as mock_youtube:
        mock_youtube.get_transcript.side_effect = Exception("YouTube API error")
        
        with pytest.raises(Exception):
            youtube_transcript_tool("https://www.youtube.com/watch?v=invalid")
    
    # Scenario 2: Wikipedia API error
    with patch('multi_agent_system.WikipediaQueryRun') as mock_wikipedia:
        mock_wikipedia_instance = Mock()
        mock_wikipedia_instance.run.side_effect = Exception("Wikipedia API error")
        mock_wikipedia.return_value = mock_wikipedia_instance
        
        with pytest.raises(Exception):
            wikipedia_tool = WikipediaQueryRun()
            wikipedia_tool.run("invalid query")
    
    # Scenario 3: Web search API error
    with patch('multi_agent_system.TavilySearchResults') as mock_tavily:
        mock_tavily_instance = Mock()
        mock_tavily_instance.invoke.side_effect = Exception("Web search API error")
        mock_tavily.return_value = mock_tavily_instance
        
        with pytest.raises(Exception):
            tavily_tool = TavilySearchResults()
            tavily_tool.invoke("invalid search")
    
    # Scenario 4: File loader error
    with patch('multi_agent_system.UnstructuredPDFLoader') as mock_pdf_loader:
        mock_loader_instance = Mock()
        mock_loader_instance.load.side_effect = Exception("File not found")
        mock_pdf_loader.return_value = mock_loader_instance
        
        with pytest.raises(Exception):
            loader = UnstructuredPDFLoader("nonexistent.pdf")
            loader.load()

@patch('multi_agent_system.datetime')
def test_external_service_performance_integration(mock_datetime):
    """Test performance aspects of external service integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test external service performance
    import time
    start_time = time.time()
    
    # Simulate multiple external service operations
    with patch('multi_agent_system.YouTubeTranscriptApi'), patch('multi_agent_system.WikipediaQueryRun'), patch('multi_agent_system.TavilySearchResults'):
        
        for i in range(10):
            # Mock external service operations for performance testing
            mock_youtube = Mock()
            mock_youtube.get_transcript.return_value = [{"text": f"Transcript {i}", "start": 0, "duration": 5}]
            
            # Test YouTube service performance
            youtube_transcript_tool(f"https://www.youtube.com/watch?v=test{i}")
            
            # Test Wikipedia service performance
            mock_wikipedia = Mock()
            mock_wikipedia.run.return_value = f"Wikipedia result {i}"
            
            # Test web search service performance
            mock_tavily = Mock()
            mock_tavily.invoke.return_value = [{"content": f"Web search result {i}", "url": f"https://example{i}.com"}]
    
    service_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert service_time < 2.0  # Should complete in less than 2 seconds

@patch('multi_agent_system.datetime')
def test_external_service_state_management_integration(mock_datetime):
    """Test external service state management and integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test external service state management scenarios
    
    # Scenario 1: Researcher state with external service results
    researcher_state = ResearcherState(
        messages=[],
        step_index=0,
        result="External service research completed"
    )
    
    # Verify researcher state management
    assert researcher_state["step_index"] == 0
    assert researcher_state["result"] == "External service research completed"
    
    # Scenario 2: GraphState with external service integration
    state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="researcher",
        next_step="",
        research_steps=["Search Wikipedia", "Search web", "Check YouTube"],
        expert_steps=["Analyze research results"],
        researcher_states={0: researcher_state},
        current_research_index=1,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify GraphState external service integration
    assert state["current_step"] == "researcher"
    assert state["researcher_states"][0] == researcher_state
    assert state["research_results"] == ["AI is artificial intelligence"]
    assert state["current_research_index"] == 1
```
### 4.4 Graph Component Integration
#### 4.4.1 Subgraph Integration

**Integration Test Information:**
- **Test Name**: Subgraph Integration
- **Location**: Section 4.4.1
- **Purpose**: Tests how researcher and expert subgraphs integrate with the main multi-agent graph, including subgraph compilation, state management, tool integration, and workflow coordination between subgraphs and the orchestrator

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `create_researcher_subgraph()` - Researcher subgraph factory with tool integration
  - `create_expert_subgraph()` - Expert subgraph factory with tool integration
  - `create_researcher_llm_node()` - Researcher LLM node factory
  - `create_expert_llm_node()` - Expert LLM node factory
  - `get_research_tools()` - Research tool aggregation for subgraph
  - `get_expert_tools()` - Expert tool aggregation for subgraph
  - `create_multi_agent_graph()` - Main graph with subgraph integration
  - `orchestrator()` - Main orchestrator with subgraph coordination
  - `execute_next_step()` - Step execution with subgraph routing
  - `ResearcherState` - State for researcher subgraph operations
  - `ExpertState` - State for expert subgraph operations
  - `GraphState` - Main state container for subgraph results
- **Integration Points**: 
  - Subgraph integration with main multi-agent graph
  - Subgraph compilation and state management
  - Tool integration within subgraphs
  - Subgraph-to-orchestrator communication
  - Subgraph workflow coordination and routing
  - Subgraph state synchronization with main graph
  - Subgraph error handling and recovery
- **Data Flow**: 
  - Orchestrator request triggers subgraph selection
  - Subgraph compilation prepares workflow execution
  - Tool integration provides capabilities to subgraph
  - Subgraph execution processes state and tools
  - Results flow back to orchestrator through state
  - State updates propagate through main graph
  - Subgraph coordination manages workflow transitions
- **State Management**: 
  - ResearcherState manages subgraph-specific operations
  - ExpertState manages subgraph-specific operations
  - GraphState coordinates subgraph state integration
  - Subgraph state tracks workflow progression
  - State updates reflect subgraph processing outcomes
  - Subgraph state synchronization with main graph
- **Message Passing**: 
  - Orchestrator messages trigger subgraph execution
  - Subgraph results propagate through state updates
  - Subgraph state updates communicate outcomes
  - Subgraph coordination messages manage workflow
  - Error messages handle subgraph failures

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses for subgraph nodes
  - Mock tool functions within subgraphs
  - Use real subgraph factory functions
  - Mock subgraph compilation process
  - Use real orchestrator and execution functions
  - Mock tool aggregation functions
- **Test Data**: 
  - Sample researcher workflow scenarios
  - Sample expert workflow scenarios
  - Tool invocation test data
  - Subgraph state transition data
  - Workflow coordination test data
- **Environment Setup**: 
  - Initialize GraphState with subgraph workflow
  - Set up mock LLM environments for subgraphs
  - Configure tool functions for subgraphs
  - Prepare test subgraph execution data
  - Set up mock subgraph compilation
- **Dependencies**: 
  - Mock ChatOpenAI for subgraph LLM nodes
  - Mock tool functions for subgraph integration
  - Mock StateGraph compilation
  - Real subgraph factory functions
  - Real orchestrator and execution functions
- **Graph State Initialization**: 
  - Initialize GraphState with subgraph workflow
  - Set up researcher and expert states
  - Initialize subgraph results storage
  - Configure subgraph integration parameters

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete subgraph workflow integration
  - Multi-subgraph coordination and routing
  - Subgraph result integration into main graph
  - Subgraph state management and updates
  - Subgraph tool integration and execution
- **Component Interaction Patterns**: 
  - Subgraph-to-orchestrator integration patterns
  - Subgraph-to-main-graph integration patterns
  - Subgraph coordination and routing patterns
  - Subgraph tool integration patterns
  - Subgraph error handling patterns
- **Data Transformation**: 
  - Subgraph output transformation into main graph results
  - Researcher subgraph data transformation
  - Expert subgraph data transformation
  - Tool result transformation within subgraphs
  - Subgraph state transformation and integration
- **Error Propagation**: 
  - Subgraph failure handling and recovery
  - Subgraph compilation error propagation
  - Tool integration error handling
  - Subgraph timeout and retry scenarios
  - Subgraph integration error recovery
- **State Synchronization**: 
  - Subgraph state synchronization with main graph
  - Researcher subgraph state management
  - Expert subgraph state management
  - Subgraph coordination state updates
  - State consistency across subgraph operations
- **Message Flow**: 
  - Orchestrator-to-subgraph message flows
  - Subgraph execution message patterns
  - Subgraph result message propagation
  - Subgraph error message handling
  - Subgraph completion message flows
- **Retry Logic Integration**: 
  - Subgraph retry mechanisms and state management
  - Subgraph failure recovery and retry
  - Subgraph timeout handling and retry
  - Subgraph integration retry logic
  - Main graph retry coordination with subgraphs
- **Critic Decision Integration**: 
  - Subgraph result quality assessment
  - Subgraph selection validation
  - Subgraph output validation and feedback
  - Subgraph integration decision impact
  - Subgraph coordination decisions

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock LLM responses for subgraph nodes
  - Mock tool functions within subgraphs
  - Use real subgraph factory functions
  - Mock subgraph compilation
  - Use real orchestrator and execution functions
- **Integration Boundaries**: 
  - Test subgraph integration with real factory functions
  - Test subgraph execution with mock LLMs and tools
  - Mock external dependencies (LLMs, tools)
- **Mock Data Setup**: 
  - Realistic subgraph execution scenarios
  - Sample researcher workflow data
  - Expert workflow data and tool results
  - Subgraph state transition data
  - Tool integration test data
- **Mock Behavior**: 
  - Consistent subgraph execution patterns
  - Proper error simulation for subgraphs
  - Realistic subgraph compilation behavior
  - Subgraph timeout and retry simulation
  - Subgraph coordination behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI for subgraph LLM nodes
  - Simulate subgraph response patterns
  - Mock tool selection and coordination within subgraphs
  - Simulate subgraph result processing
- **Tool Mocking**: 
  - Mock research tools for researcher subgraph
  - Mock expert tools for expert subgraph
  - Mock tool invocation and results
  - Mock tool integration within subgraphs

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify subgraph integration with main graph
  - Verify subgraph coordination and routing
  - Verify subgraph result integration into workflow
  - Verify subgraph state management
- **Data Flow Validation**: 
  - Validate subgraph output transformation
  - Verify subgraph result data flow
  - Check subgraph coordination data flow
  - Validate subgraph state updates
- **State Consistency**: 
  - Verify subgraph state consistency
  - Check subgraph state synchronization
  - Validate subgraph result storage
  - Verify subgraph workflow state
- **Error Handling**: 
  - Verify subgraph error propagation
  - Check subgraph failure recovery
  - Validate subgraph retry mechanisms
  - Verify subgraph integration error handling
- **Message Validation**: 
  - Verify orchestrator-to-subgraph messages
  - Check subgraph execution messages
  - Validate subgraph result messages
  - Verify subgraph error messages
- **Graph State Validation**: 
  - Verify subgraph result storage in GraphState
  - Check subgraph state updates
  - Validate subgraph workflow state
  - Verify subgraph integration state
- **Performance Metrics**: 
  - Measure subgraph integration performance
  - Check subgraph coordination efficiency
  - Validate subgraph result processing
  - Measure subgraph state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    create_researcher_subgraph, create_expert_subgraph,
    create_researcher_llm_node, create_expert_llm_node,
    get_research_tools, get_expert_tools, create_multi_agent_graph,
    orchestrator, execute_next_step, ResearcherState, ExpertState, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph

@patch('multi_agent_system.get_research_tools')
@patch('multi_agent_system.create_researcher_llm_node')
def test_researcher_subgraph_integration(mock_create_llm_node, mock_get_tools):
    """Test researcher subgraph integration with main graph."""
    # Setup mocks
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    mock_llm_node = Mock()
    mock_create_llm_node.return_value = mock_llm_node
    
    # Create researcher subgraph
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    
    subgraph = create_researcher_subgraph(mock_llm_node, mock_tools)
    
    # Verify researcher subgraph integration
    assert isinstance(subgraph, StateGraph)
    mock_create_llm_node.assert_called_once_with(mock_config, Mock())
    mock_get_tools.assert_called_once()

@patch('multi_agent_system.get_expert_tools')
@patch('multi_agent_system.create_expert_llm_node')
def test_expert_subgraph_integration(mock_create_llm_node, mock_get_tools):
    """Test expert subgraph integration with main graph."""
    # Setup mocks
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    mock_llm_node = Mock()
    mock_create_llm_node.return_value = mock_llm_node
    
    # Create expert subgraph
    mock_config = AgentConfig("expert", "openai", "gpt-4", 0.7, 
                             {"expert_answer": "string", "expert_reasoning": "string"}, 
                             "You are an expert agent", 5)
    
    subgraph = create_expert_subgraph(mock_llm_node, mock_tools)
    
    # Verify expert subgraph integration
    assert isinstance(subgraph, StateGraph)
    mock_create_llm_node.assert_called_once_with(mock_config, Mock())
    mock_get_tools.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.get_research_tools')
def test_researcher_llm_node_integration(mock_get_tools, mock_system_message, mock_get_conversation, 
                                       mock_convert, mock_validate, mock_compose_message, 
                                       mock_send_message, mock_datetime):
    """Test researcher LLM node integration within subgraph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock research tools
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "Research completed"}
    
    # Mock response message
    mock_response_message = {
        "sender": "researcher", "receiver": "orchestrator", "type": "response",
        "content": "Research completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create researcher LLM node
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    researcher_node = create_researcher_llm_node(mock_config, mock_llm)
    
    # Initialize test researcher state
    researcher_state = ResearcherState(
        messages=[],
        step_index=0,
        result=None
    )
    
    # Execute researcher LLM node
    result_state = researcher_node(researcher_state)
    
    # Verify researcher LLM node integration
    assert result_state["result"] == "Research completed"
    assert result_state["step_index"] == 0
    mock_get_tools.assert_called_once()
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
@patch('multi_agent_system.get_expert_tools')
def test_expert_llm_node_integration(mock_get_tools, mock_system_message, mock_get_conversation, 
                                   mock_convert, mock_validate, mock_compose_message, 
                                   mock_send_message, mock_datetime):
    """Test expert LLM node integration within subgraph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock expert tools
    mock_tools = [Mock(), Mock(), Mock()]
    mock_get_tools.return_value = mock_tools
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "Analysis complete", "expert_reasoning": "Based on analysis"}
    
    # Mock response message
    mock_response_message = {
        "sender": "expert", "receiver": "orchestrator", "type": "response",
        "content": "Analysis completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create expert LLM node
    mock_config = AgentConfig("expert", "openai", "gpt-4", 0.7, 
                             {"expert_answer": "string", "expert_reasoning": "string"}, 
                             "You are an expert agent", 5)
    expert_node = create_expert_llm_node(mock_config, mock_llm)
    
    # Initialize test expert state
    expert_state = ExpertState(
        messages=[],
        question="What is AI?",
        research_steps=["Research AI basics"],
        research_results=["AI is artificial intelligence"],
        expert_answer="",
        expert_reasoning=""
    )
    
    # Execute expert LLM node
    result_state = expert_node(expert_state)
    
    # Verify expert LLM node integration
    assert result_state["expert_answer"] == "Analysis complete"
    assert result_state["expert_reasoning"] == "Based on analysis"
    assert result_state["research_results"] == ["AI is artificial intelligence"]
    mock_get_tools.assert_called_once()
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_subgraph_workflow_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test subgraph workflow integration with main graph."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
        "content": "Research AI definition", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with subgraph workflow
    state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="",
        next_step="researcher",
        research_steps=["Research AI definition", "Research AI applications"],
        expert_steps=["Analyze research results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (workflow to researcher subgraph)
    result_state = execute_next_step(state)
    
    # Verify subgraph workflow integration
    assert result_state["current_step"] == "researcher"
    assert result_state["research_steps"] == ["Research AI definition", "Research AI applications"]
    assert result_state["current_research_index"] == 0
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="researcher", type="instruction",
        content="Research AI definition"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_subgraph_error_handling_integration(mock_datetime):
    """Test subgraph error handling and recovery integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test subgraph error scenarios
    
    # Scenario 1: Researcher subgraph compilation error
    with patch('multi_agent_system.create_researcher_subgraph') as mock_create_subgraph:
        mock_create_subgraph.side_effect = Exception("Subgraph compilation error")
        
        with pytest.raises(Exception):
            create_researcher_subgraph(Mock(), [])
    
    # Scenario 2: Expert subgraph compilation error
    with patch('multi_agent_system.create_expert_subgraph') as mock_create_subgraph:
        mock_create_subgraph.side_effect = Exception("Subgraph compilation error")
        
        with pytest.raises(Exception):
            create_expert_subgraph(Mock(), [])
    
    # Scenario 3: LLM node creation error
    with patch('multi_agent_system.create_researcher_llm_node') as mock_create_node:
        mock_create_node.side_effect = Exception("LLM node creation error")
        
        with pytest.raises(Exception):
            create_researcher_llm_node(Mock(), Mock())

@patch('multi_agent_system.datetime')
def test_subgraph_performance_integration(mock_datetime):
    """Test performance aspects of subgraph integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test subgraph performance
    import time
    start_time = time.time()
    
    # Simulate multiple subgraph operations
    with patch('multi_agent_system.get_research_tools'), patch('multi_agent_system.get_expert_tools'):
        
        for i in range(5):
            # Mock subgraph operations for performance testing
            mock_tools = [Mock(), Mock(), Mock()]
            
            # Test researcher subgraph creation performance
            create_researcher_subgraph(Mock(), mock_tools)
            
            # Test expert subgraph creation performance
            create_expert_subgraph(Mock(), mock_tools)
            
            # Test LLM node creation performance
            create_researcher_llm_node(Mock(), Mock())
            create_expert_llm_node(Mock(), Mock())
    
    subgraph_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert subgraph_time < 1.0  # Should complete in less than 1 second

@patch('multi_agent_system.datetime')
def test_subgraph_state_management_integration(mock_datetime):
    """Test subgraph state management and integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test subgraph state management scenarios
    
    # Scenario 1: Researcher state with subgraph results
    researcher_state = ResearcherState(
        messages=[],
        step_index=1,
        result="Subgraph research completed"
    )
    
    # Verify researcher state management
    assert researcher_state["step_index"] == 1
    assert researcher_state["result"] == "Subgraph research completed"
    
    # Scenario 2: Expert state with subgraph results
    expert_state = ExpertState(
        messages=[],
        question="What is AI?",
        research_steps=["Research AI basics"],
        research_results=["AI is artificial intelligence"],
        expert_answer="AI is artificial intelligence",
        expert_reasoning="Based on research analysis"
    )
    
    # Verify expert state management
    assert expert_state["expert_answer"] == "AI is artificial intelligence"
    assert expert_state["expert_reasoning"] == "Based on research analysis"
    
    # Scenario 3: GraphState with subgraph integration
    state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="researcher",
        next_step="expert",
        research_steps=["Research AI definition"],
        expert_steps=["Analyze research results"],
        researcher_states={0: researcher_state},
        current_research_index=1,
        research_results=["AI is artificial intelligence"],
        expert_state=expert_state,
        expert_answer="AI is artificial intelligence",
        expert_reasoning="Based on research analysis",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify GraphState subgraph integration
    assert state["current_step"] == "researcher"
    assert state["next_step"] == "expert"
    assert state["researcher_states"][0] == researcher_state
    assert state["expert_state"] == expert_state
    assert state["research_results"] == ["AI is artificial intelligence"]
    assert state["expert_answer"] == "AI is artificial intelligence"

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.create_researcher_subgraph')
@patch('multi_agent_system.create_expert_subgraph')
@patch('multi_agent_system.create_planner_agent')
@patch('multi_agent_system.create_researcher_agent')
@patch('multi_agent_system.create_expert_agent')
@patch('multi_agent_system.create_critic_agent')
@patch('multi_agent_system.create_finalizer_agent')
def test_multi_agent_graph_subgraph_integration(mock_create_finalizer, mock_create_critic, 
                                               mock_create_expert, mock_create_researcher, 
                                               mock_create_planner, mock_create_expert_subgraph,
                                               mock_create_researcher_subgraph, mock_datetime):
    """Test multi-agent graph integration with subgraphs."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock subgraphs and agents
    mock_researcher_subgraph = Mock()
    mock_expert_subgraph = Mock()
    mock_create_researcher_subgraph.return_value = mock_researcher_subgraph
    mock_create_expert_subgraph.return_value = mock_expert_subgraph
    
    mock_planner_agent = Mock()
    mock_researcher_agent = Mock()
    mock_expert_agent = Mock()
    mock_critic_agent = Mock()
    mock_finalizer_agent = Mock()
    
    mock_create_planner.return_value = mock_planner_agent
    mock_create_researcher.return_value = mock_researcher_agent
    mock_create_expert.return_value = mock_expert_agent
    mock_create_critic.return_value = mock_critic_agent
    mock_create_finalizer.return_value = mock_finalizer_agent
    
    # Create multi-agent graph with subgraph integration
    mock_configs = {
        "planner": AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3),
        "researcher": AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5),
        "expert": AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5),
        "critic": AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3),
        "finalizer": AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    }
    
    graph = create_multi_agent_graph(mock_configs)
    
    # Verify multi-agent graph subgraph integration
    assert isinstance(graph, StateGraph)
    mock_create_researcher_subgraph.assert_called_once()
    mock_create_expert_subgraph.assert_called_once()
    mock_create_planner.assert_called_once()
    mock_create_researcher.assert_called_once()
    mock_create_expert.assert_called_once()
    mock_create_critic.assert_called_once()
    mock_create_finalizer.assert_called_once()
```
#### 4.4.2 Agent Node Integration

**Integration Test Information:**
- **Test Name**: Agent Node Integration
- **Location**: Section 4.4.2
- **Purpose**: Tests how agent nodes (planner, researcher, expert, critic, finalizer) integrate with the main graph and orchestrator, including node execution, state management, message handling, and workflow coordination between different agent types

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `create_planner_agent()` - Planner agent node factory
  - `create_researcher_agent()` - Researcher agent node factory
  - `create_expert_agent()` - Expert agent node factory
  - `create_critic_agent()` - Critic agent node factory
  - `create_finalizer_agent()` - Finalizer agent node factory
  - `orchestrator()` - Main orchestrator with agent node coordination
  - `execute_next_step()` - Step execution with agent node routing
  - `determine_next_step()` - Next step determination for agent nodes
  - `check_retry_limit()` - Retry limit checking for agent nodes
  - `route_from_orchestrator()` - Routing logic for agent node selection
  - `GraphState` - Main state container for agent node results
  - `AgentMessage` - Message protocol for agent node communication
- **Integration Points**: 
  - Agent node integration with main graph execution
  - Agent node state management and updates
  - Agent node message handling and communication
  - Agent node workflow coordination and routing
  - Agent node error handling and retry mechanisms
  - Agent node result integration into main workflow
  - Agent node synchronization with orchestrator
- **Data Flow**: 
  - Orchestrator request triggers agent node selection
  - Agent node execution processes state and tools
  - Agent node results flow back to orchestrator
  - State updates propagate through main graph
  - Agent node coordination manages workflow transitions
  - Message flow handles agent node communication
  - Retry logic manages agent node failures
- **State Management**: 
  - GraphState coordinates agent node state integration
  - Agent node state tracks workflow progression
  - State updates reflect agent node processing outcomes
  - Agent node state synchronization with main graph
  - Retry state management for agent node failures
  - Agent node result state integration
- **Message Passing**: 
  - Orchestrator messages trigger agent node execution
  - Agent node results propagate through message system
  - Agent node state updates communicate outcomes
  - Agent node coordination messages manage workflow
  - Error messages handle agent node failures

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses for agent nodes
  - Mock tool functions within agent nodes
  - Use real agent node factory functions
  - Mock agent node compilation process
  - Use real orchestrator and execution functions
  - Mock tool aggregation functions
- **Test Data**: 
  - Sample agent workflow scenarios
  - Agent node execution test data
  - Tool invocation test data
  - Agent node state transition data
  - Workflow coordination test data
- **Environment Setup**: 
  - Initialize GraphState with agent node workflow
  - Set up mock LLM environments for agent nodes
  - Configure tool functions for agent nodes
  - Prepare test agent node execution data
  - Set up mock agent node compilation
- **Dependencies**: 
  - Mock ChatOpenAI for agent node LLMs
  - Mock tool functions for agent node integration
  - Mock StateGraph compilation
  - Real agent node factory functions
  - Real orchestrator and execution functions
- **Graph State Initialization**: 
  - Initialize GraphState with agent node workflow
  - Set up agent node states and tools
  - Initialize agent node results storage
  - Configure agent node integration parameters

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete agent node workflow integration
  - Multi-agent node coordination and routing
  - Agent node result integration into main graph
  - Agent node state management and updates
  - Agent node tool integration and execution
- **Component Interaction Patterns**: 
  - Agent node-to-orchestrator integration patterns
  - Agent node-to-main-graph integration patterns
  - Agent node coordination and routing patterns
  - Agent node tool integration patterns
  - Agent node error handling patterns
- **Data Transformation**: 
  - Agent node output transformation into main graph results
  - Planner agent node data transformation
  - Researcher agent node data transformation
  - Expert agent node data transformation
  - Critic agent node data transformation
  - Finalizer agent node data transformation
- **Error Propagation**: 
  - Agent node failure handling and recovery
  - Agent node compilation error propagation
  - Tool integration error handling
  - Agent node timeout and retry scenarios
  - Agent node integration error recovery
- **State Synchronization**: 
  - Agent node state synchronization with main graph
  - Planner agent node state management
  - Researcher agent node state management
  - Expert agent node state management
  - Critic agent node state management
  - Finalizer agent node state management
- **Message Flow**: 
  - Orchestrator-to-agent node message flows
  - Agent node execution message patterns
  - Agent node result message propagation
  - Agent node error message handling
  - Agent node completion message flows
- **Retry Logic Integration**: 
  - Agent node retry mechanisms and state management
  - Agent node failure recovery and retry
  - Agent node timeout handling and retry
  - Agent node integration retry logic
  - Main graph retry coordination with agent nodes
- **Critic Decision Integration**: 
  - Agent node result quality assessment
  - Agent node selection validation
  - Agent node output validation and feedback
  - Agent node integration decision impact
  - Agent node coordination decisions

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock LLM responses for agent nodes
  - Mock tool functions within agent nodes
  - Use real agent node factory functions
  - Mock agent node compilation
  - Use real orchestrator and execution functions
- **Integration Boundaries**: 
  - Test agent node integration with real factory functions
  - Test agent node execution with mock LLMs and tools
  - Mock external dependencies (LLMs, tools)
- **Mock Data Setup**: 
  - Realistic agent node execution scenarios
  - Sample planner workflow data
  - Researcher workflow data and tool results
  - Expert workflow data and analysis results
  - Critic workflow data and decision results
  - Finalizer workflow data and final results
- **Mock Behavior**: 
  - Consistent agent node execution patterns
  - Proper error simulation for agent nodes
  - Realistic agent node compilation behavior
  - Agent node timeout and retry simulation
  - Agent node coordination behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI for agent node LLMs
  - Simulate agent node response patterns
  - Mock tool selection and coordination within agent nodes
  - Simulate agent node result processing
- **Tool Mocking**: 
  - Mock research tools for researcher agent node
  - Mock expert tools for expert agent node
  - Mock tool invocation and results
  - Mock tool integration within agent nodes

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify agent node integration with main graph
  - Verify agent node coordination and routing
  - Verify agent node result integration into workflow
  - Verify agent node state management
- **Data Flow Validation**: 
  - Validate agent node output transformation
  - Verify agent node result data flow
  - Check agent node coordination data flow
  - Validate agent node state updates
- **State Consistency**: 
  - Verify agent node state consistency
  - Check agent node state synchronization
  - Validate agent node result storage
  - Verify agent node workflow state
- **Error Handling**: 
  - Verify agent node error propagation
  - Check agent node failure recovery
  - Validate agent node retry mechanisms
  - Verify agent node integration error handling
- **Message Validation**: 
  - Verify orchestrator-to-agent node messages
  - Check agent node execution messages
  - Validate agent node result messages
  - Verify agent node error messages
- **Graph State Validation**: 
  - Verify agent node result storage in GraphState
  - Check agent node state updates
  - Validate agent node workflow state
  - Verify agent node integration state
- **Performance Metrics**: 
  - Measure agent node integration performance
  - Check agent node coordination efficiency
  - Validate agent node result processing
  - Measure agent node state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    create_planner_agent, create_researcher_agent, create_expert_agent,
    create_critic_agent, create_finalizer_agent, orchestrator, execute_next_step,
    determine_next_step, check_retry_limit, route_from_orchestrator,
    GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_planner_agent_node_integration(mock_system_message, mock_get_conversation, 
                                      mock_convert, mock_validate, mock_compose_message, 
                                      mock_send_message, mock_datetime):
    """Test planner agent node integration with main graph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "Plan this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"research_steps": ["Research AI basics"], "expert_steps": ["Analyze results"]}
    
    # Mock response message
    mock_response_message = {
        "sender": "planner", "receiver": "orchestrator", "type": "response",
        "content": "Planning completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create planner agent node
    mock_config = AgentConfig("planner", "openai", "gpt-4", 0.7, 
                             {"research_steps": "list", "expert_steps": "list"}, 
                             "You are a planner agent", 3)
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Initialize test state
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "planner", "type": "instruction",
            "content": "Plan AI research", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute planner agent node
    result_state = planner_agent(state)
    
    # Verify planner agent node integration
    assert result_state["research_steps"] == ["Research AI basics"]
    assert result_state["expert_steps"] == ["Analyze results"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_researcher_agent_node_integration(mock_system_message, mock_get_conversation, 
                                         mock_convert, mock_validate, mock_compose_message, 
                                         mock_send_message, mock_datetime):
    """Test researcher agent node integration with main graph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "researcher", "type": "instruction", "content": "Research this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"result": "Research completed"}
    
    # Mock response message
    mock_response_message = {
        "sender": "researcher", "receiver": "orchestrator", "type": "response",
        "content": "Research completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create researcher agent node
    mock_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, 
                             {"result": "string"}, 
                             "You are a researcher agent", 5)
    researcher_agent = create_researcher_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "researcher", "type": "instruction",
            "content": "Research AI definition", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="researcher",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute researcher agent node
    result_state = researcher_agent(state)
    
    # Verify researcher agent node integration
    assert result_state["research_results"] == ["Research completed"]
    assert result_state["current_research_index"] == 1
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_expert_agent_node_integration(mock_system_message, mock_get_conversation, 
                                     mock_convert, mock_validate, mock_compose_message, 
                                     mock_send_message, mock_datetime):
    """Test expert agent node integration with main graph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "expert", "type": "instruction", "content": "Analyze this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"expert_answer": "Analysis complete", "expert_reasoning": "Based on analysis"}
    
    # Mock response message
    mock_response_message = {
        "sender": "expert", "receiver": "orchestrator", "type": "response",
        "content": "Analysis completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create expert agent node
    mock_config = AgentConfig("expert", "openai", "gpt-4", 0.7, 
                             {"expert_answer": "string", "expert_reasoning": "string"}, 
                             "You are an expert agent", 5)
    expert_agent = create_expert_agent(mock_config, Mock())  # Mock compiled graph
    
    # Initialize test state
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "expert", "type": "instruction",
            "content": "Analyze AI research", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="expert",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=2,  # All research completed
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute expert agent node
    result_state = expert_agent(state)
    
    # Verify expert agent node integration
    assert result_state["expert_answer"] == "Analysis complete"
    assert result_state["expert_reasoning"] == "Based on analysis"
    assert result_state["research_results"] == ["AI is artificial intelligence"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_critic_agent_node_integration(mock_system_message, mock_get_conversation, 
                                     mock_convert, mock_validate, mock_compose_message, 
                                     mock_send_message, mock_datetime):
    """Test critic agent node integration with main graph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "critic", "type": "instruction", "content": "Review this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"decision": "approve", "feedback": "Good work"}
    
    # Mock response message
    mock_response_message = {
        "sender": "critic", "receiver": "orchestrator", "type": "response",
        "content": "Review completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create critic agent node
    mock_config = AgentConfig("critic", "openai", "gpt-4", 0.7, 
                             {"decision": "string", "feedback": "string"}, 
                             "You are a critic agent", 3)
    critic_agent = create_critic_agent(mock_config, mock_llm)
    
    # Initialize test state
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "critic", "type": "instruction",
            "content": "Review expert analysis", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="critic_expert",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=2,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="AI is artificial intelligence",
        expert_reasoning="Based on research analysis",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute critic agent node
    result_state = critic_agent(state)
    
    # Verify critic agent node integration
    assert result_state["critic_expert_decision"] == "approve"
    assert result_state["critic_expert_feedback"] == "Good work"
    assert result_state["expert_answer"] == "AI is artificial intelligence"
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_finalizer_agent_node_integration(mock_system_message, mock_get_conversation, 
                                        mock_convert, mock_validate, mock_compose_message, 
                                        mock_send_message, mock_datetime):
    """Test finalizer agent node integration with main graph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "finalizer", "type": "instruction", "content": "Finalize this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"final_answer": "AI is artificial intelligence", "final_reasoning_trace": "Complete analysis"}
    
    # Mock response message
    mock_response_message = {
        "sender": "finalizer", "receiver": "orchestrator", "type": "response",
        "content": "Finalization completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create finalizer agent node
    mock_config = AgentConfig("finalizer", "openai", "gpt-4", 0.7, 
                             {"final_answer": "string", "final_reasoning_trace": "string"}, 
                             "You are a finalizer agent", 3)
    finalizer_agent = create_finalizer_agent(mock_config, mock_llm)
    
    # Initialize test state
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "finalizer", "type": "instruction",
            "content": "Finalize AI analysis", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="finalizer",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=2,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="AI is artificial intelligence",
        expert_reasoning="Based on research analysis",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="approve",
        critic_researcher_feedback="",
        critic_expert_decision="approve",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute finalizer agent node
    result_state = finalizer_agent(state)
    
    # Verify finalizer agent node integration
    assert result_state["final_answer"] == "AI is artificial intelligence"
    assert result_state["final_reasoning_trace"] == "Complete analysis"
    assert result_state["expert_answer"] == "AI is artificial intelligence"
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_agent_node_workflow_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test agent node workflow integration with main graph."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "planner", "type": "instruction",
        "content": "Plan AI research", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with agent node workflow
    state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (workflow to planner agent node)
    result_state = execute_next_step(state)
    
    # Verify agent node workflow integration
    assert result_state["current_step"] == "planner"
    assert result_state["next_step"] == "planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="planner", type="instruction",
        content="Plan AI research"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_agent_node_error_handling_integration(mock_datetime):
    """Test agent node error handling and recovery integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test agent node error scenarios
    
    # Scenario 1: Planner agent node creation error
    with patch('multi_agent_system.create_planner_agent') as mock_create_planner:
        mock_create_planner.side_effect = Exception("Planner agent creation error")
        
        with pytest.raises(Exception):
            create_planner_agent(Mock(), Mock())
    
    # Scenario 2: Researcher agent node creation error
    with patch('multi_agent_system.create_researcher_agent') as mock_create_researcher:
        mock_create_researcher.side_effect = Exception("Researcher agent creation error")
        
        with pytest.raises(Exception):
            create_researcher_agent(Mock(), Mock())
    
    # Scenario 3: Expert agent node creation error
    with patch('multi_agent_system.create_expert_agent') as mock_create_expert:
        mock_create_expert.side_effect = Exception("Expert agent creation error")
        
        with pytest.raises(Exception):
            create_expert_agent(Mock(), Mock())

@patch('multi_agent_system.datetime')
def test_agent_node_performance_integration(mock_datetime):
    """Test performance aspects of agent node integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test agent node performance
    import time
    start_time = time.time()
    
    # Simulate multiple agent node operations
    with patch('multi_agent_system.create_planner_agent'), patch('multi_agent_system.create_researcher_agent'), patch('multi_agent_system.create_expert_agent'):
        
        for i in range(5):
            # Mock agent node operations for performance testing
            mock_llm = Mock()
            mock_config = AgentConfig("test", "openai", "gpt-4", 0.7, {}, "Test agent", 3)
            
            # Test agent node creation performance
            create_planner_agent(mock_config, mock_llm)
            create_researcher_agent(mock_config, Mock())
            create_expert_agent(mock_config, Mock())
    
    agent_node_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert agent_node_time < 1.0  # Should complete in less than 1 second

@patch('multi_agent_system.datetime')
def test_agent_node_state_management_integration(mock_datetime):
    """Test agent node state management and integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test agent node state management scenarios
    
    # Scenario 1: Planner agent node state
    planner_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="researcher",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify planner agent node state management
    assert planner_state["current_step"] == "planner"
    assert planner_state["next_step"] == "researcher"
    assert planner_state["research_steps"] == ["Research AI basics"]
    
    # Scenario 2: Researcher agent node state
    researcher_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="researcher",
        next_step="expert",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=1,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify researcher agent node state management
    assert researcher_state["current_step"] == "researcher"
    assert researcher_state["next_step"] == "expert"
    assert researcher_state["research_results"] == ["AI is artificial intelligence"]
    assert researcher_state["current_research_index"] == 1
```
#### 4.4.3 Orchestration Integration

**Integration Test Information:**
- **Test Name**: Orchestration Integration
- **Location**: Section 4.4.3
- **Purpose**: Tests how the orchestrator integrates with all components of the multi-agent system, including workflow coordination, state management, agent routing, retry logic, and end-to-end workflow execution across the entire system

**Test Scope and Boundaries:**
- **Components Involved**: 
  - `orchestrator()` - Main orchestrator function with workflow coordination
  - `execute_next_step()` - Step execution with orchestrator integration
  - `determine_next_step()` - Next step determination for orchestrator
  - `check_retry_limit()` - Retry limit checking for orchestrator
  - `route_from_orchestrator()` - Routing logic for orchestrator
  - `create_input_interface()` - Input interface with orchestrator integration
  - `create_multi_agent_graph()` - Main graph with orchestrator integration
  - `create_planner_agent()` - Planner agent with orchestrator coordination
  - `create_researcher_agent()` - Researcher agent with orchestrator coordination
  - `create_expert_agent()` - Expert agent with orchestrator coordination
  - `create_critic_agent()` - Critic agent with orchestrator coordination
  - `create_finalizer_agent()` - Finalizer agent with orchestrator coordination
  - `GraphState` - Main state container for orchestrator operations
  - `AgentMessage` - Message protocol for orchestrator communication
- **Integration Points**: 
  - Orchestrator integration with main graph execution
  - Orchestrator workflow coordination and state management
  - Orchestrator agent routing and selection
  - Orchestrator retry logic and error handling
  - Orchestrator message handling and communication
  - Orchestrator result integration and workflow completion
  - Orchestrator synchronization with all agent types
- **Data Flow**: 
  - Input triggers orchestrator workflow initiation
  - Orchestrator determines next step and routes to appropriate agent
  - Agent execution results flow back to orchestrator
  - Orchestrator manages state updates and workflow progression
  - Orchestrator coordinates retry logic and error recovery
  - Orchestrator handles workflow completion and final results
  - Orchestrator manages message flow between all components
- **State Management**: 
  - GraphState coordinates orchestrator state integration
  - Orchestrator state tracks workflow progression and agent coordination
  - State updates reflect orchestrator processing outcomes
  - Orchestrator state synchronization with all agent states
  - Retry state management for orchestrator failures
  - Orchestrator result state integration and workflow completion
- **Message Passing**: 
  - Orchestrator messages trigger agent execution
  - Agent results propagate through orchestrator message system
  - Orchestrator state updates communicate workflow outcomes
  - Orchestrator coordination messages manage workflow
  - Error messages handle orchestrator and agent failures

**Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock LLM responses for all agent types
  - Mock tool functions within agents
  - Use real orchestrator and execution functions
  - Mock agent factory functions
  - Use real graph creation and routing functions
  - Mock tool aggregation functions
- **Test Data**: 
  - Sample end-to-end workflow scenarios
  - Orchestrator execution test data
  - Agent coordination test data
  - Workflow state transition data
  - Retry and error handling test data
- **Environment Setup**: 
  - Initialize GraphState with complete workflow
  - Set up mock LLM environments for all agents
  - Configure tool functions for all agents
  - Prepare test orchestrator execution data
  - Set up mock agent factory functions
- **Dependencies**: 
  - Mock ChatOpenAI for all agent LLMs
  - Mock tool functions for agent integration
  - Mock StateGraph compilation
  - Real orchestrator and execution functions
  - Real graph creation and routing functions
- **Graph State Initialization**: 
  - Initialize GraphState with complete workflow
  - Set up all agent states and tools
  - Initialize orchestrator results storage
  - Configure orchestrator integration parameters

**Test Scenarios to Include:**
- **Happy Path Integration**: 
  - Complete end-to-end workflow integration
  - Multi-agent coordination through orchestrator
  - Orchestrator result integration into main graph
  - Orchestrator state management and updates
  - Orchestrator workflow completion and final results
- **Component Interaction Patterns**: 
  - Orchestrator-to-agent integration patterns
  - Orchestrator-to-main-graph integration patterns
  - Orchestrator coordination and routing patterns
  - Orchestrator tool integration patterns
  - Orchestrator error handling patterns
- **Data Transformation**: 
  - Orchestrator output transformation into workflow results
  - Planner-to-researcher data transformation
  - Researcher-to-expert data transformation
  - Expert-to-critic data transformation
  - Critic-to-finalizer data transformation
  - Finalizer result transformation and completion
- **Error Propagation**: 
  - Orchestrator failure handling and recovery
  - Agent failure propagation through orchestrator
  - Orchestrator retry logic and error recovery
  - Workflow timeout and retry scenarios
  - Orchestrator integration error recovery
- **State Synchronization**: 
  - Orchestrator state synchronization with main graph
  - Planner state management through orchestrator
  - Researcher state management through orchestrator
  - Expert state management through orchestrator
  - Critic state management through orchestrator
  - Finalizer state management through orchestrator
- **Message Flow**: 
  - Orchestrator-to-agent message flows
  - Agent execution message patterns
  - Agent result message propagation
  - Orchestrator error message handling
  - Workflow completion message flows
- **Retry Logic Integration**: 
  - Orchestrator retry mechanisms and state management
  - Agent failure recovery through orchestrator
  - Orchestrator timeout handling and retry
  - Orchestrator integration retry logic
  - Main graph retry coordination with orchestrator
- **Critic Decision Integration**: 
  - Orchestrator result quality assessment
  - Agent selection validation through orchestrator
  - Agent output validation and feedback
  - Orchestrator integration decision impact
  - Orchestrator coordination decisions

**Mock Configurations:**
- **Partial Mocking**: 
  - Mock LLM responses for all agents
  - Mock tool functions within agents
  - Use real orchestrator and execution functions
  - Mock agent factory functions
  - Use real graph creation and routing functions
- **Integration Boundaries**: 
  - Test orchestrator integration with real execution functions
  - Test orchestrator execution with mock LLMs and tools
  - Mock external dependencies (LLMs, tools)
- **Mock Data Setup**: 
  - Realistic end-to-end workflow scenarios
  - Sample planner workflow data
  - Researcher workflow data and tool results
  - Expert workflow data and analysis results
  - Critic workflow data and decision results
  - Finalizer workflow data and final results
- **Mock Behavior**: 
  - Consistent orchestrator execution patterns
  - Proper error simulation for orchestrator
  - Realistic orchestrator coordination behavior
  - Orchestrator timeout and retry simulation
  - Orchestrator workflow completion behavior
- **LLM Mocking**: 
  - Mock ChatOpenAI for all agent LLMs
  - Simulate orchestrator response patterns
  - Mock agent selection and coordination
  - Simulate orchestrator result processing
- **Tool Mocking**: 
  - Mock research tools for researcher agent
  - Mock expert tools for expert agent
  - Mock tool invocation and results
  - Mock tool integration within agents

**Assertion Specifications:**
- **Integration Verification**: 
  - Verify orchestrator integration with main graph
  - Verify orchestrator coordination and routing
  - Verify orchestrator result integration into workflow
  - Verify orchestrator state management
- **Data Flow Validation**: 
  - Validate orchestrator output transformation
  - Verify orchestrator result data flow
  - Check orchestrator coordination data flow
  - Validate orchestrator state updates
- **State Consistency**: 
  - Verify orchestrator state consistency
  - Check orchestrator state synchronization
  - Validate orchestrator result storage
  - Verify orchestrator workflow state
- **Error Handling**: 
  - Verify orchestrator error propagation
  - Check orchestrator failure recovery
  - Validate orchestrator retry mechanisms
  - Verify orchestrator integration error handling
- **Message Validation**: 
  - Verify orchestrator-to-agent messages
  - Check orchestrator execution messages
  - Validate orchestrator result messages
  - Verify orchestrator error messages
- **Graph State Validation**: 
  - Verify orchestrator result storage in GraphState
  - Check orchestrator state updates
  - Validate orchestrator workflow state
  - Verify orchestrator integration state
- **Performance Metrics**: 
  - Measure orchestrator integration performance
  - Check orchestrator coordination efficiency
  - Validate orchestrator result processing
  - Measure orchestrator state update efficiency

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    orchestrator, execute_next_step, determine_next_step, check_retry_limit,
    route_from_orchestrator, create_input_interface, create_multi_agent_graph,
    create_planner_agent, create_researcher_agent, create_expert_agent,
    create_critic_agent, create_finalizer_agent, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
@patch('multi_agent_system.validate_llm_response')
@patch('multi_agent_system.convert_agent_messages_to_langchain')
@patch('multi_agent_system.get_agent_conversation')
@patch('multi_agent_system.SystemMessage')
def test_orchestrator_workflow_integration(mock_system_message, mock_get_conversation, 
                                         mock_convert, mock_validate, mock_compose_message, 
                                         mock_send_message, mock_datetime):
    """Test orchestrator workflow integration with main graph."""
    # Setup mocks
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Mock LLM response
    mock_llm = Mock()
    mock_llm.invoke.return_value = Mock()
    
    # Mock agent conversation and conversion
    mock_get_conversation.return_value = [{"sender": "orchestrator", "receiver": "planner", "type": "instruction", "content": "Plan this"}]
    mock_convert.return_value = [Mock()]
    mock_validate.return_value = {"research_steps": ["Research AI basics"], "expert_steps": ["Analyze results"]}
    
    # Mock response message
    mock_response_message = {
        "sender": "planner", "receiver": "orchestrator", "type": "response",
        "content": "Planning completed", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_response_message
    mock_send_message.return_value = None
    
    # Create orchestrator with agent integration
    mock_config = AgentConfig("planner", "openai", "gpt-4", 0.7, 
                             {"research_steps": "list", "expert_steps": "list"}, 
                             "You are a planner agent", 3)
    planner_agent = create_planner_agent(mock_config, mock_llm)
    
    # Initialize test state with orchestrator workflow
    state = GraphState(
        agent_messages=[{
            "sender": "orchestrator", "receiver": "planner", "type": "instruction",
            "content": "Plan AI research", "timestamp": "2024-01-01T12:00:00"
        }],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute orchestrator workflow
    result_state = orchestrator(state)
    
    # Verify orchestrator workflow integration
    assert result_state["research_steps"] == ["Research AI basics"]
    assert result_state["expert_steps"] == ["Analyze results"]
    assert len(result_state["agent_messages"]) == 2  # Original + response
    mock_llm.invoke.assert_called_once()
    mock_send_message.assert_called_once()

@patch('multi_agent_system.datetime')
@patch('multi_agent_system.send_message')
@patch('multi_agent_system.compose_agent_message')
def test_orchestrator_step_execution_integration(mock_compose_message, mock_send_message, mock_datetime):
    """Test orchestrator step execution integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Setup mock message functions
    mock_instruction_message = {
        "sender": "orchestrator", "receiver": "planner", "type": "instruction",
        "content": "Plan AI research", "timestamp": "2024-01-01T12:00:00"
    }
    mock_compose_message.return_value = mock_instruction_message
    mock_send_message.return_value = None
    
    # Initialize test state with orchestrator step execution
    state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute next step (orchestrator step execution)
    result_state = execute_next_step(state)
    
    # Verify orchestrator step execution integration
    assert result_state["current_step"] == "planner"
    assert result_state["next_step"] == "planner"
    mock_compose_message.assert_called_once_with(
        sender="orchestrator", receiver="planner", type="instruction",
        content="Plan AI research"
    )
    mock_send_message.assert_called_once_with(state, mock_instruction_message)

@patch('multi_agent_system.datetime')
def test_orchestrator_next_step_determination_integration(mock_datetime):
    """Test orchestrator next step determination integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator next step determination scenarios
    
    # Scenario 1: Initial state - should go to planner
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_state = determine_next_step(initial_state)
    assert result_state["next_step"] == "planner"
    
    # Scenario 2: After planner - should go to researcher
    planner_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_state = determine_next_step(planner_state)
    assert result_state["next_step"] == "researcher"

@patch('multi_agent_system.datetime')
def test_orchestrator_retry_logic_integration(mock_datetime):
    """Test orchestrator retry logic integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator retry logic scenarios
    
    # Scenario 1: Under retry limit - should continue
    under_limit_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=1,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_state = check_retry_limit(under_limit_state)
    assert result_state["next_step"] == "planner"
    
    # Scenario 2: At retry limit - should stop
    at_limit_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="planner",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=3,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    result_state = check_retry_limit(at_limit_state)
    assert result_state["next_step"] == ""

@patch('multi_agent_system.datetime')
def test_orchestrator_routing_integration(mock_datetime):
    """Test orchestrator routing integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator routing scenarios
    
    # Scenario 1: Route to planner
    planner_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="planner",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    route = route_from_orchestrator(planner_state)
    assert route == "planner"
    
    # Scenario 2: Route to researcher
    researcher_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="researcher",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    route = route_from_orchestrator(researcher_state)
    assert route == "researcher"

@patch('multi_agent_system.datetime')
def test_orchestrator_input_interface_integration(mock_datetime):
    """Test orchestrator input interface integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator input interface integration
    
    # Create input interface with orchestrator integration
    mock_configs = {
        "planner": AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3),
        "researcher": AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5),
        "expert": AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5),
        "critic": AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3),
        "finalizer": AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    }
    
    input_interface = create_input_interface(mock_configs)
    
    # Initialize test state
    state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute input interface with orchestrator integration
    result_state = input_interface(state)
    
    # Verify orchestrator input interface integration
    assert result_state["question"] == "What is AI?"
    assert result_state["current_step"] == ""
    assert result_state["next_step"] == ""

@patch('multi_agent_system.datetime')
def test_orchestrator_error_handling_integration(mock_datetime):
    """Test orchestrator error handling and recovery integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator error scenarios
    
    # Scenario 1: Orchestrator step execution error
    with patch('multi_agent_system.execute_next_step') as mock_execute:
        mock_execute.side_effect = Exception("Orchestrator execution error")
        
        with pytest.raises(Exception):
            execute_next_step(Mock())
    
    # Scenario 2: Orchestrator routing error
    with patch('multi_agent_system.route_from_orchestrator') as mock_route:
        mock_route.side_effect = Exception("Orchestrator routing error")
        
        with pytest.raises(Exception):
            route_from_orchestrator(Mock())
    
    # Scenario 3: Orchestrator retry logic error
    with patch('multi_agent_system.check_retry_limit') as mock_retry:
        mock_retry.side_effect = Exception("Orchestrator retry error")
        
        with pytest.raises(Exception):
            check_retry_limit(Mock())

@patch('multi_agent_system.datetime')
def test_orchestrator_performance_integration(mock_datetime):
    """Test performance aspects of orchestrator integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator performance
    import time
    start_time = time.time()
    
    # Simulate multiple orchestrator operations
    with patch('multi_agent_system.determine_next_step'), patch('multi_agent_system.check_retry_limit'), patch('multi_agent_system.route_from_orchestrator'):
        
        for i in range(10):
            # Mock orchestrator operations for performance testing
            mock_state = Mock()
            
            # Test orchestrator operation performance
            determine_next_step(mock_state)
            check_retry_limit(mock_state)
            route_from_orchestrator(mock_state)
    
    orchestrator_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert orchestrator_time < 1.0  # Should complete in less than 1 second

@patch('multi_agent_system.datetime')
def test_orchestrator_state_management_integration(mock_datetime):
    """Test orchestrator state management and integration."""
    # Setup mock datetime
    mock_datetime.datetime.now.return_value = datetime(2024, 1, 1, 12, 0, 0)
    mock_datetime.datetime.now.return_value.isoformat.return_value = "2024-01-01T12:00:00"
    
    # Test orchestrator state management scenarios
    
    # Scenario 1: Orchestrator initial state
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify orchestrator initial state management
    assert initial_state["current_step"] == ""
    assert initial_state["next_step"] == ""
    assert initial_state["question"] == "What is AI?"
    
    # Scenario 2: Orchestrator workflow state
    workflow_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="researcher",
        next_step="expert",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze results"],
        researcher_states={},
        current_research_index=1,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="approve",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Verify orchestrator workflow state management
    assert workflow_state["current_step"] == "researcher"
    assert workflow_state["next_step"] == "expert"
    assert workflow_state["research_results"] == ["AI is artificial intelligence"]
    assert workflow_state["current_research_index"] == 1
```

## 5. System Tests
### 5.1 Complete Workflow Tests
#### 5.1.1 End-to-End Question Answering

**System Test Information:**
- **Test Name**: End-to-End Question Answering
- **Location**: Section 5.1.1
- **Purpose**: Tests the complete end-to-end workflow from user question input to final answer output, involving all agents (planner, researcher, expert, critic, finalizer) working together as a unified system to demonstrate the full multi-agent question answering capability

**Test Scope and System Boundaries:**
- **System Components Involved**: 
  - `main()` - Application entry point and system initialization
  - `create_multi_agent_graph()` - Complete graph creation with all agents
  - `orchestrator()` - Main orchestrator coordinating all agents
  - `create_planner_agent()` - Planner agent for workflow planning
  - `create_researcher_agent()` - Researcher agent with external tools
  - `create_expert_agent()` - Expert agent with computation tools
  - `create_critic_agent()` - Critic agent for quality assessment
  - `create_finalizer_agent()` - Finalizer agent for answer synthesis
  - `execute_next_step()` - Step execution across all agents
  - `determine_next_step()` - Workflow progression logic
  - `check_retry_limit()` - System-wide retry management
  - `GraphState` - Complete system state management
  - `AgentMessage` - System-wide message protocol
  - All research tools (YouTube, Wikipedia, web search, file loaders)
  - All expert tools (unit converter, calculator, Python REPL)
- **System Integration Points**: 
  - Complete graph integration with all agent nodes
  - Orchestrator coordination of all agent workflows
  - Agent-to-agent communication through message system
  - Tool integration across researcher and expert agents
  - State management across all system components
  - Error handling and retry logic across entire system
  - Workflow progression from input to final output
- **End-to-End Data Flow**: 
  - User question input triggers system initialization
  - Planner agent creates research and expert workflows
  - Researcher agent executes research steps with external tools
  - Expert agent analyzes research results with computation tools
  - Critic agent reviews and validates agent outputs
  - Finalizer agent synthesizes final answer and reasoning
  - Complete workflow state is maintained throughout
  - Final answer and reasoning trace are returned to user
- **System State Management**: 
  - GraphState manages complete system state across all agents
  - State transitions occur as workflow progresses through agents
  - Agent-specific states (ResearcherState, ExpertState) are integrated
  - Retry counts and limits are tracked for all agents
  - Message history is maintained for complete audit trail
  - Final results are stored in system state for output
- **System Message Flow**: 
  - Orchestrator messages trigger agent execution
  - Agent responses flow back through message system
  - Inter-agent communication occurs through AgentMessage protocol
  - Error messages propagate through system for handling
  - Completion messages signal workflow progression
  - Final answer messages complete the system workflow

**System Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock all LLM responses for all agents (planner, researcher, expert, critic, finalizer)
  - Mock all external services (YouTube API, Wikipedia API, web search, file loaders)
  - Mock all computation tools (unit converter, calculator, Python REPL)
  - Use real system components (orchestrator, agents, state management, message system)
  - Mock file system operations for document loading
  - Mock browser MCP client for web interactions
- **System Test Data**: 
  - Sample user questions covering different complexity levels
  - Mock research data (YouTube transcripts, Wikipedia content, web search results)
  - Mock computation scenarios (unit conversions, calculations, code execution)
  - Mock critic review scenarios (approve/reject decisions with feedback)
  - Mock finalization scenarios (answer synthesis and reasoning traces)
  - Complete workflow state data for end-to-end testing
- **System Environment Setup**: 
  - Initialize complete multi-agent graph with all components
  - Set up mock external service environments
  - Configure all agent configurations and tools
  - Prepare complete system state for workflow execution
  - Set up mock file system and document loading
  - Configure system-wide retry limits and error handling
- **External Dependencies**: 
  - Mock ChatOpenAI for all agent LLMs
  - Mock YouTube Transcript API
  - Mock Wikipedia Query API
  - Mock Tavily web search API
  - Mock file system operations
  - Mock browser MCP client
  - Mock computation libraries (pint, math, Python REPL)
- **System State Initialization**: 
  - Initialize GraphState with complete workflow parameters
  - Set up all agent configurations and retry limits
  - Initialize message history and agent states
  - Configure tool integrations for researcher and expert agents
  - Set up critic review parameters and finalization settings

**System Test Scenarios to Include:**
- **Happy Path System Test**: 
  - Complete end-to-end question answering with all agents working correctly
  - Successful research execution with external tools
  - Successful expert analysis with computation tools
  - Successful critic review and approval
  - Successful finalization with complete answer and reasoning
  - Proper state management throughout entire workflow
- **System Component Interaction Patterns**: 
  - Planner-to-researcher workflow coordination
  - Researcher-to-expert data handoff and analysis
  - Expert-to-critic review and validation
  - Critic-to-finalizer approval and synthesis
  - Orchestrator coordination of all agent interactions
  - Tool integration patterns across researcher and expert agents
- **System Data Transformation**: 
  - User question transformation into research steps
  - Research results transformation into expert analysis
  - Expert analysis transformation into critic review
  - Critic feedback transformation into final answer
  - Complete data flow validation across all agents
  - State transformation and progression throughout workflow
- **System Error Propagation**: 
  - Agent failure handling and retry mechanisms
  - External service failure recovery
  - Tool execution error handling
  - Communication breakdown recovery
  - System degradation handling and graceful failure
  - Error propagation through complete workflow
- **System State Synchronization**: 
  - State consistency across all agents during workflow
  - State transitions and progression validation
  - Agent state synchronization with main system state
  - State recovery mechanisms for failures
  - State persistence and restoration capabilities
  - Global state management across entire system
- **System Message Flow**: 
  - Complete message flow from orchestrator to all agents
  - Agent response message propagation
  - Inter-agent communication validation
  - Error message handling and propagation
  - Message history maintenance and audit trail
  - Final answer message completion
- **System Retry Logic**: 
  - Retry mechanisms for all agent types
  - Retry limit enforcement across system
  - Retry state management and recovery
  - Retry coordination between agents
  - Retry failure handling and system degradation
  - Retry performance and efficiency validation
- **System Decision Making**: 
  - Planner decision making for workflow creation
  - Researcher decision making for research execution
  - Expert decision making for analysis approach
  - Critic decision making for quality assessment
  - Finalizer decision making for answer synthesis
  - System-level decision emergence from agent interactions

**System Mock Configurations:**
- **Complete System Mocking**: 
  - Mock all external LLM calls for deterministic responses
  - Mock all external service APIs for controlled data
  - Mock all computation tools for predictable results
  - Use real system orchestration and state management
  - Mock file system operations for document handling
  - Mock browser interactions for web research
- **System Integration Boundaries**: 
  - Test real system components with mocked external dependencies
  - Maintain real agent coordination and communication
  - Use real state management and message protocols
  - Mock only external services and LLM responses
  - Preserve real workflow orchestration and routing
- **System Mock Data Setup**: 
  - Realistic LLM responses for each agent type
  - Comprehensive external service response data
  - Varied computation tool result scenarios
  - Different critic review and decision scenarios
  - Multiple finalization and synthesis scenarios
  - Complete workflow state progression data
- **System Mock Behavior**: 
  - Consistent LLM response patterns for each agent
  - Realistic external service behavior and error simulation
  - Predictable computation tool execution
  - Varied critic decision patterns and feedback
  - Reliable finalization behavior and output generation
  - System-wide error simulation and recovery
- **LLM Mocking for System**: 
  - Mock ChatOpenAI for all agent types with specific responses
  - Simulate realistic agent behavior and decision making
  - Provide consistent response patterns for workflow progression
  - Mock error scenarios and retry responses
  - Simulate agent coordination and communication
  - Mock final answer generation and reasoning synthesis
- **External Service Mocking**: 
  - Mock YouTube API with realistic transcript data
  - Mock Wikipedia API with search result content
  - Mock web search API with relevant search results
  - Mock file loaders with document content
  - Mock browser MCP with web interaction data
  - Mock computation tools with calculation results

**System Assertion Specifications:**
- **System Integration Verification**: 
  - Verify complete system workflow execution
  - Verify all agent coordination and communication
  - Verify tool integration across researcher and expert agents
  - Verify orchestrator coordination of all components
  - Verify state management across entire system
  - Verify message flow through complete workflow
- **System Data Flow Validation**: 
  - Validate data transformation through all agents
  - Validate research results integration into expert analysis
  - Validate expert analysis integration into critic review
  - Validate critic feedback integration into final answer
  - Validate complete data flow from input to output
  - Validate state progression throughout workflow
- **System State Consistency**: 
  - Verify state consistency across all agents
  - Verify state transitions and progression
  - Verify agent state synchronization with main state
  - Verify state recovery and persistence
  - Verify global state management
  - Verify state integrity throughout workflow
- **System Error Handling**: 
  - Verify error propagation through system
  - Verify retry mechanisms for all agents
  - Verify error recovery and system resilience
  - Verify graceful degradation handling
  - Verify error message propagation
  - Verify system stability under error conditions
- **System Message Validation**: 
  - Verify complete message flow through system
  - Verify agent communication protocols
  - Verify message history and audit trail
  - Verify error message handling
  - Verify completion message flow
  - Verify final answer message generation
- **System State Validation**: 
  - Verify GraphState updates across all agents
  - Verify workflow state progression
  - Verify agent state integration
  - Verify retry state management
  - Verify final state completion
  - Verify state consistency throughout workflow
- **System Performance Metrics**: 
  - Measure complete workflow execution time
  - Measure agent coordination efficiency
  - Measure state management performance
  - Measure message flow efficiency
  - Measure error handling performance
  - Measure system resource utilization

**System Test Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    main, create_multi_agent_graph, orchestrator, execute_next_step,
    create_planner_agent, create_researcher_agent, create_expert_agent,
    create_critic_agent, create_finalizer_agent, determine_next_step,
    check_retry_limit, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
@patch('multi_agent_system.UnstructuredPDFLoader')
@patch('multi_agent_system.pint')
@patch('multi_agent_system.math')
@patch('multi_agent_system.PythonREPLTool')
def test_complete_end_to_end_question_answering(mock_repl, mock_math, mock_pint, 
                                               mock_pdf_loader, mock_tavily, 
                                               mock_wikipedia, mock_youtube, mock_llm):
    """Test complete end-to-end question answering workflow."""
    # Setup comprehensive mocks for all external dependencies
    
    # Mock LLM responses for all agents in sequence
    mock_llm.return_value.invoke.side_effect = [
        # Planner agent response
        Mock(content='{"research_steps": ["Research AI definition", "Research AI applications"], "expert_steps": ["Analyze AI definition", "Analyze AI impact"]}'),
        # Researcher agent response (first research step)
        Mock(content='{"result": "AI is artificial intelligence that enables machines to learn and perform tasks"}'),
        # Researcher agent response (second research step)
        Mock(content='{"result": "AI has applications in healthcare, finance, transportation, and entertainment"}'),
        # Expert agent response (first expert step)
        Mock(content='{"expert_answer": "AI is artificial intelligence", "expert_reasoning": "Based on research, AI enables machines to learn and perform tasks"}'),
        # Expert agent response (second expert step)
        Mock(content='{"expert_answer": "AI has significant impact across industries", "expert_reasoning": "Research shows AI applications in healthcare, finance, transportation, and entertainment"}'),
        # Critic agent response (planner review)
        Mock(content='{"decision": "approve", "feedback": "Good planning with clear research and expert steps"}'),
        # Critic agent response (researcher review)
        Mock(content='{"decision": "approve", "feedback": "Comprehensive research covering definition and applications"}'),
        # Critic agent response (expert review)
        Mock(content='{"decision": "approve", "feedback": "Thorough analysis with clear reasoning"}'),
        # Finalizer agent response
        Mock(content='{"final_answer": "AI is artificial intelligence that enables machines to learn and perform tasks, with significant applications across healthcare, finance, transportation, and entertainment.", "final_reasoning_trace": "Complete analysis based on research and expert evaluation"}')
    ]
    
    # Mock external service responses
    mock_youtube.get_transcript.return_value = [{"text": "AI is artificial intelligence that enables machines to learn"}]
    mock_wikipedia.return_value.run.return_value = "Artificial Intelligence (AI) is intelligence demonstrated by machines"
    mock_tavily.return_value.invoke.return_value = [{"content": "AI applications include healthcare, finance, and transportation"}]
    mock_pdf_loader.return_value.load.return_value = [Mock(page_content="AI research document content")]
    
    # Mock computation tool responses
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.magnitude = 1000.0
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.units = "meter"
    mock_math.eval.return_value = 25.0
    mock_repl.return_value.invoke.return_value = "Result: 42"
    
    # Create complete multi-agent system
    agent_configs = {
        "planner": AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3),
        "researcher": AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5),
        "expert": AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5),
        "critic": AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3),
        "finalizer": AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    }
    
    # Create complete multi-agent graph
    graph = create_multi_agent_graph(agent_configs)
    
    # Initialize complete system state
    initial_state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence and what are its applications?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute complete end-to-end workflow
    final_state = graph.invoke(initial_state)
    
    # Verify complete system workflow execution
    assert final_state["final_answer"] == "AI is artificial intelligence that enables machines to learn and perform tasks, with significant applications across healthcare, finance, transportation, and entertainment."
    assert final_state["final_reasoning_trace"] == "Complete analysis based on research and expert evaluation"
    assert final_state["research_steps"] == ["Research AI definition", "Research AI applications"]
    assert final_state["expert_steps"] == ["Analyze AI definition", "Analyze AI impact"]
    assert len(final_state["research_results"]) == 2
    assert final_state["expert_answer"] == "AI has significant impact across industries"
    assert final_state["critic_planner_decision"] == "approve"
    assert final_state["critic_researcher_decision"] == "approve"
    assert final_state["critic_expert_decision"] == "approve"
    
    # Verify system state consistency
    assert final_state["current_step"] == "finalizer"
    assert final_state["planner_retry_count"] == 0
    assert final_state["researcher_retry_count"] == 0
    assert final_state["expert_retry_count"] == 0
    
    # Verify message flow through system
    assert len(final_state["agent_messages"]) > 0
    
    # Verify all external dependencies were called
    mock_llm.assert_called()
    mock_youtube.get_transcript.assert_called()
    mock_wikipedia.return_value.run.assert_called()
    mock_tavily.return_value.invoke.assert_called()

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_end_to_end_workflow_with_retry_logic(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test end-to-end workflow with retry logic and error handling."""
    # Setup mocks with some failures to test retry logic
    mock_llm.return_value.invoke.side_effect = [
        # First planner attempt fails
        Exception("LLM API error"),
        # Second planner attempt succeeds
        Mock(content='{"research_steps": ["Research AI basics"], "expert_steps": ["Analyze results"]}'),
        # Researcher succeeds
        Mock(content='{"result": "AI is artificial intelligence"}'),
        # Expert succeeds
        Mock(content='{"expert_answer": "AI is artificial intelligence", "expert_reasoning": "Based on research"}'),
        # Critic succeeds
        Mock(content='{"decision": "approve", "feedback": "Good work"}'),
        # Finalizer succeeds
        Mock(content='{"final_answer": "AI is artificial intelligence", "final_reasoning_trace": "Complete analysis"}')
    ]
    
    # Mock external services
    mock_youtube.get_transcript.return_value = [{"text": "AI transcript content"}]
    mock_wikipedia.return_value.run.return_value = "Wikipedia AI content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Web search AI content"}]
    
    # Create system and execute with retry logic
    agent_configs = {
        "planner": AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3),
        "researcher": AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5),
        "expert": AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5),
        "critic": AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3),
        "finalizer": AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    }
    
    graph = create_multi_agent_graph(agent_configs)
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute workflow with retry logic
    final_state = graph.invoke(initial_state)
    
    # Verify retry logic worked
    assert final_state["final_answer"] == "AI is artificial intelligence"
    assert final_state["planner_retry_count"] == 1  # One retry occurred
    assert final_state["critic_planner_decision"] == "approve"
    
    # Verify system recovered and completed successfully
    assert len(final_state["research_results"]) == 1
    assert final_state["expert_answer"] == "AI is artificial intelligence"

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_end_to_end_workflow_performance(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test end-to-end workflow performance and efficiency."""
    import time
    
    # Setup mocks for performance testing
    mock_llm.return_value.invoke.return_value = Mock(content='{"result": "Test result"}')
    mock_youtube.get_transcript.return_value = [{"text": "Test transcript"}]
    mock_wikipedia.return_value.run.return_value = "Test Wikipedia content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Test web content"}]
    
    # Create system
    agent_configs = {
        "planner": AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3),
        "researcher": AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5),
        "expert": AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5),
        "critic": AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3),
        "finalizer": AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    }
    
    graph = create_multi_agent_graph(agent_configs)
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Measure performance
    start_time = time.time()
    final_state = graph.invoke(initial_state)
    execution_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert execution_time < 5.0  # Should complete in less than 5 seconds
    assert final_state["final_answer"] == "Test result"
    
    # Verify system efficiency
    assert len(final_state["agent_messages"]) > 0
    assert final_state["planner_retry_count"] == 0
    assert final_state["researcher_retry_count"] == 0
    assert final_state["expert_retry_count"] == 0
```
#### 5.1.2 Research and Expert Workflow

**System Test Information:**
- **Test Name**: Research and Expert Workflow
- **Location**: Section 5.1.2
- **Purpose**: Tests the complete research and expert workflow integration within the multi-agent system, focusing on the seamless coordination between researcher agents (with external research tools) and expert agents (with computation tools) to demonstrate how research findings are transformed into expert analysis through system-wide collaboration

**Test Scope and System Boundaries:**
- **System Components Involved**: 
  - `create_researcher_agent()` - Researcher agent with external research tools
  - `create_expert_agent()` - Expert agent with computation tools
  - `create_researcher_subgraph()` - Researcher subgraph with research tools
  - `create_expert_subgraph()` - Expert subgraph with computation tools
  - `create_researcher_llm_node()` - Researcher LLM node for research execution
  - `create_expert_llm_node()` - Expert LLM node for analysis execution
  - `get_research_tools()` - Research tool integration (YouTube, Wikipedia, web search, file loaders)
  - `get_expert_tools()` - Expert tool integration (unit converter, calculator, Python REPL)
  - `ResearcherState` - Researcher-specific state management
  - `ExpertState` - Expert-specific state management
  - `GraphState` - Complete system state integration
  - `orchestrator()` - System coordination between research and expert workflows
  - `execute_next_step()` - Step execution across research and expert phases
  - `determine_next_step()` - Workflow progression between research and expert phases
- **System Integration Points**: 
  - Researcher subgraph integration with main system graph
  - Expert subgraph integration with main system graph
  - Research tool integration within researcher workflow
  - Expert tool integration within expert workflow
  - State handoff between researcher and expert phases
  - Message flow between researcher and expert agents
  - Orchestrator coordination of research-to-expert transitions
  - Data flow from research results to expert analysis
- **End-to-End Data Flow**: 
  - User question triggers research phase initialization
  - Planner creates research steps for researcher agent
  - Researcher agent executes research steps using external tools
  - Research results are collected and stored in system state
  - Research completion triggers expert phase initialization
  - Expert agent analyzes research results using computation tools
  - Expert analysis is integrated into system state
  - Complete research-to-expert workflow state is maintained
  - Final expert answer and reasoning are prepared for critic review
- **System State Management**: 
  - GraphState manages complete research and expert workflow state
  - ResearcherState manages individual research step execution
  - ExpertState manages expert analysis and reasoning
  - State transitions occur between research and expert phases
  - Research results are preserved and passed to expert phase
  - Expert analysis is integrated back into main system state
  - Retry counts and limits are tracked for both phases
  - Message history is maintained for complete audit trail
- **System Message Flow**: 
  - Orchestrator messages trigger research phase execution
  - Researcher agent messages flow through research subgraph
  - Research completion messages trigger expert phase
  - Expert agent messages flow through expert subgraph
  - Inter-agent communication occurs through AgentMessage protocol
  - Error messages propagate through both phases
  - Completion messages signal phase transitions
  - Final expert messages complete the research-expert workflow

**System Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock all LLM responses for researcher and expert agents
  - Mock all external research services (YouTube API, Wikipedia API, web search, file loaders)
  - Mock all computation tools (unit converter, calculator, Python REPL)
  - Use real system components (orchestrator, subgraphs, state management, message system)
  - Mock file system operations for document loading
  - Mock browser MCP client for web research
- **System Test Data**: 
  - Sample research questions requiring external data gathering
  - Mock research data (YouTube transcripts, Wikipedia content, web search results, documents)
  - Mock computation scenarios (unit conversions, calculations, code execution)
  - Research-to-expert workflow state progression data
  - Multiple research step scenarios with varying complexity
  - Expert analysis scenarios with different computation requirements
  - Complete workflow state data for research-expert integration testing
- **System Environment Setup**: 
  - Initialize complete research and expert subgraphs
  - Set up mock external research service environments
  - Configure researcher and expert agent configurations
  - Prepare complete system state for research-expert workflow
  - Set up mock file system and document loading
  - Configure research and expert retry limits and error handling
- **External Dependencies**: 
  - Mock ChatOpenAI for researcher and expert agent LLMs
  - Mock YouTube Transcript API for video research
  - Mock Wikipedia Query API for knowledge research
  - Mock Tavily web search API for web research
  - Mock file loaders (PDF, Excel, PowerPoint, text)
  - Mock browser MCP client for web interactions
  - Mock computation libraries (pint, math, Python REPL)
- **System State Initialization**: 
  - Initialize GraphState with research and expert workflow parameters
  - Set up researcher and expert agent configurations and retry limits
  - Initialize research and expert states and message history
  - Configure research and expert tool integrations
  - Set up research-to-expert transition parameters
  - Prepare research steps and expert analysis requirements

**System Test Scenarios to Include:**
- **Happy Path System Test**: 
  - Complete research-to-expert workflow with all components working correctly
  - Successful research execution with multiple external tools
  - Successful expert analysis with multiple computation tools
  - Proper state handoff between research and expert phases
  - Complete data flow from research results to expert analysis
  - Proper state management throughout research-expert workflow
- **System Component Interaction Patterns**: 
  - Researcher-to-expert workflow coordination
  - Research tool integration within researcher subgraph
  - Expert tool integration within expert subgraph
  - State synchronization between research and expert phases
  - Message flow between researcher and expert agents
  - Orchestrator coordination of research-expert transitions
- **System Data Transformation**: 
  - Research question transformation into research steps
  - Research results transformation into expert analysis
  - External data transformation into structured research results
  - Computation results transformation into expert reasoning
  - Complete data flow validation from research to expert
  - State transformation and progression throughout workflow
- **System Error Propagation**: 
  - Research tool failure handling and recovery
  - Expert tool failure handling and recovery
  - Research-to-expert transition error handling
  - External service failure recovery
  - Computation error handling and recovery
  - Error propagation through research-expert workflow
- **System State Synchronization**: 
  - State consistency between research and expert phases
  - Research state synchronization with main system state
  - Expert state synchronization with main system state
  - State recovery mechanisms for research-expert failures
  - State persistence and restoration capabilities
  - Global state management across research-expert workflow
- **System Message Flow**: 
  - Complete message flow through research subgraph
  - Complete message flow through expert subgraph
  - Research-to-expert message handoff
  - Inter-agent communication validation
  - Error message handling and propagation
  - Message history maintenance and audit trail
- **System Retry Logic**: 
  - Retry mechanisms for research agent
  - Retry mechanisms for expert agent
  - Retry limit enforcement across research-expert workflow
  - Retry state management and recovery
  - Retry coordination between research and expert phases
  - Retry failure handling and workflow degradation
- **System Decision Making**: 
  - Researcher decision making for research execution
  - Expert decision making for analysis approach
  - Research-to-expert transition decision making
  - Tool selection decision making within each phase
  - System-level decision emergence from research-expert interactions

**System Mock Configurations:**
- **Complete System Mocking**: 
  - Mock all external research service calls for controlled data
  - Mock all computation tool calls for predictable results
  - Mock all LLM calls for deterministic responses
  - Use real system orchestration and state management
  - Mock file system operations for document handling
  - Mock browser interactions for web research
- **System Integration Boundaries**: 
  - Test real system components with mocked external dependencies
  - Maintain real research and expert subgraph coordination
  - Use real state management and message protocols
  - Mock only external services and LLM responses
  - Preserve real research-to-expert workflow orchestration
- **System Mock Data Setup**: 
  - Realistic LLM responses for researcher and expert agents
  - Comprehensive external research service response data
  - Varied computation tool result scenarios
  - Multiple research step execution scenarios
  - Different expert analysis scenarios
  - Complete research-to-expert workflow state progression data
- **System Mock Behavior**: 
  - Consistent LLM response patterns for researcher and expert agents
  - Realistic external research service behavior and error simulation
  - Predictable computation tool execution
  - Varied research tool behavior and response patterns
  - Reliable expert tool behavior and output generation
  - System-wide error simulation and recovery
- **LLM Mocking for System**: 
  - Mock ChatOpenAI for researcher and expert agents with specific responses
  - Simulate realistic research execution behavior
  - Simulate realistic expert analysis behavior
  - Provide consistent response patterns for workflow progression
  - Mock error scenarios and retry responses
  - Simulate research-to-expert coordination and communication
- **External Service Mocking**: 
  - Mock YouTube API with realistic transcript data
  - Mock Wikipedia API with search result content
  - Mock web search API with relevant search results
  - Mock file loaders with document content
  - Mock browser MCP with web interaction data
  - Mock computation tools with calculation results

**System Assertion Specifications:**
- **System Integration Verification**: 
  - Verify complete research-to-expert workflow execution
  - Verify research and expert subgraph coordination
  - Verify research tool integration within researcher workflow
  - Verify expert tool integration within expert workflow
  - Verify orchestrator coordination of research-expert transitions
  - Verify state management across research-expert workflow
- **System Data Flow Validation**: 
  - Validate research results integration into expert analysis
  - Validate external data transformation through research tools
  - Validate computation results integration into expert reasoning
  - Validate complete data flow from research to expert
  - Validate state progression throughout research-expert workflow
  - Validate research-to-expert data handoff
- **System State Consistency**: 
  - Verify state consistency between research and expert phases
  - Verify research state synchronization with main state
  - Verify expert state synchronization with main state
  - Verify state recovery and persistence
  - Verify global state management across research-expert workflow
  - Verify state integrity throughout workflow
- **System Error Handling**: 
  - Verify error propagation through research-expert workflow
  - Verify retry mechanisms for research and expert agents
  - Verify error recovery and system resilience
  - Verify graceful degradation handling
  - Verify error message propagation
  - Verify system stability under error conditions
- **System Message Validation**: 
  - Verify complete message flow through research subgraph
  - Verify complete message flow through expert subgraph
  - Verify research-to-expert message handoff
  - Verify agent communication protocols
  - Verify message history and audit trail
  - Verify error message handling
- **System State Validation**: 
  - Verify GraphState updates across research-expert workflow
  - Verify research state progression and completion
  - Verify expert state progression and completion
  - Verify retry state management
  - Verify final state completion
  - Verify state consistency throughout workflow
- **System Performance Metrics**: 
  - Measure research-to-expert workflow execution time
  - Measure research and expert coordination efficiency
  - Measure state management performance
  - Measure message flow efficiency
  - Measure error handling performance
  - Measure system resource utilization

**System Test Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    create_researcher_agent, create_expert_agent, create_researcher_subgraph,
    create_expert_subgraph, create_researcher_llm_node, create_expert_llm_node,
    get_research_tools, get_expert_tools, orchestrator, execute_next_step,
    determine_next_step, GraphState, ResearcherState, ExpertState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
@patch('multi_agent_system.UnstructuredPDFLoader')
@patch('multi_agent_system.pint')
@patch('multi_agent_system.math')
@patch('multi_agent_system.PythonREPLTool')
def test_complete_research_expert_workflow(mock_repl, mock_math, mock_pint, 
                                         mock_pdf_loader, mock_tavily, 
                                         mock_wikipedia, mock_youtube, mock_llm):
    """Test complete research-to-expert workflow integration."""
    # Setup comprehensive mocks for research and expert workflows
    
    # Mock LLM responses for researcher and expert agents
    mock_llm.return_value.invoke.side_effect = [
        # Researcher agent response (first research step)
        Mock(content='{"result": "Machine learning is a subset of AI that enables computers to learn from data"}'),
        # Researcher agent response (second research step)
        Mock(content='{"result": "Deep learning uses neural networks with multiple layers for complex pattern recognition"}'),
        # Expert agent response (first expert step)
        Mock(content='{"expert_answer": "Machine learning enables data-driven AI", "expert_reasoning": "Research shows ML is a subset of AI that learns from data"}'),
        # Expert agent response (second expert step)
        Mock(content='{"expert_answer": "Deep learning provides advanced pattern recognition", "expert_reasoning": "Research shows deep learning uses neural networks with multiple layers"}')
    ]
    
    # Mock external research service responses
    mock_youtube.get_transcript.return_value = [{"text": "Machine learning enables computers to learn from data"}]
    mock_wikipedia.return_value.run.return_value = "Deep learning is a subset of machine learning using neural networks"
    mock_tavily.return_value.invoke.return_value = [{"content": "AI and machine learning applications in modern technology"}]
    mock_pdf_loader.return_value.load.return_value = [Mock(page_content="Research document on AI and ML")]
    
    # Mock computation tool responses
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.magnitude = 1000.0
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.units = "meter"
    mock_math.eval.return_value = 25.0
    mock_repl.return_value.invoke.return_value = "Result: 42"
    
    # Create researcher and expert agent configurations
    researcher_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5)
    expert_config = AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5)
    
    # Create research and expert subgraphs
    research_tools = get_research_tools()
    expert_tools = get_expert_tools()
    
    researcher_llm_node = create_researcher_llm_node(researcher_config, mock_llm.return_value)
    expert_llm_node = create_expert_llm_node(expert_config, mock_llm.return_value)
    
    researcher_subgraph = create_researcher_subgraph(researcher_llm_node, research_tools)
    expert_subgraph = create_expert_subgraph(expert_llm_node, expert_tools)
    
    # Create researcher and expert agents
    researcher_agent = create_researcher_agent(researcher_config, researcher_subgraph.compile())
    expert_agent = create_expert_agent(expert_config, expert_subgraph.compile())
    
    # Initialize complete system state for research-expert workflow
    initial_state = GraphState(
        agent_messages=[],
        question="What is machine learning and how does deep learning work?",
        current_step="",
        next_step="",
        research_steps=["Research machine learning basics", "Research deep learning concepts"],
        expert_steps=["Analyze machine learning definition", "Analyze deep learning architecture"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute research phase
    research_state = researcher_agent(initial_state)
    
    # Verify research phase completion
    assert len(research_state["research_results"]) == 2
    assert research_state["research_results"][0] == "Machine learning is a subset of AI that enables computers to learn from data"
    assert research_state["research_results"][1] == "Deep learning uses neural networks with multiple layers for complex pattern recognition"
    assert research_state["current_research_index"] == 2
    
    # Execute expert phase
    expert_state = expert_agent(research_state)
    
    # Verify expert phase completion
    assert expert_state["expert_answer"] == "Deep learning provides advanced pattern recognition"
    assert expert_state["expert_reasoning"] == "Research shows deep learning uses neural networks with multiple layers"
    assert len(expert_state["research_results"]) == 2  # Research results preserved
    
    # Verify complete research-expert workflow integration
    assert expert_state["current_step"] == "expert"
    assert expert_state["researcher_retry_count"] == 0
    assert expert_state["expert_retry_count"] == 0
    
    # Verify message flow through research-expert workflow
    assert len(expert_state["agent_messages"]) > 0
    
    # Verify all external dependencies were called
    mock_llm.assert_called()
    mock_youtube.get_transcript.assert_called()
    mock_wikipedia.return_value.run.assert_called()
    mock_tavily.return_value.invoke.assert_called()

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_research_expert_workflow_with_retry_logic(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test research-to-expert workflow with retry logic and error handling."""
    # Setup mocks with some failures to test retry logic
    mock_llm.return_value.invoke.side_effect = [
        # First researcher attempt fails
        Exception("Research API error"),
        # Second researcher attempt succeeds
        Mock(content='{"result": "Machine learning enables computers to learn"}'),
        # Expert succeeds
        Mock(content='{"expert_answer": "ML enables learning", "expert_reasoning": "Based on research"}')
    ]
    
    # Mock external services
    mock_youtube.get_transcript.return_value = [{"text": "ML transcript content"}]
    mock_wikipedia.return_value.run.return_value = "Wikipedia ML content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Web search ML content"}]
    
    # Create system and execute with retry logic
    researcher_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5)
    expert_config = AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5)
    
    research_tools = get_research_tools()
    expert_tools = get_expert_tools()
    
    researcher_llm_node = create_researcher_llm_node(researcher_config, mock_llm.return_value)
    expert_llm_node = create_expert_llm_node(expert_config, mock_llm.return_value)
    
    researcher_subgraph = create_researcher_subgraph(researcher_llm_node, research_tools)
    expert_subgraph = create_expert_subgraph(expert_llm_node, expert_tools)
    
    researcher_agent = create_researcher_agent(researcher_config, researcher_subgraph.compile())
    expert_agent = create_expert_agent(expert_config, expert_subgraph.compile())
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is machine learning?",
        current_step="",
        next_step="",
        research_steps=["Research ML basics"],
        expert_steps=["Analyze ML definition"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute research-expert workflow with retry logic
    research_state = researcher_agent(initial_state)
    expert_state = expert_agent(research_state)
    
    # Verify retry logic worked
    assert expert_state["expert_answer"] == "ML enables learning"
    assert expert_state["researcher_retry_count"] == 1  # One retry occurred
    assert len(expert_state["research_results"]) == 1
    
    # Verify system recovered and completed successfully
    assert expert_state["expert_reasoning"] == "Based on research"

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_research_expert_workflow_performance(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test research-to-expert workflow performance and efficiency."""
    import time
    
    # Setup mocks for performance testing
    mock_llm.return_value.invoke.return_value = Mock(content='{"result": "Test research result"}')
    mock_youtube.get_transcript.return_value = [{"text": "Test transcript"}]
    mock_wikipedia.return_value.run.return_value = "Test Wikipedia content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Test web content"}]
    
    # Create system
    researcher_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5)
    expert_config = AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5)
    
    research_tools = get_research_tools()
    expert_tools = get_expert_tools()
    
    researcher_llm_node = create_researcher_llm_node(researcher_config, mock_llm.return_value)
    expert_llm_node = create_expert_llm_node(expert_config, mock_llm.return_value)
    
    researcher_subgraph = create_researcher_subgraph(researcher_llm_node, research_tools)
    expert_subgraph = create_expert_subgraph(expert_llm_node, expert_tools)
    
    researcher_agent = create_researcher_agent(researcher_config, researcher_subgraph.compile())
    expert_agent = create_expert_agent(expert_config, expert_subgraph.compile())
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze AI definition"],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Measure performance
    start_time = time.time()
    research_state = researcher_agent(initial_state)
    expert_state = expert_agent(research_state)
    execution_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert execution_time < 3.0  # Should complete in less than 3 seconds
    assert expert_state["expert_answer"] == "Test research result"
    
    # Verify system efficiency
    assert len(expert_state["agent_messages"]) > 0
    assert expert_state["researcher_retry_count"] == 0
    assert expert_state["expert_retry_count"] == 0
```
#### 5.1.3 Critic Review Workflow

**System Test Information:**
- **Test Name**: Critic Review Workflow
- **Location**: Section 5.1.3
- **Purpose**: Tests the complete critic review workflow integration within the multi-agent system, focusing on the systematic quality assessment and validation of all agent outputs (planner, researcher, expert) by the critic agent, demonstrating how the system ensures quality control and feedback loops across the entire workflow

**Test Scope and System Boundaries:**
- **System Components Involved**: 
  - `create_critic_agent()` - Critic agent for quality assessment and validation
  - `create_planner_agent()` - Planner agent whose output is reviewed by critic
  - `create_researcher_agent()` - Researcher agent whose output is reviewed by critic
  - `create_expert_agent()` - Expert agent whose output is reviewed by critic
  - `orchestrator()` - System coordination of critic review workflow
  - `execute_next_step()` - Step execution including critic review phases
  - `determine_next_step()` - Workflow progression through critic review phases
  - `check_retry_limit()` - Retry management for critic review failures
  - `GraphState` - Complete system state including critic decisions and feedback
  - `AgentMessage` - System-wide message protocol for critic communication
  - All research tools (for researcher output validation)
  - All expert tools (for expert output validation)
- **System Integration Points**: 
  - Critic agent integration with main system graph
  - Critic review coordination with planner, researcher, and expert agents
  - Critic decision integration into workflow progression
  - Critic feedback integration into agent retry mechanisms
  - State management for critic decisions and feedback
  - Message flow between critic and other agents
  - Orchestrator coordination of critic review phases
  - Quality control integration across entire workflow
- **End-to-End Data Flow**: 
  - Planner output triggers critic planner review phase
  - Critic evaluates planner output and provides decision/feedback
  - Researcher output triggers critic researcher review phase
  - Critic evaluates researcher output and provides decision/feedback
  - Expert output triggers critic expert review phase
  - Critic evaluates expert output and provides decision/feedback
  - Critic decisions determine workflow progression (approve/reject)
  - Critic feedback influences agent retry behavior
  - Complete critic review workflow state is maintained
  - Final critic decisions prepare workflow for finalization
- **System State Management**: 
  - GraphState manages complete critic review workflow state
  - Critic decisions and feedback are stored in system state
  - State transitions occur based on critic review outcomes
  - Agent retry counts are influenced by critic feedback
  - Workflow progression is controlled by critic decisions
  - Message history includes critic review communications
  - Quality control state is maintained throughout workflow
  - Final critic state prepares for finalization phase
- **System Message Flow**: 
  - Orchestrator messages trigger critic review phases
  - Critic agent messages flow through system for each review
  - Inter-agent communication occurs through AgentMessage protocol
  - Critic feedback messages influence agent behavior
  - Error messages propagate through critic review workflow
  - Completion messages signal critic review phase transitions
  - Final critic messages complete the review workflow

**System Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock all LLM responses for critic, planner, researcher, and expert agents
  - Mock all external services (research tools, computation tools)
  - Use real system components (orchestrator, critic agent, state management, message system)
  - Mock file system operations for document loading
  - Mock browser MCP client for web research
- **System Test Data**: 
  - Sample agent outputs for critic review (planner, researcher, expert)
  - Mock critic review scenarios (approve/reject decisions with feedback)
  - Quality assessment test cases with varying complexity
  - Feedback loop scenarios for agent improvement
  - Retry mechanism test cases based on critic feedback
  - Workflow progression test cases based on critic decisions
  - Complete critic review workflow state progression data
- **System Environment Setup**: 
  - Initialize complete critic review workflow with all agents
  - Set up mock external service environments
  - Configure critic, planner, researcher, and expert agent configurations
  - Prepare complete system state for critic review workflow
  - Set up mock file system and document loading
  - Configure critic review retry limits and error handling
- **External Dependencies**: 
  - Mock ChatOpenAI for critic, planner, researcher, and expert agent LLMs
  - Mock YouTube Transcript API for research validation
  - Mock Wikipedia Query API for knowledge validation
  - Mock Tavily web search API for research validation
  - Mock file loaders for document validation
  - Mock browser MCP client for web research validation
  - Mock computation libraries for expert output validation
- **System State Initialization**: 
  - Initialize GraphState with critic review workflow parameters
  - Set up critic, planner, researcher, and expert agent configurations
  - Initialize critic review states and message history
  - Configure critic review tool integrations
  - Set up critic decision and feedback parameters
  - Prepare agent output data for critic review

**System Test Scenarios to Include:**
- **Happy Path System Test**: 
  - Complete critic review workflow with all components working correctly
  - Successful critic review of planner, researcher, and expert outputs
  - Proper critic decision making and feedback generation
  - Correct workflow progression based on critic decisions
  - Proper state management throughout critic review workflow
  - Complete quality control integration across all agents
- **System Component Interaction Patterns**: 
  - Critic-to-planner review coordination
  - Critic-to-researcher review coordination
  - Critic-to-expert review coordination
  - Critic decision integration with workflow progression
  - Critic feedback integration with agent retry mechanisms
  - Orchestrator coordination of critic review phases
- **System Data Transformation**: 
  - Agent output transformation into critic review data
  - Critic evaluation transformation into decisions and feedback
  - Critic decision transformation into workflow progression
  - Critic feedback transformation into agent retry behavior
  - Complete data flow validation through critic review workflow
  - State transformation and progression throughout review process
- **System Error Propagation**: 
  - Critic review failure handling and recovery
  - Agent output validation error handling
  - Critic decision error handling and recovery
  - Feedback loop error handling and recovery
  - Quality control error propagation through system
  - Error propagation through critic review workflow
- **System State Synchronization**: 
  - State consistency across critic review phases
  - Critic state synchronization with main system state
  - Agent state synchronization with critic decisions
  - State recovery mechanisms for critic review failures
  - State persistence and restoration capabilities
  - Global state management across critic review workflow
- **System Message Flow**: 
  - Complete message flow through critic review phases
  - Critic-to-agent communication validation
  - Inter-agent communication through critic review
  - Error message handling and propagation
  - Message history maintenance and audit trail
  - Final critic message completion
- **System Retry Logic**: 
  - Retry mechanisms for critic agent
  - Retry mechanisms triggered by critic feedback
  - Retry limit enforcement across critic review workflow
  - Retry state management and recovery
  - Retry coordination between critic and other agents
  - Retry failure handling and workflow degradation
- **System Decision Making**: 
  - Critic decision making for quality assessment
  - Critic feedback generation for agent improvement
  - Workflow progression decision making based on critic decisions
  - Agent retry decision making based on critic feedback
  - System-level decision emergence from critic review interactions

**System Mock Configurations:**
- **Complete System Mocking**: 
  - Mock all external service calls for controlled validation data
  - Mock all LLM calls for deterministic critic responses
  - Use real system orchestration and state management
  - Mock file system operations for document validation
  - Mock browser interactions for web research validation
- **System Integration Boundaries**: 
  - Test real system components with mocked external dependencies
  - Maintain real critic review workflow coordination
  - Use real state management and message protocols
  - Mock only external services and LLM responses
  - Preserve real critic review workflow orchestration
- **System Mock Data Setup**: 
  - Realistic LLM responses for critic agent with varied decisions
  - Comprehensive agent output data for critic review
  - Varied critic review scenarios and decision patterns
  - Multiple quality assessment test cases
  - Different feedback loop scenarios
  - Complete critic review workflow state progression data
- **System Mock Behavior**: 
  - Consistent LLM response patterns for critic agent
  - Realistic agent output behavior for validation
  - Predictable critic decision making and feedback generation
  - Varied quality assessment behavior and response patterns
  - Reliable critic review workflow behavior and output generation
  - System-wide error simulation and recovery
- **LLM Mocking for System**: 
  - Mock ChatOpenAI for critic agent with specific review responses
  - Mock ChatOpenAI for planner, researcher, and expert agents
  - Simulate realistic critic review behavior and decision making
  - Provide consistent response patterns for workflow progression
  - Mock error scenarios and retry responses
  - Simulate critic-to-agent coordination and communication
- **External Service Mocking**: 
  - Mock research tools for output validation
  - Mock computation tools for expert output validation
  - Mock file loaders for document validation
  - Mock browser MCP for web research validation
  - Mock all external services for comprehensive validation

**System Assertion Specifications:**
- **System Integration Verification**: 
  - Verify complete critic review workflow execution
  - Verify critic coordination with all other agents
  - Verify critic decision integration into workflow progression
  - Verify critic feedback integration into agent retry mechanisms
  - Verify orchestrator coordination of critic review phases
  - Verify state management across critic review workflow
- **System Data Flow Validation**: 
  - Validate agent output integration into critic review
  - Validate critic decision integration into workflow progression
  - Validate critic feedback integration into agent behavior
  - Validate complete data flow through critic review workflow
  - Validate state progression throughout critic review process
  - Validate quality control data handoff
- **System State Consistency**: 
  - Verify state consistency across critic review phases
  - Verify critic state synchronization with main state
  - Verify agent state synchronization with critic decisions
  - Verify state recovery and persistence
  - Verify global state management across critic review workflow
  - Verify state integrity throughout review process
- **System Error Handling**: 
  - Verify error propagation through critic review workflow
  - Verify retry mechanisms for critic agent
  - Verify error recovery and system resilience
  - Verify graceful degradation handling
  - Verify error message propagation
  - Verify system stability under error conditions
- **System Message Validation**: 
  - Verify complete message flow through critic review phases
  - Verify critic-to-agent communication protocols
  - Verify message history and audit trail
  - Verify error message handling
  - Verify completion message flow
  - Verify final critic message generation
- **System State Validation**: 
  - Verify GraphState updates across critic review workflow
  - Verify critic decision state progression
  - Verify critic feedback state integration
  - Verify retry state management
  - Verify final state completion
  - Verify state consistency throughout workflow
- **System Performance Metrics**: 
  - Measure critic review workflow execution time
  - Measure critic coordination efficiency
  - Measure state management performance
  - Measure message flow efficiency
  - Measure error handling performance
  - Measure system resource utilization

**System Test Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    create_critic_agent, create_planner_agent, create_researcher_agent,
    create_expert_agent, orchestrator, execute_next_step, determine_next_step,
    check_retry_limit, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
@patch('multi_agent_system.UnstructuredPDFLoader')
@patch('multi_agent_system.pint')
@patch('multi_agent_system.math')
@patch('multi_agent_system.PythonREPLTool')
def test_complete_critic_review_workflow(mock_repl, mock_math, mock_pint, 
                                       mock_pdf_loader, mock_tavily, 
                                       mock_wikipedia, mock_youtube, mock_llm):
    """Test complete critic review workflow integration."""
    # Setup comprehensive mocks for critic review workflow
    
    # Mock LLM responses for all agents in critic review sequence
    mock_llm.return_value.invoke.side_effect = [
        # Planner agent response
        Mock(content='{"research_steps": ["Research AI basics"], "expert_steps": ["Analyze AI impact"]}'),
        # Critic agent response (planner review)
        Mock(content='{"decision": "approve", "feedback": "Good planning with clear research and expert steps"}'),
        # Researcher agent response
        Mock(content='{"result": "AI is artificial intelligence that enables machines to learn"}'),
        # Critic agent response (researcher review)
        Mock(content='{"decision": "approve", "feedback": "Comprehensive research covering AI definition"}'),
        # Expert agent response
        Mock(content='{"expert_answer": "AI has significant impact", "expert_reasoning": "Based on research, AI enables learning"}'),
        # Critic agent response (expert review)
        Mock(content='{"decision": "approve", "feedback": "Thorough analysis with clear reasoning"}')
    ]
    
    # Mock external service responses
    mock_youtube.get_transcript.return_value = [{"text": "AI is artificial intelligence"}]
    mock_wikipedia.return_value.run.return_value = "Artificial Intelligence (AI) is intelligence demonstrated by machines"
    mock_tavily.return_value.invoke.return_value = [{"content": "AI applications in modern technology"}]
    mock_pdf_loader.return_value.load.return_value = [Mock(page_content="AI research document")]
    
    # Mock computation tool responses
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.magnitude = 1000.0
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.units = "meter"
    mock_math.eval.return_value = 25.0
    mock_repl.return_value.invoke.return_value = "Result: 42"
    
    # Create all agent configurations
    planner_config = AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3)
    researcher_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5)
    expert_config = AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5)
    critic_config = AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3)
    
    # Create all agents
    planner_agent = create_planner_agent(planner_config, mock_llm.return_value)
    researcher_agent = create_researcher_agent(researcher_config, Mock())  # Mock compiled graph
    expert_agent = create_expert_agent(expert_config, Mock())  # Mock compiled graph
    critic_agent = create_critic_agent(critic_config, mock_llm.return_value)
    
    # Initialize complete system state for critic review workflow
    initial_state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute planner phase
    planner_state = planner_agent(initial_state)
    
    # Execute critic planner review
    critic_planner_state = critic_agent(planner_state)
    
    # Verify critic planner review
    assert critic_planner_state["critic_planner_decision"] == "approve"
    assert critic_planner_state["critic_planner_feedback"] == "Good planning with clear research and expert steps"
    assert critic_planner_state["research_steps"] == ["Research AI basics"]
    assert critic_planner_state["expert_steps"] == ["Analyze AI impact"]
    
    # Execute researcher phase (simulated)
    researcher_state = GraphState(**critic_planner_state)
    researcher_state["research_results"] = ["AI is artificial intelligence that enables machines to learn"]
    researcher_state["current_step"] = "researcher"
    
    # Execute critic researcher review
    critic_researcher_state = critic_agent(researcher_state)
    
    # Verify critic researcher review
    assert critic_researcher_state["critic_researcher_decision"] == "approve"
    assert critic_researcher_state["critic_researcher_feedback"] == "Comprehensive research covering AI definition"
    assert len(critic_researcher_state["research_results"]) == 1
    
    # Execute expert phase (simulated)
    expert_state = GraphState(**critic_researcher_state)
    expert_state["expert_answer"] = "AI has significant impact"
    expert_state["expert_reasoning"] = "Based on research, AI enables learning"
    expert_state["current_step"] = "expert"
    
    # Execute critic expert review
    critic_expert_state = critic_agent(expert_state)
    
    # Verify critic expert review
    assert critic_expert_state["critic_expert_decision"] == "approve"
    assert critic_expert_state["critic_expert_feedback"] == "Thorough analysis with clear reasoning"
    assert critic_expert_state["expert_answer"] == "AI has significant impact"
    
    # Verify complete critic review workflow integration
    assert critic_expert_state["critic_planner_decision"] == "approve"
    assert critic_expert_state["critic_researcher_decision"] == "approve"
    assert critic_expert_state["critic_expert_decision"] == "approve"
    assert critic_expert_state["planner_retry_count"] == 0
    assert critic_expert_state["researcher_retry_count"] == 0
    assert critic_expert_state["expert_retry_count"] == 0
    
    # Verify message flow through critic review workflow
    assert len(critic_expert_state["agent_messages"]) > 0
    
    # Verify all external dependencies were called
    mock_llm.assert_called()

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_critic_review_workflow_with_rejection(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test critic review workflow with rejection and retry logic."""
    # Setup mocks with critic rejection scenarios
    mock_llm.return_value.invoke.side_effect = [
        # Planner agent response
        Mock(content='{"research_steps": ["Research AI basics"], "expert_steps": ["Analyze AI impact"]}'),
        # Critic agent response (planner review - reject)
        Mock(content='{"decision": "reject", "feedback": "Research steps are too vague, need more specific steps"}'),
        # Planner agent response (retry)
        Mock(content='{"research_steps": ["Research AI definition", "Research AI applications"], "expert_steps": ["Analyze AI definition", "Analyze AI impact"]}'),
        # Critic agent response (planner review - approve)
        Mock(content='{"decision": "approve", "feedback": "Much better planning with specific steps"}'),
        # Researcher agent response
        Mock(content='{"result": "AI is artificial intelligence"}'),
        # Critic agent response (researcher review - approve)
        Mock(content='{"decision": "approve", "feedback": "Good research"}'),
        # Expert agent response
        Mock(content='{"expert_answer": "AI has impact", "expert_reasoning": "Based on research"}'),
        # Critic agent response (expert review - approve)
        Mock(content='{"decision": "approve", "feedback": "Good analysis"}')
    ]
    
    # Mock external services
    mock_youtube.get_transcript.return_value = [{"text": "AI transcript content"}]
    mock_wikipedia.return_value.run.return_value = "Wikipedia AI content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Web search AI content"}]
    
    # Create agents
    planner_config = AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3)
    critic_config = AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3)
    
    planner_agent = create_planner_agent(planner_config, mock_llm.return_value)
    critic_agent = create_critic_agent(critic_config, mock_llm.return_value)
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer="",
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute planner phase
    planner_state = planner_agent(initial_state)
    
    # Execute critic planner review (rejection)
    critic_planner_state = critic_agent(planner_state)
    
    # Verify rejection and retry
    assert critic_planner_state["critic_planner_decision"] == "reject"
    assert "vague" in critic_planner_state["critic_planner_feedback"]
    
    # Simulate planner retry
    planner_retry_state = GraphState(**critic_planner_state)
    planner_retry_state["planner_retry_count"] = 1
    planner_retry_state["research_steps"] = ["Research AI definition", "Research AI applications"]
    planner_retry_state["expert_steps"] = ["Analyze AI definition", "Analyze AI impact"]
    
    # Execute critic planner review (approval)
    critic_planner_approval_state = critic_agent(planner_retry_state)
    
    # Verify approval after retry
    assert critic_planner_approval_state["critic_planner_decision"] == "approve"
    assert "better" in critic_planner_approval_state["critic_planner_feedback"]
    assert critic_planner_approval_state["planner_retry_count"] == 1

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_critic_review_workflow_performance(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test critic review workflow performance and efficiency."""
    import time
    
    # Setup mocks for performance testing
    mock_llm.return_value.invoke.return_value = Mock(content='{"decision": "approve", "feedback": "Good work"}')
    
    # Create agents
    critic_config = AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3)
    critic_agent = create_critic_agent(critic_config, mock_llm.return_value)
    
    # Mock external services
    mock_youtube.get_transcript.return_value = [{"text": "Test transcript"}]
    mock_wikipedia.return_value.run.return_value = "Test Wikipedia content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Test web content"}]
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze AI impact"],
        researcher_states={},
        current_research_index=0,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="AI has significant impact",
        expert_reasoning="Based on research",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Measure performance
    start_time = time.time()
    critic_state = critic_agent(initial_state)
    execution_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert execution_time < 2.0  # Should complete in less than 2 seconds
    assert critic_state["critic_expert_decision"] == "approve"
    
    # Verify system efficiency
    assert len(critic_state["agent_messages"]) > 0
    assert critic_state["planner_retry_count"] == 0
    assert critic_state["researcher_retry_count"] == 0
    assert critic_state["expert_retry_count"] == 0
```
#### 5.1.4 Finalization Workflow

**System Test Information:**
- **Test Name**: Finalization Workflow
- **Location**: Section 5.1.4
- **Purpose**: Tests the complete finalization workflow integration within the multi-agent system, focusing on the synthesis and consolidation of all previous agent outputs (planner, researcher, expert, critic) by the finalizer agent, demonstrating how the system produces the final answer and reasoning trace that represents the culmination of the entire multi-agent collaboration

**Test Scope and System Boundaries:**
- **System Components Involved**: 
  - `create_finalizer_agent()` - Finalizer agent for answer synthesis and consolidation
  - `create_planner_agent()` - Planner agent whose output contributes to finalization
  - `create_researcher_agent()` - Researcher agent whose output contributes to finalization
  - `create_expert_agent()` - Expert agent whose output contributes to finalization
  - `create_critic_agent()` - Critic agent whose decisions and feedback contribute to finalization
  - `orchestrator()` - System coordination of finalization workflow
  - `execute_next_step()` - Step execution including finalization phase
  - `determine_next_step()` - Workflow progression to finalization phase
  - `check_retry_limit()` - Retry management for finalization failures
  - `GraphState` - Complete system state including final answer and reasoning trace
  - `AgentMessage` - System-wide message protocol for finalizer communication
  - All previous agent outputs and states for synthesis
  - Complete workflow history and audit trail
- **System Integration Points**: 
  - Finalizer agent integration with main system graph
  - Finalizer coordination with all previous agent outputs
  - Final answer synthesis from all agent contributions
  - Reasoning trace generation from complete workflow history
  - State consolidation and finalization
  - Message flow between finalizer and system
  - Orchestrator coordination of finalization phase
  - Complete workflow culmination and output generation
- **End-to-End Data Flow**: 
  - All agent outputs (planner, researcher, expert, critic) are collected
  - Finalizer agent receives complete workflow state and history
  - Finalizer synthesizes final answer from all agent contributions
  - Finalizer generates comprehensive reasoning trace
  - Final answer and reasoning trace are integrated into system state
  - Complete workflow state is finalized and prepared for output
  - Final system output represents culmination of all agent work
  - Workflow completion signals successful system operation
- **System State Management**: 
  - GraphState manages complete finalization workflow state
  - All previous agent states are preserved for finalization
  - Final answer and reasoning trace are stored in system state
  - Workflow completion state is maintained
  - Complete audit trail is preserved in final state
  - Message history includes finalization communications
  - Final state represents complete system output
  - State consistency is maintained throughout finalization
- **System Message Flow**: 
  - Orchestrator messages trigger finalization phase
  - Finalizer agent messages flow through system
  - Inter-agent communication includes finalization messages
  - Completion messages signal workflow finalization
  - Final answer messages complete the system workflow
  - Audit trail messages preserve complete workflow history

**System Test Setup Requirements:**
- **Mock Strategy**: 
  - Mock all LLM responses for finalizer and all previous agents
  - Mock all external services (research tools, computation tools)
  - Use real system components (orchestrator, finalizer agent, state management, message system)
  - Mock file system operations for document loading
  - Mock browser MCP client for web research
- **System Test Data**: 
  - Complete workflow state with all agent outputs
  - Mock finalizer synthesis scenarios with varying complexity
  - Final answer generation test cases with different content types
  - Reasoning trace generation test cases with complete workflow history
  - Workflow completion scenarios with different outcomes
  - Complete system output validation test cases
  - Finalization workflow state progression data
- **System Environment Setup**: 
  - Initialize complete finalization workflow with all agents
  - Set up mock external service environments
  - Configure finalizer and all previous agent configurations
  - Prepare complete system state for finalization workflow
  - Set up mock file system and document loading
  - Configure finalization retry limits and error handling
- **External Dependencies**: 
  - Mock ChatOpenAI for finalizer and all previous agent LLMs
  - Mock YouTube Transcript API for research validation
  - Mock Wikipedia Query API for knowledge validation
  - Mock Tavily web search API for research validation
  - Mock file loaders for document validation
  - Mock browser MCP client for web research validation
  - Mock computation libraries for expert output validation
- **System State Initialization**: 
  - Initialize GraphState with complete finalization workflow parameters
  - Set up finalizer and all previous agent configurations
  - Initialize complete workflow state with all agent outputs
  - Configure finalization tool integrations
  - Set up final answer and reasoning trace parameters
  - Prepare complete workflow history for finalization

**System Test Scenarios to Include:**
- **Happy Path System Test**: 
  - Complete finalization workflow with all components working correctly
  - Successful synthesis of all agent outputs into final answer
  - Proper reasoning trace generation from complete workflow history
  - Correct final state consolidation and completion
  - Proper state management throughout finalization workflow
  - Complete system output generation and validation
- **System Component Interaction Patterns**: 
  - Finalizer-to-all-agents synthesis coordination
  - Final answer generation from multiple agent contributions
  - Reasoning trace generation from complete workflow history
  - State consolidation and finalization patterns
  - Message flow between finalizer and system
  - Orchestrator coordination of finalization phase
- **System Data Transformation**: 
  - All agent output transformation into final answer
  - Workflow history transformation into reasoning trace
  - State consolidation transformation into final state
  - Complete data flow validation through finalization workflow
  - State transformation and progression throughout finalization
  - Final output generation and validation
- **System Error Propagation**: 
  - Finalization failure handling and recovery
  - Synthesis error handling and recovery
  - Reasoning trace generation error handling
  - State consolidation error handling and recovery
  - Complete workflow error propagation through finalization
  - Error propagation through finalization workflow
- **System State Synchronization**: 
  - State consistency across finalization phase
  - Finalizer state synchronization with main system state
  - Complete workflow state preservation and consolidation
  - State recovery mechanisms for finalization failures
  - State persistence and restoration capabilities
  - Global state management across finalization workflow
- **System Message Flow**: 
  - Complete message flow through finalization phase
  - Finalizer-to-system communication validation
  - Inter-agent communication through finalization
  - Error message handling and propagation
  - Message history maintenance and audit trail
  - Final answer message completion
- **System Retry Logic**: 
  - Retry mechanisms for finalizer agent
  - Retry mechanisms for synthesis failures
  - Retry limit enforcement across finalization workflow
  - Retry state management and recovery
  - Retry coordination between finalizer and system
  - Retry failure handling and workflow degradation
- **System Decision Making**: 
  - Finalizer decision making for answer synthesis
  - Reasoning trace generation decision making
  - State consolidation decision making
  - Workflow completion decision making
  - System-level decision emergence from finalization interactions

**System Mock Configurations:**
- **Complete System Mocking**: 
  - Mock all external service calls for controlled validation data
  - Mock all LLM calls for deterministic finalizer responses
  - Use real system orchestration and state management
  - Mock file system operations for document validation
  - Mock browser interactions for web research validation
- **System Integration Boundaries**: 
  - Test real system components with mocked external dependencies
  - Maintain real finalization workflow coordination
  - Use real state management and message protocols
  - Mock only external services and LLM responses
  - Preserve real finalization workflow orchestration
- **System Mock Data Setup**: 
  - Realistic LLM responses for finalizer agent with varied synthesis
  - Comprehensive agent output data for finalization
  - Varied finalization scenarios and synthesis patterns
  - Multiple final answer generation test cases
  - Different reasoning trace generation scenarios
  - Complete finalization workflow state progression data
- **System Mock Behavior**: 
  - Consistent LLM response patterns for finalizer agent
  - Realistic agent output behavior for synthesis
  - Predictable finalization behavior and synthesis generation
  - Varied final answer generation behavior and response patterns
  - Reliable finalization workflow behavior and output generation
  - System-wide error simulation and recovery
- **LLM Mocking for System**: 
  - Mock ChatOpenAI for finalizer agent with specific synthesis responses
  - Mock ChatOpenAI for all previous agents
  - Simulate realistic finalization behavior and synthesis decision making
  - Provide consistent response patterns for workflow completion
  - Mock error scenarios and retry responses
  - Simulate finalizer-to-system coordination and communication
- **External Service Mocking**: 
  - Mock research tools for output validation
  - Mock computation tools for expert output validation
  - Mock file loaders for document validation
  - Mock browser MCP for web research validation
  - Mock all external services for comprehensive validation

**System Assertion Specifications:**
- **System Integration Verification**: 
  - Verify complete finalization workflow execution
  - Verify finalizer coordination with all previous agents
  - Verify final answer synthesis from all agent contributions
  - Verify reasoning trace generation from complete workflow
  - Verify orchestrator coordination of finalization phase
  - Verify state management across finalization workflow
- **System Data Flow Validation**: 
  - Validate all agent output integration into final answer
  - Validate workflow history integration into reasoning trace
  - Validate state consolidation into final state
  - Validate complete data flow through finalization workflow
  - Validate state progression throughout finalization process
  - Validate final output generation and validation
- **System State Consistency**: 
  - Verify state consistency across finalization phase
  - Verify finalizer state synchronization with main state
  - Verify complete workflow state preservation and consolidation
  - Verify state recovery and persistence
  - Verify global state management across finalization workflow
  - Verify state integrity throughout finalization process
- **System Error Handling**: 
  - Verify error propagation through finalization workflow
  - Verify retry mechanisms for finalizer agent
  - Verify error recovery and system resilience
  - Verify graceful degradation handling
  - Verify error message propagation
  - Verify system stability under error conditions
- **System Message Validation**: 
  - Verify complete message flow through finalization phase
  - Verify finalizer-to-system communication protocols
  - Verify message history and audit trail
  - Verify error message handling
  - Verify completion message flow
  - Verify final answer message generation
- **System State Validation**: 
  - Verify GraphState updates across finalization workflow
  - Verify final answer state progression
  - Verify reasoning trace state integration
  - Verify retry state management
  - Verify final state completion
  - Verify state consistency throughout workflow
- **System Performance Metrics**: 
  - Measure finalization workflow execution time
  - Measure finalizer synthesis efficiency
  - Measure state management performance
  - Measure message flow efficiency
  - Measure error handling performance
  - Measure system resource utilization

**System Test Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
from datetime import datetime
from multi_agent_system import (
    create_finalizer_agent, create_planner_agent, create_researcher_agent,
    create_expert_agent, create_critic_agent, orchestrator, execute_next_step,
    determine_next_step, check_retry_limit, GraphState, AgentConfig
)
from langchain_core.messages import HumanMessage, AIMessage

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
@patch('multi_agent_system.UnstructuredPDFLoader')
@patch('multi_agent_system.pint')
@patch('multi_agent_system.math')
@patch('multi_agent_system.PythonREPLTool')
def test_complete_finalization_workflow(mock_repl, mock_math, mock_pint, 
                                      mock_pdf_loader, mock_tavily, 
                                      mock_wikipedia, mock_youtube, mock_llm):
    """Test complete finalization workflow integration."""
    # Setup comprehensive mocks for finalization workflow
    
    # Mock LLM responses for all agents in finalization sequence
    mock_llm.return_value.invoke.side_effect = [
        # Planner agent response
        Mock(content='{"research_steps": ["Research AI basics"], "expert_steps": ["Analyze AI impact"]}'),
        # Researcher agent response
        Mock(content='{"result": "AI is artificial intelligence that enables machines to learn"}'),
        # Expert agent response
        Mock(content='{"expert_answer": "AI has significant impact", "expert_reasoning": "Based on research, AI enables learning"}'),
        # Critic agent response (planner review)
        Mock(content='{"decision": "approve", "feedback": "Good planning"}'),
        # Critic agent response (researcher review)
        Mock(content='{"decision": "approve", "feedback": "Comprehensive research"}'),
        # Critic agent response (expert review)
        Mock(content='{"decision": "approve", "feedback": "Thorough analysis"}'),
        # Finalizer agent response
        Mock(content='{"final_answer": "Artificial Intelligence (AI) is a technology that enables machines to learn and perform tasks, with significant impact across various industries. Based on comprehensive research and expert analysis, AI represents a fundamental shift in how machines can process information and make decisions.", "final_reasoning_trace": "The final answer synthesizes research findings on AI definition and applications, expert analysis of AI impact, and critic validation of all agent outputs. The reasoning trace demonstrates how each agent contributed to the comprehensive understanding of AI."}')
    ]
    
    # Mock external service responses
    mock_youtube.get_transcript.return_value = [{"text": "AI is artificial intelligence"}]
    mock_wikipedia.return_value.run.return_value = "Artificial Intelligence (AI) is intelligence demonstrated by machines"
    mock_tavily.return_value.invoke.return_value = [{"content": "AI applications in modern technology"}]
    mock_pdf_loader.return_value.load.return_value = [Mock(page_content="AI research document")]
    
    # Mock computation tool responses
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.magnitude = 1000.0
    mock_pint.UnitRegistry.return_value.Quantity.return_value.to.return_value.units = "meter"
    mock_math.eval.return_value = 25.0
    mock_repl.return_value.invoke.return_value = "Result: 42"
    
    # Create all agent configurations
    planner_config = AgentConfig("planner", "openai", "gpt-4", 0.7, {}, "You are a planner", 3)
    researcher_config = AgentConfig("researcher", "openai", "gpt-4", 0.7, {"result": "string"}, "You are a researcher", 5)
    expert_config = AgentConfig("expert", "openai", "gpt-4", 0.7, {"expert_answer": "string", "expert_reasoning": "string"}, "You are an expert", 5)
    critic_config = AgentConfig("critic", "openai", "gpt-4", 0.7, {"decision": "string", "feedback": "string"}, "You are a critic", 3)
    finalizer_config = AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    
    # Create all agents
    planner_agent = create_planner_agent(planner_config, mock_llm.return_value)
    researcher_agent = create_researcher_agent(researcher_config, Mock())  # Mock compiled graph
    expert_agent = create_expert_agent(expert_config, Mock())  # Mock compiled graph
    critic_agent = create_critic_agent(critic_config, mock_llm.return_value)
    finalizer_agent = create_finalizer_agent(finalizer_config, mock_llm.return_value)
    
    # Initialize complete system state for finalization workflow
    initial_state = GraphState(
        agent_messages=[],
        question="What is artificial intelligence and what is its impact?",
        current_step="",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze AI impact"],
        researcher_states={},
        current_research_index=1,
        research_results=["AI is artificial intelligence that enables machines to learn"],
        expert_state=None,
        expert_answer="AI has significant impact",
        expert_reasoning="Based on research, AI enables learning",
        critic_planner_decision="approve",
        critic_planner_feedback="Good planning",
        critic_researcher_decision="approve",
        critic_researcher_feedback="Comprehensive research",
        critic_expert_decision="approve",
        critic_expert_feedback="Thorough analysis",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute finalization phase
    final_state = finalizer_agent(initial_state)
    
    # Verify complete finalization workflow integration
    assert final_state["final_answer"] == "Artificial Intelligence (AI) is a technology that enables machines to learn and perform tasks, with significant impact across various industries. Based on comprehensive research and expert analysis, AI represents a fundamental shift in how machines can process information and make decisions."
    assert final_state["final_reasoning_trace"] == "The final answer synthesizes research findings on AI definition and applications, expert analysis of AI impact, and critic validation of all agent outputs. The reasoning trace demonstrates how each agent contributed to the comprehensive understanding of AI."
    
    # Verify all previous agent outputs are preserved
    assert final_state["research_steps"] == ["Research AI basics"]
    assert final_state["expert_steps"] == ["Analyze AI impact"]
    assert len(final_state["research_results"]) == 1
    assert final_state["expert_answer"] == "AI has significant impact"
    assert final_state["critic_planner_decision"] == "approve"
    assert final_state["critic_researcher_decision"] == "approve"
    assert final_state["critic_expert_decision"] == "approve"
    
    # Verify system state consistency
    assert final_state["current_step"] == "finalizer"
    assert final_state["planner_retry_count"] == 0
    assert final_state["researcher_retry_count"] == 0
    assert final_state["expert_retry_count"] == 0
    
    # Verify message flow through finalization workflow
    assert len(final_state["agent_messages"]) > 0
    
    # Verify all external dependencies were called
    mock_llm.assert_called()

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_finalization_workflow_with_retry_logic(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test finalization workflow with retry logic and error handling."""
    # Setup mocks with finalization retry scenarios
    mock_llm.return_value.invoke.side_effect = [
        # First finalizer attempt fails
        Exception("Finalization API error"),
        # Second finalizer attempt succeeds
        Mock(content='{"final_answer": "AI is artificial intelligence with significant impact", "final_reasoning_trace": "Complete synthesis based on all agent outputs"}')
    ]
    
    # Mock external services
    mock_youtube.get_transcript.return_value = [{"text": "AI transcript content"}]
    mock_wikipedia.return_value.run.return_value = "Wikipedia AI content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Web search AI content"}]
    
    # Create agents
    finalizer_config = AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    finalizer_agent = create_finalizer_agent(finalizer_config, mock_llm.return_value)
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze AI impact"],
        researcher_states={},
        current_research_index=1,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="AI has impact",
        expert_reasoning="Based on research",
        critic_planner_decision="approve",
        critic_planner_feedback="Good planning",
        critic_researcher_decision="approve",
        critic_researcher_feedback="Good research",
        critic_expert_decision="approve",
        critic_expert_feedback="Good analysis",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Execute finalization workflow with retry logic
    final_state = finalizer_agent(initial_state)
    
    # Verify retry logic worked
    assert final_state["final_answer"] == "AI is artificial intelligence with significant impact"
    assert final_state["final_reasoning_trace"] == "Complete synthesis based on all agent outputs"
    
    # Verify system recovered and completed successfully
    assert len(final_state["research_results"]) == 1
    assert final_state["expert_answer"] == "AI has impact"

@patch('multi_agent_system.ChatOpenAI')
@patch('multi_agent_system.YouTubeTranscriptApi')
@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_finalization_workflow_performance(mock_tavily, mock_wikipedia, mock_youtube, mock_llm):
    """Test finalization workflow performance and efficiency."""
    import time
    
    # Setup mocks for performance testing
    mock_llm.return_value.invoke.return_value = Mock(content='{"final_answer": "Test final answer", "final_reasoning_trace": "Test reasoning trace"}')
    
    # Create agents
    finalizer_config = AgentConfig("finalizer", "openai", "gpt-4", 0.7, {"final_answer": "string", "final_reasoning_trace": "string"}, "You are a finalizer", 3)
    finalizer_agent = create_finalizer_agent(finalizer_config, mock_llm.return_value)
    
    # Mock external services
    mock_youtube.get_transcript.return_value = [{"text": "Test transcript"}]
    mock_wikipedia.return_value.run.return_value = "Test Wikipedia content"
    mock_tavily.return_value.invoke.return_value = [{"content": "Test web content"}]
    
    initial_state = GraphState(
        agent_messages=[],
        question="What is AI?",
        current_step="",
        next_step="",
        research_steps=["Research AI basics"],
        expert_steps=["Analyze AI impact"],
        researcher_states={},
        current_research_index=1,
        research_results=["AI is artificial intelligence"],
        expert_state=None,
        expert_answer="AI has impact",
        expert_reasoning="Based on research",
        critic_planner_decision="approve",
        critic_planner_feedback="Good planning",
        critic_researcher_decision="approve",
        critic_researcher_feedback="Good research",
        critic_expert_decision="approve",
        critic_expert_feedback="Good analysis",
        final_answer="",
        final_reasoning_trace="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )
    
    # Measure performance
    start_time = time.time()
    final_state = finalizer_agent(initial_state)
    execution_time = time.time() - start_time
    
    # Verify performance is reasonable
    assert execution_time < 2.0  # Should complete in less than 2 seconds
    assert final_state["final_answer"] == "Test final answer"
    
    # Verify system efficiency
    assert len(final_state["agent_messages"]) > 0
    assert final_state["planner_retry_count"] == 0
    assert final_state["researcher_retry_count"] == 0
    assert final_state["expert_retry_count"] == 0
```

## 6. Test Infrastructure
### 6.1 Test Fixtures and Setup
#### 6.1.1 Agent Configuration Fixtures

**Function Information:**
- Purpose: Define reusable test fixtures and setup utilities for agent configuration management
- Scope: Standardized AgentConfig objects, LLM configurations, and test agent setups used across multiple test suites
- Dependencies: pytest fixtures, AgentConfig class, mock LLM objects, test data structures

**Signature and Parameters:**
```python
@pytest.fixture
def agent_config_fixture(agent_type: str, custom_config: dict = None) -> AgentConfig:
    """Create standardized agent configuration for testing."""
    
@pytest.fixture
def llm_fixture(model: str = "gpt-4", temperature: float = 0.7) -> Mock:
    """Create mock LLM instance for testing."""
    
@pytest.fixture
def agent_configs_fixture() -> dict[str, AgentConfig]:
    """Create complete set of agent configurations for system testing."""
    
@pytest.fixture
def minimal_agent_config_fixture() -> AgentConfig:
    """Create minimal agent configuration for edge case testing."""
```

**Dependencies:**
- `pytest.fixture` decorator for fixture registration
- `AgentConfig` class from multi_agent_system module
- `Mock` from unittest.mock for LLM simulation
- `typing` for proper type annotations
- Test data generation utilities

**Test Cases:**
- **Happy Path**: Standard agent configurations with typical parameters (planner, researcher, expert, critic, finalizer)
- **Edge Cases**: Minimal configurations, high/low temperature values, empty output schemas, special characters in prompts
- **Error Conditions**: Invalid model names, negative temperatures, malformed output schemas, missing required fields

**Mock Configurations:**
```python
# Mock LLM with structured output
mock_llm = Mock()
mock_llm.with_structured_output.return_value = mock_llm
mock_llm.invoke.return_value = Mock(content='{"result": "test"}')

# Mock AgentConfig validation
@patch('multi_agent_system.AgentConfig')
def test_agent_config_fixture(mock_agent_config):
    """Test agent configuration fixture creation."""
```

**Direct Usage Examples:**
```python
def test_planner_agent_creation(agent_config_fixture, llm_fixture):
    """Test planner agent creation using fixtures."""
    config = agent_config_fixture("planner")
    llm = llm_fixture("gpt-4", 0.5)
    planner_agent = create_planner_agent(config, llm)
    assert planner_agent is not None

def test_system_integration(agent_configs_fixture):
    """Test complete system with all agent configurations."""
    configs = agent_configs_fixture()
    graph = create_multi_agent_graph(configs)
    assert len(configs) == 5  # All agent types
```

**Assertion Specifications:**
- Validate AgentConfig object creation with correct parameters
- Verify LLM mock setup with proper method chaining
- Check configuration completeness for all agent types
- Ensure fixture reusability across different test scenarios

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from multi_agent_system import AgentConfig

@pytest.fixture
def agent_config_fixture(agent_type: str, custom_config: dict = None) -> AgentConfig:
    """Create standardized agent configuration for testing."""
    default_configs = {
        "planner": {
            "name": "planner",
            "provider": "openai",
            "model": "gpt-4",
            "temperature": 0.7,
            "output_schema": {"plan": "string", "steps": "array"},
            "system_prompt": "You are a planning agent.",
            "retry_limit": 3
        },
        "researcher": {
            "name": "researcher", 
            "provider": "openai",
            "model": "gpt-4",
            "temperature": 0.5,
            "output_schema": {"research_findings": "string"},
            "system_prompt": "You are a research agent.",
            "retry_limit": 5
        },
        "expert": {
            "name": "expert",
            "provider": "openai", 
            "model": "gpt-4",
            "temperature": 0.3,
            "output_schema": {"expert_analysis": "string", "reasoning": "string"},
            "system_prompt": "You are an expert analysis agent.",
            "retry_limit": 5
        },
        "critic": {
            "name": "critic",
            "provider": "openai",
            "model": "gpt-4", 
            "temperature": 0.6,
            "output_schema": {"decision": "string", "feedback": "string"},
            "system_prompt": "You are a critic agent.",
            "retry_limit": 3
        },
        "finalizer": {
            "name": "finalizer",
            "provider": "openai",
            "model": "gpt-4",
            "temperature": 0.4,
            "output_schema": {"final_answer": "string", "reasoning_trace": "string"},
            "system_prompt": "You are a finalizer agent.",
            "retry_limit": 3
        }
    }
    
    config_data = custom_config or default_configs.get(agent_type, default_configs["planner"])
    return AgentConfig(**config_data)

@pytest.fixture
def llm_fixture(model: str = "gpt-4", temperature: float = 0.7) -> Mock:
    """Create mock LLM instance for testing."""
    mock_llm = Mock()
    mock_llm.model = model
    mock_llm.temperature = temperature
    mock_llm.with_structured_output.return_value = mock_llm
    mock_llm.invoke.return_value = Mock(content='{"result": "test_response"}')
    return mock_llm

@pytest.fixture
def agent_configs_fixture() -> dict[str, AgentConfig]:
    """Create complete set of agent configurations for system testing."""
    agent_types = ["planner", "researcher", "expert", "critic", "finalizer"]
    configs = {}
    
    for agent_type in agent_types:
        configs[agent_type] = agent_config_fixture(agent_type)
    
    return configs

@pytest.fixture
def minimal_agent_config_fixture() -> AgentConfig:
    """Create minimal agent configuration for edge case testing."""
    return AgentConfig(
        name="minimal",
        provider="openai",
        model="gpt-3.5-turbo",
        temperature=0.0,
        output_schema={"output": "string"},
        system_prompt="",
        retry_limit=1
    )

# Test the fixtures themselves
def test_agent_config_fixture_creation():
    """Test agent configuration fixture creates valid configs."""
    config = agent_config_fixture("planner")
    assert isinstance(config, AgentConfig)
    assert config.name == "planner"
    assert config.provider == "openai"
    assert config.model == "gpt-4"
    assert config.temperature == 0.7
    assert "plan" in config.output_schema
    assert config.system_prompt == "You are a planning agent."

def test_llm_fixture_creation():
    """Test LLM fixture creates properly configured mock."""
    llm = llm_fixture("gpt-4", 0.5)
    assert llm.model == "gpt-4"
    assert llm.temperature == 0.5
    assert hasattr(llm, 'with_structured_output')
    assert hasattr(llm, 'invoke')

def test_agent_configs_fixture_completeness():
    """Test complete agent configurations fixture."""
    configs = agent_configs_fixture()
    expected_agents = ["planner", "researcher", "expert", "critic", "finalizer"]
    assert len(configs) == len(expected_agents)
    for agent_type in expected_agents:
        assert agent_type in configs
        assert isinstance(configs[agent_type], AgentConfig)
        assert configs[agent_type].name == agent_type

def test_minimal_agent_config_fixture():
    """Test minimal agent configuration for edge cases."""
    config = minimal_agent_config_fixture()
    assert config.name == "minimal"
    assert config.temperature == 0.0
    assert config.system_prompt == ""
    assert config.retry_limit == 1
```
#### 6.1.2 State Management Fixtures

**Function Information:**
- Purpose: Define reusable test fixtures and setup utilities for state management and GraphState initialization
- Scope: Standardized GraphState, ResearcherState, ExpertState objects, and state transition helpers used across multiple test suites
- Dependencies: pytest fixtures, GraphState, ResearcherState, ExpertState classes, AgentMessage structures, test data generators

**Signature and Parameters:**
```python
@pytest.fixture
def graph_state_fixture(question: str = "Test question", current_step: str = "input") -> GraphState:
    """Create standardized GraphState for testing."""
    
@pytest.fixture
def researcher_state_fixture(step_index: int = 0, result: Any = None) -> ResearcherState:
    """Create ResearcherState for testing researcher subgraph."""
    
@pytest.fixture
def expert_state_fixture(question: str = "Test question", research_steps: list = None) -> ExpertState:
    """Create ExpertState for testing expert subgraph."""
    
@pytest.fixture
def agent_message_fixture(sender: str = "orchestrator", receiver: str = "planner") -> AgentMessage:
    """Create standardized AgentMessage for testing."""
    
@pytest.fixture
def complete_workflow_state_fixture() -> GraphState:
    """Create complete workflow state with all stages populated."""
```

**Dependencies:**
- `pytest.fixture` decorator for fixture registration
- `GraphState`, `ResearcherState`, `ExpertState` classes from multi_agent_system module
- `AgentMessage` TypedDict for message structures
- `typing` for proper type annotations
- Test data generation utilities

**Test Cases:**
- **Happy Path**: Standard states with typical data, complete workflow states, normal message flows
- **Edge Cases**: Empty states, minimal data, large datasets, special characters in content, maximum retry counts
- **Error Conditions**: Invalid state transitions, corrupted state data, missing required fields, malformed messages

**Mock Configurations:**
```python
# Mock state validation
@patch('multi_agent_system.GraphState')
def test_graph_state_fixture(mock_graph_state):
    """Test graph state fixture creation."""
    
# Mock message composition
@patch('multi_agent_system.compose_agent_message')
def test_agent_message_fixture(mock_compose):
    """Test agent message fixture creation."""
```

**Direct Usage Examples:**
```python
def test_planner_agent_execution(graph_state_fixture, agent_config_fixture, llm_fixture):
    """Test planner agent using state fixtures."""
    state = graph_state_fixture("What is AI?", "input")
    config = agent_config_fixture("planner")
    llm = llm_fixture()
    planner_agent = create_planner_agent(config, llm)
    result_state = planner_agent(state)
    assert result_state["current_step"] == "planner"

def test_researcher_subgraph(researcher_state_fixture, agent_config_fixture, llm_fixture):
    """Test researcher subgraph using state fixtures."""
    state = researcher_state_fixture(0, "Initial research")
    config = agent_config_fixture("researcher")
    llm = llm_fixture()
    researcher_node = create_researcher_llm_node(config, llm)
    result_state = researcher_node(state)
    assert result_state.step_index == 0
```

**Assertion Specifications:**
- Validate state object creation with correct structure and default values
- Verify state transitions maintain data integrity
- Check message composition and transmission correctness
- Ensure fixture reusability across different test scenarios

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from typing import Any, List
from multi_agent_system import GraphState, ResearcherState, ExpertState, AgentMessage

@pytest.fixture
def graph_state_fixture(question: str = "Test question", current_step: str = "input") -> GraphState:
    """Create standardized GraphState for testing."""
    return GraphState(
        agent_messages=[],
        question=question,
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer=None,
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step=current_step,
        next_step="",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )

@pytest.fixture
def researcher_state_fixture(step_index: int = 0, result: Any = None) -> ResearcherState:
    """Create ResearcherState for testing researcher subgraph."""
    return ResearcherState(
        messages=[],
        step_index=step_index,
        result=result
    )

@pytest.fixture
def expert_state_fixture(question: str = "Test question", research_steps: List[str] = None) -> ExpertState:
    """Create ExpertState for testing expert subgraph."""
    if research_steps is None:
        research_steps = ["Research step 1", "Research step 2"]
    
    return ExpertState(
        messages=[],
        question=question,
        research_steps=research_steps,
        research_results=["Result 1", "Result 2"],
        expert_answer=None,
        expert_reasoning=""
    )

@pytest.fixture
def agent_message_fixture(sender: str = "orchestrator", receiver: str = "planner", 
                         message_type: str = "instruction", content: str = "Test message") -> AgentMessage:
    """Create standardized AgentMessage for testing."""
    return AgentMessage(
        timestamp="2024-01-01T00:00:00Z",
        sender=sender,
        receiver=receiver,
        type=message_type,
        content=content,
        step_id=None
    )

@pytest.fixture
def complete_workflow_state_fixture() -> GraphState:
    """Create complete workflow state with all stages populated."""
    return GraphState(
        agent_messages=[
            AgentMessage(
                timestamp="2024-01-01T00:00:00Z",
                sender="orchestrator",
                receiver="planner",
                type="instruction",
                content="Plan the research",
                step_id=1
            ),
            AgentMessage(
                timestamp="2024-01-01T00:01:00Z",
                sender="planner",
                receiver="orchestrator",
                type="feedback",
                content="Research plan completed",
                step_id=1
            )
        ],
        question="What is the impact of AI on society?",
        file=None,
        research_steps=["Research AI basics", "Analyze societal impact"],
        expert_steps=["Evaluate economic effects", "Assess social implications"],
        researcher_states={
            0: ResearcherState(
                messages=[],
                step_index=0,
                result="AI basics research completed"
            )
        },
        current_research_index=1,
        research_results=["AI is artificial intelligence", "AI has significant societal impact"],
        expert_state=ExpertState(
            messages=[],
            question="What is the impact of AI on society?",
            research_steps=["Research AI basics", "Analyze societal impact"],
            research_results=["AI is artificial intelligence", "AI has significant societal impact"],
            expert_answer="AI has transformative impact on society",
            expert_reasoning="Based on research findings, AI affects multiple sectors"
        ),
        expert_answer="AI has transformative impact on society",
        expert_reasoning="Based on research findings, AI affects multiple sectors",
        critic_planner_decision="approve",
        critic_planner_feedback="Good planning approach",
        critic_researcher_decision="approve",
        critic_researcher_feedback="Comprehensive research",
        critic_expert_decision="approve",
        critic_expert_feedback="Solid analysis",
        final_answer="",
        final_reasoning_trace="",
        current_step="critic_expert",
        next_step="finalizer",
        planner_retry_count=0,
        researcher_retry_count=0,
        expert_retry_count=0,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )

@pytest.fixture
def edge_case_state_fixture() -> GraphState:
    """Create edge case state with minimal data and special characters."""
    return GraphState(
        agent_messages=[],
        question="",  # Empty question
        file=None,
        research_steps=[],
        expert_steps=[],
        researcher_states={},
        current_research_index=0,
        research_results=[],
        expert_state=None,
        expert_answer=None,
        expert_reasoning="",
        critic_planner_decision="",
        critic_planner_feedback="",
        critic_researcher_decision="",
        critic_researcher_feedback="",
        critic_expert_decision="",
        critic_expert_feedback="",
        final_answer="",
        final_reasoning_trace="",
        current_step="",
        next_step="",
        planner_retry_count=3,  # Maximum retries
        researcher_retry_count=5,
        expert_retry_count=5,
        planner_retry_limit=3,
        researcher_retry_limit=5,
        expert_retry_limit=5
    )

# Test the fixtures themselves
def test_graph_state_fixture_creation():
    """Test graph state fixture creates valid states."""
    state = graph_state_fixture("Test question", "planner")
    assert isinstance(state, GraphState)
    assert state["question"] == "Test question"
    assert state["current_step"] == "planner"
    assert state["planner_retry_count"] == 0
    assert len(state["agent_messages"]) == 0

def test_researcher_state_fixture_creation():
    """Test researcher state fixture creates valid states."""
    state = researcher_state_fixture(1, "Test result")
    assert isinstance(state, ResearcherState)
    assert state.step_index == 1
    assert state.result == "Test result"
    assert len(state.messages) == 0

def test_expert_state_fixture_creation():
    """Test expert state fixture creates valid states."""
    custom_steps = ["Custom step 1", "Custom step 2"]
    state = expert_state_fixture("Custom question", custom_steps)
    assert isinstance(state, ExpertState)
    assert state.question == "Custom question"
    assert state.research_steps == custom_steps
    assert len(state.research_results) == 2

def test_agent_message_fixture_creation():
    """Test agent message fixture creates valid messages."""
    message = agent_message_fixture("researcher", "expert", "feedback", "Test feedback")
    assert isinstance(message, dict)
    assert message["sender"] == "researcher"
    assert message["receiver"] == "expert"
    assert message["type"] == "feedback"
    assert message["content"] == "Test feedback"
    assert "timestamp" in message

def test_complete_workflow_state_fixture():
    """Test complete workflow state fixture."""
    state = complete_workflow_state_fixture()
    assert len(state["agent_messages"]) == 2
    assert len(state["research_steps"]) == 2
    assert len(state["expert_steps"]) == 2
    assert state["current_step"] == "critic_expert"
    assert state["next_step"] == "finalizer"
    assert state["expert_state"] is not None
    assert state["expert_answer"] is not None

def test_edge_case_state_fixture():
    """Test edge case state fixture with minimal data."""
    state = edge_case_state_fixture()
    assert state["question"] == ""
    assert len(state["research_steps"]) == 0
    assert state["planner_retry_count"] == 3
    assert state["current_step"] == ""
```
#### 6.1.3 Mock Data Fixtures

**Function Information:**
- Purpose: Define reusable test fixtures and setup utilities for mock data generation and external service simulation
- Scope: Standardized mock responses, test documents, external service data, and file content used across multiple test suites
- Dependencies: pytest fixtures, Mock objects, Document structures, test data generators, external service simulators

**Signature and Parameters:**
```python
@pytest.fixture
def mock_llm_response_fixture(response_type: str = "standard") -> dict:
    """Create standardized mock LLM responses for testing."""
    
@pytest.fixture
def mock_document_fixture(content: str = "Test document content", source: str = "test.pdf") -> Document:
    """Create mock Document objects for testing file loaders."""
    
@pytest.fixture
def mock_external_service_fixture(service_type: str = "youtube") -> dict:
    """Create mock external service responses for testing."""
    
@pytest.fixture
def mock_file_content_fixture(file_type: str = "text", size: str = "small") -> str:
    """Create mock file content for testing file operations."""
    
@pytest.fixture
def mock_tool_response_fixture(tool_type: str = "calculator") -> Any:
    """Create mock tool responses for testing expert tools."""
```

**Dependencies:**
- `pytest.fixture` decorator for fixture registration
- `Mock` from unittest.mock for object simulation
- `Document` from langchain.schema for document structures
- `typing` for proper type annotations
- Test data generation utilities

**Test Cases:**
- **Happy Path**: Standard mock responses, typical document content, normal external service data
- **Edge Cases**: Large datasets, special characters, empty responses, malformed data, maximum content sizes
- **Error Conditions**: Invalid response formats, corrupted data, missing fields, network errors, timeout scenarios

**Mock Configurations:**
```python
# Mock LLM response with structured output
@patch('multi_agent_system.ChatOpenAI')
def test_llm_response_fixture(mock_chat_openai):
    """Test LLM response fixture creation."""
    
# Mock external service with error simulation
@patch('multi_agent_system.YouTubeTranscriptApi')
def test_external_service_fixture(mock_youtube):
    """Test external service fixture creation."""
```

**Direct Usage Examples:**
```python
def test_researcher_with_mock_data(mock_llm_response_fixture, mock_document_fixture):
    """Test researcher agent with mock data fixtures."""
    llm_response = mock_llm_response_fixture("research")
    document = mock_document_fixture("AI research findings", "research.pdf")
    # Use in researcher testing...

def test_expert_tools_with_mock_data(mock_tool_response_fixture):
    """Test expert tools with mock response fixtures."""
    calculator_result = mock_tool_response_fixture("calculator")
    unit_converter_result = mock_tool_response_fixture("unit_converter")
    # Use in expert testing...
```

**Assertion Specifications:**
- Validate mock response structure and content correctness
- Verify document object creation with proper metadata
- Check external service response format and completeness
- Ensure fixture reusability across different test scenarios

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch
from typing import Any, Dict, List
from langchain.schema import Document

@pytest.fixture
def mock_llm_response_fixture(response_type: str = "standard") -> dict:
    """Create standardized mock LLM responses for testing."""
    response_templates = {
        "standard": {
            "content": '{"result": "Standard response", "status": "success"}',
            "model": "gpt-4",
            "usage": {"total_tokens": 100}
        },
        "planner": {
            "content": '{"plan": "Research AI basics", "steps": ["Step 1", "Step 2"]}',
            "model": "gpt-4",
            "usage": {"total_tokens": 150}
        },
        "researcher": {
            "content": '{"research_findings": "AI is artificial intelligence", "sources": ["source1", "source2"]}',
            "model": "gpt-4",
            "usage": {"total_tokens": 200}
        },
        "expert": {
            "content": '{"expert_analysis": "AI has significant impact", "reasoning": "Based on research"}',
            "model": "gpt-4",
            "usage": {"total_tokens": 180}
        },
        "critic": {
            "content": '{"decision": "approve", "feedback": "Good work"}',
            "model": "gpt-4",
            "usage": {"total_tokens": 120}
        },
        "finalizer": {
            "content": '{"final_answer": "AI is transformative", "reasoning_trace": "Complete analysis"}',
            "model": "gpt-4",
            "usage": {"total_tokens": 250}
        },
        "error": {
            "content": '{"error": "Invalid request", "details": "Missing parameters"}',
            "model": "gpt-4",
            "usage": {"total_tokens": 50}
        },
        "large": {
            "content": '{"result": "' + "x" * 10000 + '"}',
            "model": "gpt-4",
            "usage": {"total_tokens": 5000}
        }
    }
    
    template = response_templates.get(response_type, response_templates["standard"])
    mock_response = Mock()
    mock_response.content = template["content"]
    mock_response.model = template["model"]
    mock_response.usage = template["usage"]
    return mock_response

@pytest.fixture
def mock_document_fixture(content: str = "Test document content", source: str = "test.pdf") -> Document:
    """Create mock Document objects for testing file loaders."""
    return Document(
        page_content=content,
        metadata={
            "source": source,
            "page": 1,
            "total_pages": 1,
            "file_type": source.split(".")[-1] if "." in source else "unknown"
        }
    )

@pytest.fixture
def mock_external_service_fixture(service_type: str = "youtube") -> dict:
    """Create mock external service responses for testing."""
    service_responses = {
        "youtube": {
            "transcript": [
                {"text": "This is a YouTube video transcript", "start": 0.0, "duration": 5.0},
                {"text": "It contains information about AI", "start": 5.0, "duration": 5.0}
            ],
            "video_info": {
                "title": "AI Explained",
                "duration": 600,
                "views": 10000
            }
        },
        "wikipedia": {
            "content": "Artificial Intelligence (AI) is intelligence demonstrated by machines.",
            "summary": "AI is machine intelligence",
            "url": "https://en.wikipedia.org/wiki/Artificial_intelligence"
        },
        "web_search": {
            "results": [
                {
                    "title": "AI Research Paper",
                    "snippet": "Recent advances in artificial intelligence...",
                    "url": "https://example.com/ai-paper"
                },
                {
                    "title": "AI Applications",
                    "snippet": "AI is used in various industries...",
                    "url": "https://example.com/ai-applications"
                }
            ],
            "total_results": 1000000
        },
        "browser_mcp": {
            "page_content": "<html><body><h1>AI Information</h1><p>AI is transforming the world.</p></body></html>",
            "title": "AI Information Page",
            "url": "https://example.com/ai-info"
        },
        "error": {
            "error": "Service unavailable",
            "status_code": 503,
            "message": "External service is down"
        }
    }
    
    return service_responses.get(service_type, service_responses["youtube"])

@pytest.fixture
def mock_file_content_fixture(file_type: str = "text", size: str = "small") -> str:
    """Create mock file content for testing file operations."""
    content_templates = {
        "text": {
            "small": "This is a small text file with basic content.",
            "medium": "This is a medium-sized text file with more detailed content about artificial intelligence and its applications in modern technology. It covers various aspects including machine learning, neural networks, and practical implementations.",
            "large": "This is a large text file with extensive content about artificial intelligence. " * 1000
        },
        "json": {
            "small": '{"name": "test", "value": 123}',
            "medium": '{"data": {"items": [{"id": 1, "name": "item1"}, {"id": 2, "name": "item2"}]}, "metadata": {"version": "1.0"}}',
            "large": '{"large_dataset": [' + ','.join([f'{{"id": {i}, "data": "item{i}"}}' for i in range(1000)]) + ']}'
        },
        "csv": {
            "small": "name,age,city\nJohn,30,New York\nJane,25,Boston",
            "medium": "id,name,email,department,salary\n1,John Doe,john@example.com,Engineering,75000\n2,Jane Smith,jane@example.com,Marketing,65000\n3,Bob Johnson,bob@example.com,Sales,70000",
            "large": "id,name,email,department,salary,start_date,manager,location\n" + "\n".join([f"{i},Employee{i},emp{i}@example.com,Dept{i%5},{(i%10+5)*10000},2023-01-01,Manager{i//10},Location{i%3}" for i in range(1000)])
        },
        "excel": {
            "small": "Mock Excel content - small dataset",
            "medium": "Mock Excel content - medium dataset with multiple sheets and formulas",
            "large": "Mock Excel content - large dataset with complex calculations and multiple worksheets"
        },
        "pdf": {
            "small": "Mock PDF content - basic document",
            "medium": "Mock PDF content - research paper with tables and figures",
            "large": "Mock PDF content - comprehensive report with extensive data and analysis"
        },
        "powerpoint": {
            "small": "Mock PowerPoint content - simple presentation",
            "medium": "Mock PowerPoint content - business presentation with charts",
            "large": "Mock PowerPoint content - comprehensive training material with multimedia"
        }
    }
    
    return content_templates.get(file_type, {}).get(size, content_templates["text"]["small"])

@pytest.fixture
def mock_tool_response_fixture(tool_type: str = "calculator") -> Any:
    """Create mock tool responses for testing expert tools."""
    tool_responses = {
        "calculator": {
            "expression": "2 + 2 * 3",
            "result": "8",
            "steps": ["2 + 2 * 3", "2 + 6", "8"]
        },
        "unit_converter": {
            "input": "1 mile",
            "output": "1.60934 kilometers",
            "conversion": "1 mile = 1.60934 kilometers"
        },
        "python_repl": {
            "code": "print('Hello, World!')",
            "output": "Hello, World!",
            "error": None
        },
        "python_repl_error": {
            "code": "print(undefined_variable)",
            "output": "",
            "error": "NameError: name 'undefined_variable' is not defined"
        },
        "complex_calculation": {
            "expression": "sqrt(16) + log(100)",
            "result": "6.0",
            "steps": ["sqrt(16) = 4", "log(100) = 2", "4 + 2 = 6"]
        },
        "unit_conversion_complex": {
            "input": "1000 meters per second",
            "output": "2236.94 miles per hour",
            "conversion": "1000 m/s = 2236.94 mph"
        }
    }
    
    return tool_responses.get(tool_type, tool_responses["calculator"])

@pytest.fixture
def mock_error_response_fixture(error_type: str = "network") -> dict:
    """Create mock error responses for testing error handling."""
    error_responses = {
        "network": {
            "error": "ConnectionError",
            "message": "Failed to connect to external service",
            "status_code": 503
        },
        "timeout": {
            "error": "TimeoutError",
            "message": "Request timed out after 30 seconds",
            "status_code": 408
        },
        "invalid_input": {
            "error": "ValueError",
            "message": "Invalid input parameters",
            "status_code": 400
        },
        "file_not_found": {
            "error": "FileNotFoundError",
            "message": "File not found: /path/to/file.txt",
            "status_code": 404
        },
        "permission_denied": {
            "error": "PermissionError",
            "message": "Permission denied accessing file",
            "status_code": 403
        }
    }
    
    return error_responses.get(error_type, error_responses["network"])

# Test the fixtures themselves
def test_mock_llm_response_fixture_creation():
    """Test LLM response fixture creates valid responses."""
    response = mock_llm_response_fixture("planner")
    assert hasattr(response, 'content')
    assert hasattr(response, 'model')
    assert hasattr(response, 'usage')
    assert "plan" in response.content
    assert response.model == "gpt-4"

def test_mock_document_fixture_creation():
    """Test document fixture creates valid Document objects."""
    doc = mock_document_fixture("Test content", "test.pdf")
    assert isinstance(doc, Document)
    assert doc.page_content == "Test content"
    assert doc.metadata["source"] == "test.pdf"
    assert doc.metadata["file_type"] == "pdf"

def test_mock_external_service_fixture_creation():
    """Test external service fixture creates valid responses."""
    youtube_data = mock_external_service_fixture("youtube")
    assert "transcript" in youtube_data
    assert "video_info" in youtube_data
    assert len(youtube_data["transcript"]) > 0
    
    wikipedia_data = mock_external_service_fixture("wikipedia")
    assert "content" in wikipedia_data
    assert "url" in wikipedia_data

def test_mock_file_content_fixture_creation():
    """Test file content fixture creates valid content."""
    text_content = mock_file_content_fixture("text", "small")
    assert isinstance(text_content, str)
    assert len(text_content) > 0
    
    json_content = mock_file_content_fixture("json", "small")
    assert isinstance(json_content, str)
    assert json_content.startswith("{")

def test_mock_tool_response_fixture_creation():
    """Test tool response fixture creates valid responses."""
    calc_response = mock_tool_response_fixture("calculator")
    assert "expression" in calc_response
    assert "result" in calc_response
    
    converter_response = mock_tool_response_fixture("unit_converter")
    assert "input" in converter_response
    assert "output" in converter_response

def test_mock_error_response_fixture_creation():
    """Test error response fixture creates valid error data."""
    network_error = mock_error_response_fixture("network")
    assert "error" in network_error
    assert "message" in network_error
    assert "status_code" in network_error
    assert network_error["status_code"] == 503
```
### 6.2 Mock Configurations
#### 6.2.1 LLM Mock Configurations

**Function Information:**
- Purpose: Define standardized mock configurations and setup patterns for LLM components and responses
- Scope: Reusable mock objects, patching strategies, and configuration templates for ChatOpenAI, LLM factories, and response handling
- Dependencies: unittest.mock, pytest-mock, patching decorators, mock object factories

**Signature and Parameters:**
```python
@patch('multi_agent_system.ChatOpenAI')
def llm_mock_configuration(mock_chat_openai, response_type: str = "standard") -> Mock:
    """Configure standardized LLM mock for testing."""
    
@patch('multi_agent_system.ChatOpenAI')
def llm_factory_mock_configuration(mock_chat_openai, model: str = "gpt-4") -> Mock:
    """Configure LLM factory mock for testing."""
    
@patch('multi_agent_system.ChatOpenAI')
def structured_output_mock_configuration(mock_chat_openai, output_schema: dict = None) -> Mock:
    """Configure structured output LLM mock for testing."""
```

**Dependencies:**
- `unittest.mock.Mock`, `unittest.mock.patch`, `unittest.mock.MagicMock`
- `pytest-mock` fixtures and utilities
- Mock object factories and builders
- Configuration templates and patterns

**Test Cases:**
- **Happy Path**: Standard LLM mock configurations with typical behavior, normal response patterns, structured output setup
- **Edge Cases**: Complex mock chains, conditional responses, stateful mocks, large response data, multiple LLM instances
- **Error Conditions**: Mock failures, invalid configurations, missing dependencies, corrupted mock states, LLM errors

**Mock Configurations:**
```python
# Standard LLM mock with structured output
@patch('multi_agent_system.ChatOpenAI')
def test_llm_mock_configuration(mock_chat_openai):
    """Test LLM mock configuration setup."""
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    mock_llm.invoke.return_value = Mock(content='{"result": "test"}')
    
# LLM factory mock with parameter validation
@patch('multi_agent_system.ChatOpenAI')
def test_llm_factory_mock_configuration(mock_chat_openai):
    """Test LLM factory mock configuration."""
```

**Direct Usage Examples:**
```python
@patch('multi_agent_system.ChatOpenAI')
def test_planner_agent_with_llm_mock(mock_chat_openai):
    """Test planner agent using LLM mock configuration."""
    # Configure mock
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    mock_llm.invoke.return_value = Mock(content='{"plan": "Test plan", "steps": ["Step 1"]}')
    
    # Test implementation
    config = AgentConfig("planner", "openai", "gpt-4", 0.7, {"plan": "string"}, "Test prompt")
    planner_agent = create_planner_agent(config, mock_llm)
    # ... test execution

@patch('multi_agent_system.ChatOpenAI')
def test_llm_factory_with_mock(mock_chat_openai):
    """Test LLM factory using mock configuration."""
    # Configure mock
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    
    # Test implementation
    config = AgentConfig("test", "openai", "gpt-4", 0.7, {"output": "string"}, "Test prompt")
    result = llm_factory(config)
    assert result == mock_llm
```

**Assertion Specifications:**
- Validate LLM mock configuration correctness and completeness
- Verify structured output setup and method chaining
- Check mock response format and content structure
- Ensure configuration reusability across different test scenarios

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import ChatOpenAI, AgentConfig

def configure_llm_mock(mock_chat_openai, response_type: str = "standard") -> Mock:
    """Configure standardized LLM mock for testing."""
    response_templates = {
        "standard": '{"result": "Standard response", "status": "success"}',
        "planner": '{"plan": "Research AI basics", "steps": ["Step 1", "Step 2"]}',
        "researcher": '{"research_findings": "AI is artificial intelligence", "sources": ["source1"]}',
        "expert": '{"expert_analysis": "AI has significant impact", "reasoning": "Based on research"}',
        "critic": '{"decision": "approve", "feedback": "Good work"}',
        "finalizer": '{"final_answer": "AI is transformative", "reasoning_trace": "Complete analysis"}',
        "error": '{"error": "Invalid request", "details": "Missing parameters"}',
        "large": '{"result": "' + "x" * 10000 + '"}'
    }
    
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    mock_llm.invoke.return_value = Mock(content=response_templates.get(response_type, response_templates["standard"]))
    
    return mock_llm

def configure_llm_factory_mock(mock_chat_openai, model: str = "gpt-4", temperature: float = 0.7) -> Mock:
    """Configure LLM factory mock for testing."""
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    
    # Verify factory calls
    def verify_factory_call(*args, **kwargs):
        assert kwargs.get('model') == model
        assert kwargs.get('temperature') == temperature
        return mock_llm
    
    mock_chat_openai.side_effect = verify_factory_call
    return mock_llm

def configure_structured_output_mock(mock_chat_openai, output_schema: dict = None) -> Mock:
    """Configure structured output LLM mock for testing."""
    if output_schema is None:
        output_schema = {"type": "object", "properties": {"result": {"type": "string"}}}
    
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    
    # Verify structured output setup
    def verify_structured_output(schema, method="json_mode"):
        assert schema == output_schema
        assert method == "json_mode"
        return mock_llm
    
    mock_llm.with_structured_output.side_effect = verify_structured_output
    return mock_llm

def configure_multiple_llm_mocks(mock_chat_openai) -> dict[str, Mock]:
    """Configure multiple LLM mocks for system testing."""
    llm_configs = {
        "planner": {"response_type": "planner", "model": "gpt-4", "temperature": 0.7},
        "researcher": {"response_type": "researcher", "model": "gpt-4", "temperature": 0.5},
        "expert": {"response_type": "expert", "model": "gpt-4", "temperature": 0.3},
        "critic": {"response_type": "critic", "model": "gpt-4", "temperature": 0.6},
        "finalizer": {"response_type": "finalizer", "model": "gpt-4", "temperature": 0.4}
    }
    
    llm_mocks = {}
    for agent_type, config in llm_configs.items():
        mock_llm = configure_llm_mock(mock_chat_openai, config["response_type"])
        llm_mocks[agent_type] = mock_llm
    
    return llm_mocks

def configure_llm_error_mock(mock_chat_openai, error_type: str = "invoke_error") -> Mock:
    """Configure LLM mock that simulates errors."""
    mock_llm = Mock()
    mock_chat_openai.return_value = mock_llm
    mock_llm.with_structured_output.return_value = mock_llm
    
    error_configs = {
        "invoke_error": Exception("LLM invoke failed"),
        "structured_output_error": Exception("Structured output setup failed"),
        "constructor_error": Exception("ChatOpenAI constructor failed"),
        "timeout_error": TimeoutError("LLM request timed out"),
        "rate_limit_error": Exception("Rate limit exceeded")
    }
    
    error = error_configs.get(error_type, error_configs["invoke_error"])
    
    if error_type == "constructor_error":
        mock_chat_openai.side_effect = error
    elif error_type == "structured_output_error":
        mock_llm.with_structured_output.side_effect = error
    else:
        mock_llm.invoke.side_effect = error
    
    return mock_llm

# Test the mock configurations
@patch('multi_agent_system.ChatOpenAI')
def test_configure_llm_mock(mock_chat_openai):
    """Test LLM mock configuration setup."""
    mock_llm = configure_llm_mock(mock_chat_openai, "planner")
    
    assert mock_chat_openai.called
    assert mock_llm.with_structured_output.called
    assert mock_llm.invoke.called
    
    # Test response content
    response = mock_llm.invoke.return_value
    assert "plan" in response.content
    assert "steps" in response.content

@patch('multi_agent_system.ChatOpenAI')
def test_configure_llm_factory_mock(mock_chat_openai):
    """Test LLM factory mock configuration."""
    mock_llm = configure_llm_factory_mock(mock_chat_openai, "gpt-4", 0.5)
    
    # Test factory call verification
    config = AgentConfig("test", "openai", "gpt-4", 0.5, {"output": "string"}, "Test prompt")
    result = llm_factory(config)
    
    assert result == mock_llm
    assert mock_chat_openai.called

@patch('multi_agent_system.ChatOpenAI')
def test_configure_structured_output_mock(mock_chat_openai):
    """Test structured output mock configuration."""
    output_schema = {"type": "object", "properties": {"answer": {"type": "string"}}}
    mock_llm = configure_structured_output_mock(mock_chat_openai, output_schema)
    
    # Test structured output setup
    mock_llm.with_structured_output(output_schema, "json_mode")
    assert mock_llm.with_structured_output.called

@patch('multi_agent_system.ChatOpenAI')
def test_configure_multiple_llm_mocks(mock_chat_openai):
    """Test multiple LLM mocks configuration."""
    llm_mocks = configure_multiple_llm_mocks(mock_chat_openai)
    
    assert len(llm_mocks) == 5
    assert "planner" in llm_mocks
    assert "researcher" in llm_mocks
    assert "expert" in llm_mocks
    assert "critic" in llm_mocks
    assert "finalizer" in llm_mocks
    
    # Test each mock has correct response type
    planner_response = llm_mocks["planner"].invoke.return_value.content
    assert "plan" in planner_response
    
    researcher_response = llm_mocks["researcher"].invoke.return_value.content
    assert "research_findings" in researcher_response

@patch('multi_agent_system.ChatOpenAI')
def test_configure_llm_error_mock(mock_chat_openai):
    """Test LLM error mock configuration."""
    mock_llm = configure_llm_error_mock(mock_chat_openai, "invoke_error")
    
    # Test error simulation
    with pytest.raises(Exception, match="LLM invoke failed"):
        mock_llm.invoke("test input")

@patch('multi_agent_system.ChatOpenAI')
def test_llm_mock_integration_with_agent(mock_chat_openai):
    """Test LLM mock integration with agent creation."""
    mock_llm = configure_llm_mock(mock_chat_openai, "planner")
    
    config = AgentConfig("planner", "openai", "gpt-4", 0.7, {"plan": "string"}, "Test prompt")
    planner_agent = create_planner_agent(config, mock_llm)
    
    assert planner_agent is not None
    assert callable(planner_agent)
    
    # Test agent execution
    state = GraphState(
        agent_messages=[],
        question="Test question",
        current_step="input",
        next_step="",
        # ... other required fields
    )
    
    result_state = planner_agent(state)
    assert mock_llm.invoke.called
```
#### 6.2.2 External Service Mocks

**Function Information:**
- Purpose: Define standardized mock configurations and setup patterns for external service components and APIs
- Scope: Reusable mock objects, patching strategies, and configuration templates for YouTube, Wikipedia, web search, browser MCP, and file loaders
- Dependencies: unittest.mock, pytest-mock, patching decorators, mock object factories

**Signature and Parameters:**
```python
@patch('multi_agent_system.YouTubeTranscriptApi')
def youtube_service_mock_configuration(mock_youtube_api, transcript_data: list = None) -> Mock:
    """Configure YouTube transcript service mock for testing."""
    
@patch('multi_agent_system.WikipediaQueryRun')
def wikipedia_service_mock_configuration(mock_wikipedia, content: str = None) -> Mock:
    """Configure Wikipedia service mock for testing."""
    
@patch('multi_agent_system.TavilySearchResults')
def web_search_service_mock_configuration(mock_tavily, search_results: list = None) -> Mock:
    """Configure web search service mock for testing."""
    
@patch('multi_agent_system.get_browser_mcp_tools')
def browser_mcp_service_mock_configuration(mock_browser_mcp, page_content: str = None) -> Mock:
    """Configure browser MCP service mock for testing."""
```

**Dependencies:**
- `unittest.mock.Mock`, `unittest.mock.patch`, `unittest.mock.MagicMock`
- `pytest-mock` fixtures and utilities
- Mock object factories and builders
- Configuration templates and patterns

**Test Cases:**
- **Happy Path**: Standard external service mock configurations with typical behavior, normal response patterns, successful API calls
- **Edge Cases**: Complex mock chains, conditional responses, stateful mocks, large data structures, rate limiting simulation
- **Error Conditions**: Mock failures, invalid configurations, missing dependencies, corrupted mock states, network errors, API failures

**Mock Configurations:**
```python
# YouTube transcript service mock
@patch('multi_agent_system.YouTubeTranscriptApi')
def test_youtube_service_mock_configuration(mock_youtube_api):
    """Test YouTube service mock configuration setup."""
    mock_api = Mock()
    mock_youtube_api.return_value = mock_api
    mock_api.get_transcript.return_value = [{"text": "Test transcript"}]
    
# Wikipedia service mock with content validation
@patch('multi_agent_system.WikipediaQueryRun')
def test_wikipedia_service_mock_configuration(mock_wikipedia):
    """Test Wikipedia service mock configuration."""
```

**Direct Usage Examples:**
```python
@patch('multi_agent_system.YouTubeTranscriptApi')
def test_youtube_tool_with_service_mock(mock_youtube_api):
    """Test YouTube tool using external service mock configuration."""
    # Configure mock
    mock_api = Mock()
    mock_youtube_api.return_value = mock_api
    mock_api.get_transcript.return_value = [{"text": "AI video transcript"}]
    
    # Test implementation
    result = youtube_transcript_tool("https://youtube.com/watch?v=test")
    assert "AI video transcript" in result

@patch('multi_agent_system.WikipediaQueryRun')
def test_wikipedia_tool_with_service_mock(mock_wikipedia):
    """Test Wikipedia tool using external service mock configuration."""
    # Configure mock
    mock_wiki = Mock()
    mock_wikipedia.return_value = mock_wiki
    mock_wiki.run.return_value = "AI is artificial intelligence"
    
    # Test implementation
    result = wikipedia_tool("artificial intelligence")
    assert "AI is artificial intelligence" in result
```

**Assertion Specifications:**
- Validate external service mock configuration correctness and completeness
- Verify API response format and content structure
- Check service integration and error handling
- Ensure configuration reusability across different test scenarios

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from multi_agent_system import YouTubeTranscriptApi, WikipediaQueryRun, TavilySearchResults

def configure_youtube_service_mock(mock_youtube_api, transcript_data: list = None) -> Mock:
    """Configure YouTube transcript service mock for testing."""
    if transcript_data is None:
        transcript_data = [
            {"text": "This is a YouTube video about AI", "start": 0.0, "duration": 5.0},
            {"text": "It covers machine learning and neural networks", "start": 5.0, "duration": 5.0},
            {"text": "The video explains artificial intelligence concepts", "start": 10.0, "duration": 5.0}
        ]
    
    mock_api = Mock()
    mock_youtube_api.return_value = mock_api
    mock_api.get_transcript.return_value = transcript_data
    
    # Configure video info
    mock_api.get_video_info.return_value = {
        "title": "AI Explained",
        "duration": 600,
        "views": 10000,
        "description": "Comprehensive guide to artificial intelligence"
    }
    
    return mock_api

def configure_wikipedia_service_mock(mock_wikipedia, content: str = None) -> Mock:
    """Configure Wikipedia service mock for testing."""
    if content is None:
        content = "Artificial Intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals."
    
    mock_wiki = Mock()
    mock_wikipedia.return_value = mock_wiki
    mock_wiki.run.return_value = content
    
    # Configure additional Wikipedia methods
    mock_wiki.get_summary.return_value = "AI is machine intelligence"
    mock_wiki.get_url.return_value = "https://en.wikipedia.org/wiki/Artificial_intelligence"
    
    return mock_wiki

def configure_web_search_service_mock(mock_tavily, search_results: list = None) -> Mock:
    """Configure web search service mock for testing."""
    if search_results is None:
        search_results = [
            {
                "title": "AI Research Paper",
                "snippet": "Recent advances in artificial intelligence and machine learning...",
                "url": "https://example.com/ai-paper",
                "score": 0.95
            },
            {
                "title": "AI Applications in Industry",
                "snippet": "How artificial intelligence is transforming various industries...",
                "url": "https://example.com/ai-applications",
                "score": 0.88
            },
            {
                "title": "Machine Learning Fundamentals",
                "snippet": "Understanding the basics of machine learning algorithms...",
                "url": "https://example.com/ml-basics",
                "score": 0.82
            }
        ]
    
    mock_search = Mock()
    mock_tavily.return_value = mock_search
    mock_search.invoke.return_value = search_results
    
    # Configure search metadata
    mock_search.get_total_results.return_value = 1000000
    mock_search.get_search_time.return_value = 0.5
    
    return mock_search

def configure_browser_mcp_service_mock(mock_browser_mcp, page_content: str = None) -> Mock:
    """Configure browser MCP service mock for testing."""
    if page_content is None:
        page_content = {
            "page_content": "<html><body><h1>AI Information</h1><p>Artificial Intelligence is transforming the world.</p></body></html>",
            "title": "AI Information Page",
            "url": "https://example.com/ai-info",
            "text_content": "AI Information\n\nArtificial Intelligence is transforming the world."
        }
    
    mock_browser = Mock()
    mock_browser_mcp.return_value = [mock_browser]
    
    # Configure browser tool methods
    mock_browser.name = "browser_tool"
    mock_browser.description = "Browse web pages and extract content"
    mock_browser.invoke.return_value = page_content
    
    return mock_browser

def configure_file_loader_service_mock(mock_loader_class, file_content: str = None) -> Mock:
    """Configure file loader service mock for testing."""
    if file_content is None:
        file_content = "This is test file content for document processing."
    
    mock_loader = Mock()
    mock_loader_class.return_value = mock_loader
    mock_loader.load.return_value = [
        Mock(
            page_content=file_content,
            metadata={
                "source": "test_file.pdf",
                "page": 1,
                "total_pages": 1
            }
        )
    ]
    
    return mock_loader

def configure_external_service_error_mock(service_type: str = "youtube", error_type: str = "network") -> dict:
    """Configure external service mock that simulates errors."""
    error_configs = {
        "youtube": {
            "network": Exception("YouTube API network error"),
            "video_not_found": Exception("Video not found"),
            "transcript_disabled": Exception("Transcripts are disabled for this video"),
            "rate_limit": Exception("YouTube API rate limit exceeded")
        },
        "wikipedia": {
            "network": Exception("Wikipedia API network error"),
            "page_not_found": Exception("Wikipedia page not found"),
            "invalid_query": Exception("Invalid Wikipedia query"),
            "rate_limit": Exception("Wikipedia API rate limit exceeded")
        },
        "web_search": {
            "network": Exception("Web search API network error"),
            "invalid_query": Exception("Invalid search query"),
            "no_results": Exception("No search results found"),
            "rate_limit": Exception("Search API rate limit exceeded")
        },
        "browser_mcp": {
            "network": Exception("Browser MCP network error"),
            "page_not_found": Exception("Web page not found"),
            "timeout": Exception("Browser request timed out"),
            "connection_failed": Exception("Browser connection failed")
        }
    }
    
    return error_configs.get(service_type, {}).get(error_type, Exception("Unknown error"))

def configure_multiple_external_services_mock() -> dict[str, Mock]:
    """Configure multiple external service mocks for system testing."""
    services = {}
    
    with patch('multi_agent_system.YouTubeTranscriptApi') as mock_youtube:
        services['youtube'] = configure_youtube_service_mock(mock_youtube)
    
    with patch('multi_agent_system.WikipediaQueryRun') as mock_wikipedia:
        services['wikipedia'] = configure_wikipedia_service_mock(mock_wikipedia)
    
    with patch('multi_agent_system.TavilySearchResults') as mock_tavily:
        services['web_search'] = configure_web_search_service_mock(mock_tavily)
    
    with patch('multi_agent_system.get_browser_mcp_tools') as mock_browser:
        services['browser_mcp'] = configure_browser_mcp_service_mock(mock_browser)
    
    return services

# Test the mock configurations
@patch('multi_agent_system.YouTubeTranscriptApi')
def test_configure_youtube_service_mock(mock_youtube_api):
    """Test YouTube service mock configuration setup."""
    mock_api = configure_youtube_service_mock(mock_youtube_api)
    
    assert mock_youtube_api.called
    assert mock_api.get_transcript.called
    assert mock_api.get_video_info.called
    
    # Test transcript data
    transcript = mock_api.get_transcript.return_value
    assert len(transcript) == 3
    assert "AI" in transcript[0]["text"]

@patch('multi_agent_system.WikipediaQueryRun')
def test_configure_wikipedia_service_mock(mock_wikipedia):
    """Test Wikipedia service mock configuration."""
    mock_wiki = configure_wikipedia_service_mock(mock_wikipedia)
    
    assert mock_wikipedia.called
    assert mock_wiki.run.called
    
    # Test content
    content = mock_wiki.run.return_value
    assert "Artificial Intelligence" in content
    assert "machines" in content

@patch('multi_agent_system.TavilySearchResults')
def test_configure_web_search_service_mock(mock_tavily):
    """Test web search service mock configuration."""
    mock_search = configure_web_search_service_mock(mock_tavily)
    
    assert mock_tavily.called
    assert mock_search.invoke.called
    
    # Test search results
    results = mock_search.invoke.return_value
    assert len(results) == 3
    assert "AI Research Paper" in results[0]["title"]

@patch('multi_agent_system.get_browser_mcp_tools')
def test_configure_browser_mcp_service_mock(mock_browser_mcp):
    """Test browser MCP service mock configuration."""
    mock_browser = configure_browser_mcp_service_mock(mock_browser_mcp)
    
    assert mock_browser_mcp.called
    assert mock_browser.invoke.called
    
    # Test browser content
    content = mock_browser.invoke.return_value
    assert "AI Information" in content["title"]
    assert "transforming" in content["page_content"]

@patch('multi_agent_system.UnstructuredPDFLoader')
def test_configure_file_loader_service_mock(mock_pdf_loader):
    """Test file loader service mock configuration."""
    mock_loader = configure_file_loader_service_mock(mock_pdf_loader)
    
    assert mock_pdf_loader.called
    assert mock_loader.load.called
    
    # Test document content
    documents = mock_loader.load.return_value
    assert len(documents) == 1
    assert "test file content" in documents[0].page_content

def test_configure_external_service_error_mock():
    """Test external service error mock configuration."""
    youtube_error = configure_external_service_error_mock("youtube", "network")
    assert isinstance(youtube_error, Exception)
    assert "YouTube API network error" in str(youtube_error)
    
    wikipedia_error = configure_external_service_error_mock("wikipedia", "page_not_found")
    assert isinstance(wikipedia_error, Exception)
    assert "Wikipedia page not found" in str(wikipedia_error)

def test_configure_multiple_external_services_mock():
    """Test multiple external services mock configuration."""
    services = configure_multiple_external_services_mock()
    
    assert len(services) == 4
    assert "youtube" in services
    assert "wikipedia" in services
    assert "web_search" in services
    assert "browser_mcp" in services
    
    # Test each service is properly configured
    assert services["youtube"].get_transcript.called
    assert services["wikipedia"].run.called
    assert services["web_search"].invoke.called
    assert services["browser_mcp"].invoke.called

@patch('multi_agent_system.YouTubeTranscriptApi')
def test_external_service_mock_integration_with_researcher(mock_youtube_api):
    """Test external service mock integration with researcher agent."""
    mock_api = configure_youtube_service_mock(mock_youtube_api)
    
    # Test YouTube tool integration
    result = youtube_transcript_tool("https://youtube.com/watch?v=test")
    assert "AI" in result
    assert "machine learning" in result
    
    # Verify service was called correctly
    mock_api.get_transcript.assert_called_once_with("https://youtube.com/watch?v=test")

@patch('multi_agent_system.WikipediaQueryRun')
@patch('multi_agent_system.TavilySearchResults')
def test_multiple_external_services_integration(mock_tavily, mock_wikipedia):
    """Test multiple external services integration."""
    # Configure both services
    mock_wiki = configure_wikipedia_service_mock(mock_wikipedia)
    mock_search = configure_web_search_service_mock(mock_tavily)
    
    # Test Wikipedia tool
    wiki_result = wikipedia_tool("artificial intelligence")
    assert "Artificial Intelligence" in wiki_result
    
    # Test web search tool
    search_result = web_search_tool("AI applications")
    assert len(search_result) == 3
    assert "AI Research Paper" in search_result[0]["title"]
    
    # Verify both services were called
    mock_wiki.run.assert_called_once()
    mock_search.invoke.assert_called_once()
```
#### 6.2.3 File System Mocks

**Function Information:**
- Purpose: Define standardized mock configurations and setup patterns for file system operations and file handling components
- Scope: Reusable mock objects, patching strategies, and configuration templates for file I/O, path operations, and file content handling
- Dependencies: unittest.mock, pytest-mock, patching decorators, mock object factories

**Signature and Parameters:**
```python
@patch('builtins.open')
def file_io_mock_configuration(mock_open, file_content: str = "test content") -> Mock:
    """Configure file I/O operations mock for testing."""
    
@patch('main.os.path')
def path_operations_mock_configuration(mock_os_path, file_exists: bool = True) -> Mock:
    """Configure path operations mock for testing."""
    
@patch('main.json')
def json_operations_mock_configuration(mock_json, json_data: dict = None) -> Mock:
    """Configure JSON operations mock for testing."""
    
@patch('main.load_prompt_from_file')
def prompt_file_mock_configuration(mock_load_prompt, prompt_content: str = "test prompt") -> Mock:
    """Configure prompt file loading mock for testing."""
```

**Dependencies:**
- `unittest.mock.Mock`, `unittest.mock.patch`, `unittest.mock.MagicMock`
- `pytest-mock` fixtures and utilities
- Mock object factories and builders
- Configuration templates and patterns

**Test Cases:**
- **Happy Path**: Standard file system mock configurations with typical behavior, normal file operations, successful I/O patterns
- **Edge Cases**: Complex file paths, large files, special characters, nested directories, file permissions
- **Error Conditions**: Mock failures, invalid configurations, missing dependencies, corrupted mock states, file system errors

**Mock Configurations:**
```python
# File I/O operations mock
@patch('builtins.open')
def test_file_io_mock_configuration(mock_open):
    """Test file I/O mock configuration setup."""
    mock_file = Mock()
    mock_open.return_value.__enter__.return_value = mock_file
    mock_file.read.return_value = "Test file content"
    
# Path operations mock with existence validation
@patch('main.os.path')
def test_path_operations_mock_configuration(mock_os_path):
    """Test path operations mock configuration."""
```

**Direct Usage Examples:**
```python
@patch('builtins.open')
def test_load_prompt_with_file_mock(mock_open):
    """Test prompt loading using file system mock configuration."""
    # Configure mock
    mock_file = Mock()
    mock_open.return_value.__enter__.return_value = mock_file
    mock_file.read.return_value = "You are a test agent."
    
    # Test implementation
    result = load_prompt_from_file("/path/to/prompt.txt")
    assert "You are a test agent." in result

@patch('main.os.path')
def test_baseline_prompts_with_path_mock(mock_os_path):
    """Test baseline prompts loading using path mock configuration."""
    # Configure mock
    mock_os_path.dirname.return_value = "/path/to/script"
    mock_os_path.abspath.return_value = "/absolute/path/to/script"
    mock_os_path.join.return_value = "/path/to/prompts/baseline"
    
    # Test implementation
    result = load_baseline_prompts()
    assert mock_os_path.dirname.called
```

**Assertion Specifications:**
- Validate file system mock configuration correctness and completeness
- Verify file operation patterns and content handling
- Check path resolution and file existence logic
- Ensure configuration reusability across different test scenarios

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock, mock_open
from main import load_prompt_from_file, load_baseline_prompts, read_jsonl_file, write_jsonl_file

def configure_file_io_mock(mock_open, file_content: str = "test content", file_path: str = "/path/to/file.txt") -> Mock:
    """Configure file I/O operations mock for testing."""
    mock_file = Mock()
    mock_file.read.return_value = file_content
    mock_file.write = Mock()
    mock_file.__iter__ = Mock(return_value=iter(file_content.split('\n')))
    
    mock_open.return_value.__enter__.return_value = mock_file
    mock_open.return_value.__exit__.return_value = None
    
    return mock_file

def configure_path_operations_mock(mock_os_path, file_exists: bool = True, is_file: bool = True, is_dir: bool = False) -> Mock:
    """Configure path operations mock for testing."""
    mock_os_path.exists.return_value = file_exists
    mock_os_path.isfile.return_value = is_file
    mock_os_path.isdir.return_value = is_dir
    mock_os_path.dirname.return_value = "/path/to/parent"
    mock_os_path.abspath.return_value = "/absolute/path/to/file"
    mock_os_path.join.return_value = "/path/to/joined/file"
    mock_os_path.normpath.return_value = "/normalized/path/to/file"
    mock_os_path.basename.return_value = "file.txt"
    mock_os_path.splitext.return_value = ("file", ".txt")
    
    return mock_os_path

def configure_json_operations_mock(mock_json, json_data: dict = None) -> Mock:
    """Configure JSON operations mock for testing."""
    if json_data is None:
        json_data = {"name": "test", "value": 123}
    
    mock_json.loads.return_value = json_data
    mock_json.dumps.return_value = '{"name": "test", "value": 123}'
    
    return mock_json

def configure_prompt_file_mock(mock_load_prompt, prompt_content: str = "test prompt") -> Mock:
    """Configure prompt file loading mock for testing."""
    mock_load_prompt.return_value = prompt_content
    return mock_load_prompt

def configure_file_system_error_mock(error_type: str = "file_not_found") -> Exception:
    """Configure file system error mock for testing."""
    error_configs = {
        "file_not_found": FileNotFoundError("No such file or directory: '/path/to/file.txt'"),
        "permission_denied": PermissionError("Permission denied: '/path/to/file.txt'"),
        "is_directory": IsADirectoryError("Is a directory: '/path/to/directory'"),
        "not_directory": NotADirectoryError("Not a directory: '/path/to/file.txt'"),
        "io_error": IOError("Input/output error"),
        "encoding_error": UnicodeDecodeError("utf-8", b"invalid bytes", 0, 1, "invalid continuation byte")
    }
    
    return error_configs.get(error_type, error_configs["file_not_found"])

def configure_large_file_mock(mock_open, file_size: str = "medium") -> Mock:
    """Configure large file mock for testing."""
    file_sizes = {
        "small": "Small file content",
        "medium": "Medium file content " * 1000,
        "large": "Large file content " * 10000,
        "very_large": "Very large file content " * 100000
    }
    
    content = file_sizes.get(file_size, file_sizes["medium"])
    return configure_file_io_mock(mock_open, content)

def configure_special_characters_file_mock(mock_open) -> Mock:
    """Configure file with special characters mock for testing."""
    special_content = """
    File with special characters:
    - Unicode: ä½ å¥½ä¸–ç•Œ ðŸŒ
    - Special chars: !@#$%^&*()_+-=[]{}|;':",./<>?
    - Newlines: Line 1
    Line 2
    Line 3
    - Tabs: Column1\tColumn2\tColumn3
    - Quotes: "Single quotes" and 'Double quotes'
    """
    
    return configure_file_io_mock(mock_open, special_content)

def configure_jsonl_file_mock(mock_open, jsonl_data: list = None) -> Mock:
    """Configure JSONL file mock for testing."""
    if jsonl_data is None:
        jsonl_data = [
            {"name": "John", "age": 30},
            {"name": "Jane", "age": 25},
            {"name": "Bob", "age": 35}
        ]
    
    jsonl_content = '\n'.join([json.dumps(item) for item in jsonl_data])
    return configure_file_io_mock(mock_open, jsonl_content)

def configure_directory_structure_mock(mock_os_path, directory_structure: dict = None) -> Mock:
    """Configure directory structure mock for testing."""
    if directory_structure is None:
        directory_structure = {
            "/path/to/prompts/baseline": [
                "planner.txt",
                "researcher.txt", 
                "expert.txt",
                "critic.txt",
                "finalizer.txt"
            ],
            "/path/to/data": [
                "input.jsonl",
                "output.jsonl"
            ]
        }
    
    def mock_exists(path):
        return path in directory_structure or any(
            os.path.join(dir_path, filename) == path 
            for dir_path, files in directory_structure.items() 
            for filename in files
        )
    
    def mock_isfile(path):
        return any(
            os.path.join(dir_path, filename) == path 
            for dir_path, files in directory_structure.items() 
            for filename in files
        )
    
    def mock_isdir(path):
        return path in directory_structure
    
    mock_os_path.exists.side_effect = mock_exists
    mock_os_path.isfile.side_effect = mock_isfile
    mock_os_path.isdir.side_effect = mock_isdir
    
    return mock_os_path

# Test the mock configurations
@patch('builtins.open')
def test_configure_file_io_mock(mock_open):
    """Test file I/O mock configuration setup."""
    mock_file = configure_file_io_mock(mock_open, "Test content", "/path/to/file.txt")
    
    assert mock_open.called
    assert mock_file.read.called
    
    # Test file reading
    with open("/path/to/file.txt", "r") as f:
        content = f.read()
        assert content == "Test content"

@patch('main.os.path')
def test_configure_path_operations_mock(mock_os_path):
    """Test path operations mock configuration."""
    mock_path = configure_path_operations_mock(mock_os_path, True, True, False)
    
    assert mock_path.exists.called
    assert mock_path.isfile.called
    assert mock_path.isdir.called
    
    # Test path operations
    assert mock_path.exists("/path/to/file.txt") == True
    assert mock_path.isfile("/path/to/file.txt") == True
    assert mock_path.isdir("/path/to/file.txt") == False

@patch('main.json')
def test_configure_json_operations_mock(mock_json):
    """Test JSON operations mock configuration."""
    mock_json_ops = configure_json_operations_mock(mock_json, {"test": "data"})
    
    assert mock_json.loads.called
    assert mock_json.dumps.called
    
    # Test JSON operations
    result = mock_json.loads('{"test": "data"}')
    assert result == {"test": "data"}

@patch('main.load_prompt_from_file')
def test_configure_prompt_file_mock(mock_load_prompt):
    """Test prompt file mock configuration."""
    mock_prompt = configure_prompt_file_mock(mock_load_prompt, "Test prompt content")
    
    assert mock_load_prompt.called
    
    # Test prompt loading
    result = mock_load_prompt("/path/to/prompt.txt")
    assert result == "Test prompt content"

def test_configure_file_system_error_mock():
    """Test file system error mock configuration."""
    file_not_found = configure_file_system_error_mock("file_not_found")
    assert isinstance(file_not_found, FileNotFoundError)
    assert "No such file or directory" in str(file_not_found)
    
    permission_error = configure_file_system_error_mock("permission_denied")
    assert isinstance(permission_error, PermissionError)
    assert "Permission denied" in str(permission_error)

@patch('builtins.open')
def test_configure_large_file_mock(mock_open):
    """Test large file mock configuration."""
    mock_file = configure_large_file_mock(mock_open, "large")
    
    # Test large file reading
    with open("/path/to/file.txt", "r") as f:
        content = f.read()
        assert len(content) > 10000
        assert "Large file content" in content

@patch('builtins.open')
def test_configure_special_characters_file_mock(mock_open):
    """Test special characters file mock configuration."""
    mock_file = configure_special_characters_file_mock(mock_open)
    
    # Test special characters file reading
    with open("/path/to/file.txt", "r") as f:
        content = f.read()
        assert "ä½ å¥½ä¸–ç•Œ" in content
        assert "ðŸŒ" in content
        assert "!@#$%^&*()" in content

@patch('builtins.open')
def test_configure_jsonl_file_mock(mock_open):
    """Test JSONL file mock configuration."""
    jsonl_data = [
        {"name": "Alice", "age": 28},
        {"name": "Bob", "age": 32}
    ]
    mock_file = configure_jsonl_file_mock(mock_open, jsonl_data)
    
    # Test JSONL file reading
    with open("/path/to/file.jsonl", "r") as f:
        content = f.read()
        assert "Alice" in content
        assert "Bob" in content
        assert content.count('\n') == 2

@patch('main.os.path')
def test_configure_directory_structure_mock(mock_os_path):
    """Test directory structure mock configuration."""
    directory_structure = {
        "/test/prompts": ["prompt1.txt", "prompt2.txt"],
        "/test/data": ["data1.json", "data2.json"]
    }
    mock_path = configure_directory_structure_mock(mock_os_path, directory_structure)
    
    # Test directory structure
    assert mock_path.exists("/test/prompts") == True
    assert mock_path.isdir("/test/prompts") == True
    assert mock_path.isfile("/test/prompts/prompt1.txt") == True
    assert mock_path.exists("/test/data/data1.json") == True

@patch('builtins.open')
def test_file_system_mock_integration_with_prompt_loading(mock_open):
    """Test file system mock integration with prompt loading."""
    mock_file = configure_file_io_mock(mock_open, "You are a test agent.")
    
    # Test prompt loading integration
    result = load_prompt_from_file("/path/to/prompt.txt")
    assert "You are a test agent." in result
    
    # Verify file was opened correctly
    mock_open.assert_called_once_with("/path/to/prompt.txt", "r", encoding="utf-8")

@patch('main.os.path')
@patch('main.load_prompt_from_file')
def test_file_system_mock_integration_with_baseline_prompts(mock_load_prompt, mock_os_path):
    """Test file system mock integration with baseline prompts loading."""
    # Configure path operations
    mock_path = configure_path_operations_mock(mock_os_path)
    mock_path.dirname.return_value = "/path/to/script"
    mock_path.abspath.return_value = "/absolute/path/to/script"
    mock_path.join.return_value = "/path/to/prompts/baseline"
    
    # Configure prompt loading
    mock_prompt = configure_prompt_file_mock(mock_load_prompt, "Test prompt")
    
    # Test baseline prompts loading
    result = load_baseline_prompts()
    assert mock_path.dirname.called
    assert mock_load_prompt.called

@patch('builtins.open')
@patch('main.json')
def test_file_system_mock_integration_with_jsonl_operations(mock_json, mock_open):
    """Test file system mock integration with JSONL operations."""
    # Configure JSON operations
    mock_json_ops = configure_json_operations_mock(mock_json)
    
    # Configure file I/O
    jsonl_data = [{"id": 1, "data": "test1"}, {"id": 2, "data": "test2"}]
    mock_file = configure_jsonl_file_mock(mock_open, jsonl_data)
    
    # Test JSONL reading
    result = list(read_jsonl_file("/path/to/data.jsonl"))
    assert len(result) == 2
    assert result[0]["id"] == 1
    assert result[1]["id"] == 2
    
    # Test JSONL writing
    write_jsonl_file(jsonl_data, "/path/to/output.jsonl")
    assert mock_file.write.called
```
### 6.3 Test Utilities
#### 6.3.1 Test Data Generators

**Function Information:**
- Purpose: Define standardized test utility functions and helper components for test data generation and management
- Scope: Reusable utility functions, helper methods, and support tools for generating test data used across multiple test suites
- Dependencies: pytest utilities, test data generators, assertion helpers, mock setup utilities

**Signature and Parameters:**
```python
def generate_test_questions(question_type: str = "standard", count: int = 1) -> list[str]:
    """Generate test questions for different scenarios."""
    
def generate_agent_messages(sender: str = "orchestrator", receiver: str = "planner", count: int = 1) -> list[AgentMessage]:
    """Generate test agent messages for communication testing."""
    
def generate_test_responses(response_type: str = "standard", count: int = 1) -> list[dict]:
    """Generate test LLM responses for different agent types."""
    
def generate_test_documents(document_type: str = "text", count: int = 1) -> list[Document]:
    """Generate test documents for file processing testing."""
```

**Dependencies:**
- `pytest` fixtures and utilities
- `unittest.mock` for mock setup
- Test data generation libraries
- Assertion and validation helpers

**Test Cases:**
- **Happy Path**: Standard data generation with typical behavior, normal operation patterns, successful data creation
- **Edge Cases**: Complex data structures, conditional generation, stateful data, large datasets, performance optimization
- **Error Conditions**: Generation failures, invalid configurations, missing dependencies, corrupted data states, validation errors

**Utility Configurations:**
- Data generation function setup and configuration patterns
- Helper method strategies for different data types
- Data response templates and structures
- Generation behavior customization and parameterization

**Direct Usage Examples:**
- How to apply the data generation utilities in test functions
- Parameter customization and override examples
- Integration with existing test patterns and fixtures

**Assertion Specifications:**
- Validation of data generation correctness and completeness
- Generated data structure verification
- Generation strategy effectiveness checks
- Data quality and reusability verification

**Code Examples:**
```python
import pytest
from typing import List, Dict, Any
from datetime import datetime
from langchain.schema import Document
from multi_agent_system import AgentMessage

def generate_test_questions(question_type: str = "standard", count: int = 1) -> List[str]:
    """Generate test questions for different scenarios."""
    question_templates = {
        "standard": [
            "What is artificial intelligence?",
            "How does machine learning work?",
            "What are the applications of AI in healthcare?",
            "Explain neural networks in simple terms",
            "What is the future of AI technology?"
        ],
        "complex": [
            "Analyze the ethical implications of AI decision-making in autonomous vehicles",
            "Compare and contrast supervised vs unsupervised learning with real-world examples",
            "Evaluate the impact of AI on job markets and propose mitigation strategies",
            "Investigate the relationship between AI and climate change solutions",
            "Examine the role of AI in scientific discovery and research methodology"
        ],
        "technical": [
            "Implement a binary classification algorithm using logistic regression",
            "Design a convolutional neural network architecture for image recognition",
            "Optimize hyperparameters for a random forest model",
            "Create a reinforcement learning agent for game playing",
            "Develop a natural language processing pipeline for sentiment analysis"
        ],
        "edge_case": [
            "",  # Empty question
            "A" * 1000,  # Very long question
            "Question with special chars: !@#$%^&*()_+-=[]{}|;':\",./<>?",
            "Unicode question: ä½ å¥½ä¸–ç•Œ ðŸŒ",
            "Question\nwith\nnewlines\nand\ttabs"
        ],
        "research_focused": [
            "Research the latest developments in quantum computing",
            "Find information about renewable energy technologies",
            "Investigate the history of space exploration",
            "Analyze current trends in cybersecurity",
            "Study the impact of social media on society"
        ]
    }
    
    templates = question_templates.get(question_type, question_templates["standard"])
    if count <= len(templates):
        return templates[:count]
    else:
        # Repeat templates if more questions needed
        return (templates * (count // len(templates) + 1))[:count]

def generate_agent_messages(sender: str = "orchestrator", receiver: str = "planner", 
                          message_type: str = "instruction", count: int = 1) -> List[AgentMessage]:
    """Generate test agent messages for communication testing."""
    message_templates = {
        "instruction": [
            "Please analyze the given question and create a research plan",
            "Execute the research steps as outlined in the plan",
            "Perform expert analysis on the research findings",
            "Review and critique the current workflow progress",
            "Synthesize all findings into a final comprehensive answer"
        ],
        "feedback": [
            "Good work on the research plan, proceed to execution",
            "The research findings are comprehensive and well-structured",
            "The expert analysis provides valuable insights",
            "The workflow is progressing well, continue with next steps",
            "The final answer effectively addresses the original question"
        ],
        "question": [
            "What is the current status of the research?",
            "Are there any additional resources needed?",
            "How should we proceed with the analysis?",
            "What are the key findings so far?",
            "Is the current approach effective?"
        ],
        "error": [
            "Error: Invalid input received",
            "Error: Research step failed",
            "Error: Analysis could not be completed",
            "Error: Communication timeout",
            "Error: Resource not available"
        ]
    }
    
    templates = message_templates.get(message_type, message_templates["instruction"])
    messages = []
    
    for i in range(count):
        template = templates[i % len(templates)]
        message = AgentMessage(
            timestamp=datetime.now().isoformat(),
            sender=sender,
            receiver=receiver,
            type=message_type,
            content=template,
            step_id=i + 1
        )
        messages.append(message)
    
    return messages

def generate_test_responses(response_type: str = "standard", count: int = 1) -> List[Dict[str, Any]]:
    """Generate test LLM responses for different agent types."""
    response_templates = {
        "planner": {
            "plan": "Research AI basics and applications",
            "steps": ["Research AI fundamentals", "Analyze AI applications", "Evaluate AI impact"]
        },
        "researcher": {
            "research_findings": "AI is artificial intelligence that enables machines to learn and make decisions",
            "sources": ["source1", "source2", "source3"]
        },
        "expert": {
            "expert_analysis": "AI has significant impact on multiple industries",
            "reasoning": "Based on research findings, AI transforms business processes and decision-making"
        },
        "critic": {
            "decision": "approve",
            "feedback": "Good work, proceed to next step"
        },
        "finalizer": {
            "final_answer": "AI is a transformative technology with wide-ranging applications",
            "reasoning_trace": "Comprehensive analysis of AI fundamentals, applications, and impact"
        },
        "error": {
            "error": "Invalid request",
            "details": "Missing required parameters"
        },
        "large": {
            "result": "x" * 10000,
            "metadata": {"size": "large", "type": "extensive"}
        }
    }
    
    template = response_templates.get(response_type, response_templates["standard"])
    responses = []
    
    for i in range(count):
        response = template.copy()
        if "step_id" in response:
            response["step_id"] = i + 1
        responses.append(response)
    
    return responses

def generate_test_documents(document_type: str = "text", count: int = 1) -> List[Document]:
    """Generate test documents for file processing testing."""
    document_templates = {
        "text": {
            "content": "This is a test document about artificial intelligence and its applications in modern technology.",
            "metadata": {"source": "test.txt", "type": "text", "page": 1}
        },
        "research": {
            "content": "Research findings indicate that AI has significant potential in healthcare, finance, and education sectors.",
            "metadata": {"source": "research.pdf", "type": "pdf", "page": 1}
        },
        "technical": {
            "content": "Technical specifications for neural network implementation with detailed architecture and parameters.",
            "metadata": {"source": "technical.docx", "type": "document", "page": 1}
        },
        "large": {
            "content": "Large document content " * 1000,
            "metadata": {"source": "large.txt", "type": "text", "page": 1, "size": "large"}
        },
        "special_chars": {
            "content": "Document with special characters: ä½ å¥½ä¸–ç•Œ ðŸŒ !@#$%^&*()_+-=[]{}|;':\",./<>?",
            "metadata": {"source": "special.txt", "type": "text", "page": 1}
        }
    }
    
    template = document_templates.get(document_type, document_templates["text"])
    documents = []
    
    for i in range(count):
        doc = Document(
            page_content=template["content"],
            metadata=template["metadata"].copy()
        )
        doc.metadata["id"] = i + 1
        documents.append(doc)
    
    return documents

def generate_test_state_data(state_type: str = "initial", count: int = 1) -> List[Dict[str, Any]]:
    """Generate test state data for GraphState testing."""
    state_templates = {
        "initial": {
            "agent_messages": [],
            "question": "What is AI?",
            "current_step": "input",
            "next_step": "",
            "research_steps": [],
            "expert_steps": [],
            "research_results": [],
            "planner_retry_count": 0,
            "researcher_retry_count": 0,
            "expert_retry_count": 0
        },
        "in_progress": {
            "agent_messages": generate_agent_messages("orchestrator", "planner", "instruction", 2),
            "question": "What is AI?",
            "current_step": "researcher",
            "next_step": "expert",
            "research_steps": ["Research AI basics", "Analyze AI applications"],
            "expert_steps": ["Evaluate AI impact"],
            "research_results": ["AI is artificial intelligence"],
            "planner_retry_count": 0,
            "researcher_retry_count": 1,
            "expert_retry_count": 0
        },
        "complete": {
            "agent_messages": generate_agent_messages("orchestrator", "finalizer", "instruction", 5),
            "question": "What is AI?",
            "current_step": "finalizer",
            "next_step": "",
            "research_steps": ["Research AI basics", "Analyze AI applications"],
            "expert_steps": ["Evaluate AI impact"],
            "research_results": ["AI is artificial intelligence", "AI has significant impact"],
            "planner_retry_count": 0,
            "researcher_retry_count": 0,
            "expert_retry_count": 0,
            "final_answer": "AI is a transformative technology",
            "final_reasoning_trace": "Complete analysis completed"
        },
        "error": {
            "agent_messages": generate_agent_messages("orchestrator", "planner", "error", 1),
            "question": "What is AI?",
            "current_step": "planner",
            "next_step": "",
            "research_steps": [],
            "expert_steps": [],
            "research_results": [],
            "planner_retry_count": 3,
            "researcher_retry_count": 0,
            "expert_retry_count": 0
        }
    }
    
    template = state_templates.get(state_type, state_templates["initial"])
    states = []
    
    for i in range(count):
        state = template.copy()
        state["id"] = i + 1
        states.append(state)
    
    return states

def generate_test_config_data(config_type: str = "standard", count: int = 1) -> List[Dict[str, Any]]:
    """Generate test configuration data for AgentConfig testing."""
    config_templates = {
        "standard": {
            "name": "test_agent",
            "provider": "openai",
            "model": "gpt-4",
            "temperature": 0.7,
            "output_schema": {"result": "string"},
            "system_prompt": "You are a test agent.",
            "retry_limit": 3
        },
        "minimal": {
            "name": "minimal_agent",
            "provider": "openai",
            "model": "gpt-3.5-turbo",
            "temperature": 0.0,
            "output_schema": {"output": "string"},
            "system_prompt": "",
            "retry_limit": 1
        },
        "complex": {
            "name": "complex_agent",
            "provider": "openai",
            "model": "gpt-4",
            "temperature": 0.9,
            "output_schema": {
                "analysis": "string",
                "reasoning": "string",
                "confidence": "number",
                "sources": "array"
            },
            "system_prompt": "You are a complex agent with detailed analysis capabilities.",
            "retry_limit": 5
        }
    }
    
    template = config_templates.get(config_type, config_templates["standard"])
    configs = []
    
    for i in range(count):
        config = template.copy()
        config["name"] = f"{config['name']}_{i + 1}"
        configs.append(config)
    
    return configs

# Test the utility functions
def test_generate_test_questions():
    """Test test question generation utility."""
    # Test standard questions
    questions = generate_test_questions("standard", 3)
    assert len(questions) == 3
    assert all(isinstance(q, str) for q in questions)
    assert "artificial intelligence" in questions[0].lower()
    
    # Test complex questions
    complex_questions = generate_test_questions("complex", 2)
    assert len(complex_questions) == 2
    assert "ethical implications" in complex_questions[0]
    
    # Test edge case questions
    edge_questions = generate_test_questions("edge_case", 1)
    assert len(edge_questions) == 1
    assert edge_questions[0] == ""  # Empty question

def test_generate_agent_messages():
    """Test agent message generation utility."""
    # Test instruction messages
    messages = generate_agent_messages("orchestrator", "planner", "instruction", 2)
    assert len(messages) == 2
    assert all(isinstance(m, dict) for m in messages)
    assert all(m["sender"] == "orchestrator" for m in messages)
    assert all(m["receiver"] == "planner" for m in messages)
    assert all(m["type"] == "instruction" for m in messages)
    
    # Test feedback messages
    feedback_messages = generate_agent_messages("critic", "researcher", "feedback", 1)
    assert len(feedback_messages) == 1
    assert feedback_messages[0]["sender"] == "critic"
    assert feedback_messages[0]["receiver"] == "researcher"

def test_generate_test_responses():
    """Test test response generation utility."""
    # Test planner responses
    responses = generate_test_responses("planner", 2)
    assert len(responses) == 2
    assert all("plan" in r for r in responses)
    assert all("steps" in r for r in responses)
    
    # Test error responses
    error_responses = generate_test_responses("error", 1)
    assert len(error_responses) == 1
    assert "error" in error_responses[0]
    assert "details" in error_responses[0]

def test_generate_test_documents():
    """Test test document generation utility."""
    # Test text documents
    documents = generate_test_documents("text", 2)
    assert len(documents) == 2
    assert all(isinstance(doc, Document) for doc in documents)
    assert all("artificial intelligence" in doc.page_content.lower() for doc in documents)
    
    # Test large documents
    large_docs = generate_test_documents("large", 1)
    assert len(large_docs) == 1
    assert len(large_docs[0].page_content) > 1000

def test_generate_test_state_data():
    """Test test state data generation utility."""
    # Test initial state
    states = generate_test_state_data("initial", 2)
    assert len(states) == 2
    assert all(s["current_step"] == "input" for s in states)
    assert all(len(s["agent_messages"]) == 0 for s in states)
    
    # Test complete state
    complete_states = generate_test_state_data("complete", 1)
    assert len(complete_states) == 1
    assert complete_states[0]["current_step"] == "finalizer"
    assert "final_answer" in complete_states[0]

def test_generate_test_config_data():
    """Test test configuration data generation utility."""
    # Test standard configs
    configs = generate_test_config_data("standard", 3)
    assert len(configs) == 3
    assert all(c["provider"] == "openai" for c in configs)
    assert all(c["model"] == "gpt-4" for c in configs)
    
    # Test complex configs
    complex_configs = generate_test_config_data("complex", 1)
    assert len(complex_configs) == 1
    assert "analysis" in complex_configs[0]["output_schema"]
    assert "confidence" in complex_configs[0]["output_schema"]

def test_data_generator_integration_with_tests():
    """Test data generator integration with actual test scenarios."""
    # Generate test data for a complete test scenario
    questions = generate_test_questions("research_focused", 1)
    messages = generate_agent_messages("orchestrator", "planner", "instruction", 1)
    responses = generate_test_responses("planner", 1)
    documents = generate_test_documents("research", 1)
    state_data = generate_test_state_data("in_progress", 1)
    
    # Verify data consistency
    assert len(questions) == 1
    assert len(messages) == 1
    assert len(responses) == 1
    assert len(documents) == 1
    assert len(state_data) == 1
    
    # Verify data quality
    assert "research" in questions[0].lower()
    assert messages[0]["type"] == "instruction"
    assert "plan" in responses[0]
    assert "research" in documents[0].page_content.lower()
    assert state_data[0]["current_step"] == "researcher"
```
#### 6.3.2 Assertion Helpers

**Function Information:**
- Purpose: Define standardized test utility functions and helper components for assertion validation and test verification
- Scope: Reusable utility functions, helper methods, and support tools for validating test results and verifying expected behaviors
- Dependencies: pytest utilities, assertion helpers, validation utilities, test verification tools

**Signature and Parameters:**
```python
def assert_agent_message_valid(message: AgentMessage, expected_sender: str = None, expected_receiver: str = None) -> None:
    """Validate agent message structure and content."""
    
def assert_graph_state_valid(state: GraphState, expected_step: str = None, expected_question: str = None) -> None:
    """Validate GraphState structure and required fields."""
    
def assert_llm_response_valid(response: dict, expected_fields: list[str] = None, response_type: str = "standard") -> None:
    """Validate LLM response structure and content."""
    
def assert_agent_config_valid(config: AgentConfig, expected_name: str = None, expected_provider: str = None) -> None:
    """Validate AgentConfig structure and required fields."""
```

**Dependencies:**
- `pytest` fixtures and utilities
- `unittest.mock` for mock setup
- Test data generation libraries
- Assertion and validation helpers

**Test Cases:**
- **Happy Path**: Standard assertion validation with typical behavior, normal verification patterns, successful validation execution
- **Edge Cases**: Complex validation chains, conditional assertions, stateful validation, large data structures, performance optimization
- **Error Conditions**: Validation failures, invalid configurations, missing dependencies, corrupted validation states, assertion errors

**Utility Configurations:**
- Assertion function setup and configuration patterns
- Helper method strategies for different validation types
- Validation response templates and error structures
- Assertion behavior customization and parameterization

**Direct Usage Examples:**
- How to apply the assertion utilities in test functions
- Parameter customization and override examples
- Integration with existing test patterns and fixtures

**Assertion Specifications:**
- Validation of assertion function correctness and completeness
- Assertion object behavior verification
- Helper strategy effectiveness checks
- Configuration completeness and reusability verification

**Code Examples:**
```python
import pytest
from typing import List, Dict, Any, Optional
from multi_agent_system import AgentMessage, GraphState, AgentConfig

def assert_agent_message_valid(message: AgentMessage, expected_sender: str = None, 
                              expected_receiver: str = None, expected_type: str = None) -> None:
    """Validate agent message structure and content."""
    # Validate basic structure
    assert isinstance(message, dict), "Message must be a dictionary"
    assert "timestamp" in message, "Message must have timestamp field"
    assert "sender" in message, "Message must have sender field"
    assert "receiver" in message, "Message must have receiver field"
    assert "type" in message, "Message must have type field"
    assert "content" in message, "Message must have content field"
    
    # Validate field types
    assert isinstance(message["timestamp"], str), "Timestamp must be a string"
    assert isinstance(message["sender"], str), "Sender must be a string"
    assert isinstance(message["receiver"], str), "Receiver must be a string"
    assert isinstance(message["type"], str), "Type must be a string"
    assert isinstance(message["content"], str), "Content must be a string"
    
    # Validate optional step_id
    if "step_id" in message:
        assert isinstance(message["step_id"], (int, type(None))), "Step ID must be integer or None"
    
    # Validate expected values if provided
    if expected_sender:
        assert message["sender"] == expected_sender, f"Expected sender '{expected_sender}', got '{message['sender']}'"
    if expected_receiver:
        assert message["receiver"] == expected_receiver, f"Expected receiver '{expected_receiver}', got '{message['receiver']}'"
    if expected_type:
        assert message["type"] == expected_type, f"Expected type '{expected_type}', got '{message['type']}'"
    
    # Validate content is not empty
    assert len(message["content"].strip()) > 0, "Message content cannot be empty"
    
    # Validate timestamp format (basic ISO format check)
    assert "T" in message["timestamp"] or "-" in message["timestamp"], "Timestamp should be in ISO format"

def assert_graph_state_valid(state: GraphState, expected_step: str = None, expected_question: str = None,
                           expected_next_step: str = None) -> None:
    """Validate GraphState structure and required fields."""
    # Validate basic structure
    assert isinstance(state, dict), "State must be a dictionary"
    
    # Validate required fields
    required_fields = [
        "agent_messages", "question", "current_step", "next_step",
        "research_steps", "expert_steps", "researcher_states",
        "current_research_index", "research_results", "expert_state",
        "expert_answer", "expert_reasoning", "critic_planner_decision",
        "critic_planner_feedback", "critic_researcher_decision",
        "critic_researcher_feedback", "critic_expert_decision",
        "critic_expert_feedback", "final_answer", "final_reasoning_trace",
        "planner_retry_count", "researcher_retry_count", "expert_retry_count",
        "planner_retry_limit", "researcher_retry_limit", "expert_retry_limit"
    ]
    
    for field in required_fields:
        assert field in state, f"State must have field '{field}'"
    
    # Validate field types
    assert isinstance(state["agent_messages"], list), "agent_messages must be a list"
    assert isinstance(state["question"], str), "question must be a string"
    assert isinstance(state["current_step"], str), "current_step must be a string"
    assert isinstance(state["next_step"], str), "next_step must be a string"
    assert isinstance(state["research_steps"], list), "research_steps must be a list"
    assert isinstance(state["expert_steps"], list), "expert_steps must be a list"
    assert isinstance(state["researcher_states"], dict), "researcher_states must be a dictionary"
    assert isinstance(state["current_research_index"], int), "current_research_index must be an integer"
    assert isinstance(state["research_results"], list), "research_results must be a list"
    assert isinstance(state["planner_retry_count"], int), "planner_retry_count must be an integer"
    assert isinstance(state["researcher_retry_count"], int), "researcher_retry_count must be an integer"
    assert isinstance(state["expert_retry_count"], int), "expert_retry_count must be an integer"
    
    # Validate retry counts are non-negative
    assert state["planner_retry_count"] >= 0, "planner_retry_count must be non-negative"
    assert state["researcher_retry_count"] >= 0, "researcher_retry_count must be non-negative"
    assert state["expert_retry_count"] >= 0, "expert_retry_count must be non-negative"
    
    # Validate retry limits are positive
    assert state["planner_retry_limit"] > 0, "planner_retry_limit must be positive"
    assert state["researcher_retry_limit"] > 0, "researcher_retry_limit must be positive"
    assert state["expert_retry_limit"] > 0, "expert_retry_limit must be positive"
    
    # Validate retry counts don't exceed limits
    assert state["planner_retry_count"] <= state["planner_retry_limit"], "planner_retry_count cannot exceed limit"
    assert state["researcher_retry_count"] <= state["researcher_retry_limit"], "researcher_retry_count cannot exceed limit"
    assert state["expert_retry_count"] <= state["expert_retry_limit"], "expert_retry_count cannot exceed limit"
    
    # Validate expected values if provided
    if expected_step:
        assert state["current_step"] == expected_step, f"Expected step '{expected_step}', got '{state['current_step']}'"
    if expected_question:
        assert state["question"] == expected_question, f"Expected question '{expected_question}', got '{state['question']}'"
    if expected_next_step:
        assert state["next_step"] == expected_next_step, f"Expected next step '{expected_next_step}', got '{state['next_step']}'"

def assert_llm_response_valid(response: dict, expected_fields: List[str] = None, 
                            response_type: str = "standard") -> None:
    """Validate LLM response structure and content."""
    # Validate basic structure
    assert isinstance(response, dict), "Response must be a dictionary"
    
    # Validate required fields based on response type
    required_fields_map = {
        "standard": ["result"],
        "planner": ["plan", "steps"],
        "researcher": ["research_findings"],
        "expert": ["expert_analysis", "reasoning"],
        "critic": ["decision", "feedback"],
        "finalizer": ["final_answer", "reasoning_trace"],
        "error": ["error", "details"]
    }
    
    required_fields = required_fields_map.get(response_type, required_fields_map["standard"])
    if expected_fields:
        required_fields = expected_fields
    
    for field in required_fields:
        assert field in response, f"Response must have field '{field}'"
        assert response[field] is not None, f"Field '{field}' cannot be None"
    
    # Validate field types
    for field in required_fields:
        if field in response:
            if field in ["steps", "sources"]:
                assert isinstance(response[field], list), f"Field '{field}' must be a list"
            elif field in ["decision"]:
                assert isinstance(response[field], str), f"Field '{field}' must be a string"
                assert response[field] in ["approve", "reject", "modify"], f"Decision must be approve, reject, or modify"
            else:
                assert isinstance(response[field], str), f"Field '{field}' must be a string"
                assert len(response[field].strip()) > 0, f"Field '{field}' cannot be empty"
    
    # Validate error responses
    if response_type == "error":
        assert "error" in response, "Error response must have error field"
        assert "details" in response, "Error response must have details field"
        assert len(response["error"]) > 0, "Error message cannot be empty"

def assert_agent_config_valid(config: AgentConfig, expected_name: str = None, 
                            expected_provider: str = None, expected_model: str = None) -> None:
    """Validate AgentConfig structure and required fields."""
    # Validate basic structure
    assert hasattr(config, 'name'), "Config must have name attribute"
    assert hasattr(config, 'provider'), "Config must have provider attribute"
    assert hasattr(config, 'model'), "Config must have model attribute"
    assert hasattr(config, 'temperature'), "Config must have temperature attribute"
    assert hasattr(config, 'output_schema'), "Config must have output_schema attribute"
    assert hasattr(config, 'system_prompt'), "Config must have system_prompt attribute"
    assert hasattr(config, 'retry_limit'), "Config must have retry_limit attribute"
    
    # Validate field types
    assert isinstance(config.name, str), "name must be a string"
    assert isinstance(config.provider, str), "provider must be a string"
    assert isinstance(config.model, str), "model must be a string"
    assert isinstance(config.temperature, (int, float)), "temperature must be a number"
    assert isinstance(config.output_schema, dict), "output_schema must be a dictionary"
    assert isinstance(config.system_prompt, str), "system_prompt must be a string"
    assert isinstance(config.retry_limit, int), "retry_limit must be an integer"
    
    # Validate field values
    assert len(config.name.strip()) > 0, "name cannot be empty"
    assert len(config.provider.strip()) > 0, "provider cannot be empty"
    assert len(config.model.strip()) > 0, "model cannot be empty"
    assert 0.0 <= config.temperature <= 2.0, "temperature must be between 0.0 and 2.0"
    assert len(config.output_schema) > 0, "output_schema cannot be empty"
    assert config.retry_limit > 0, "retry_limit must be positive"
    
    # Validate expected values if provided
    if expected_name:
        assert config.name == expected_name, f"Expected name '{expected_name}', got '{config.name}'"
    if expected_provider:
        assert config.provider == expected_provider, f"Expected provider '{expected_provider}', got '{config.provider}'"
    if expected_model:
        assert config.model == expected_model, f"Expected model '{expected_model}', got '{config.model}'"

def assert_document_valid(document: Document, expected_source: str = None, 
                         expected_content_length: int = None) -> None:
    """Validate Document structure and content."""
    # Validate basic structure
    assert hasattr(document, 'page_content'), "Document must have page_content attribute"
    assert hasattr(document, 'metadata'), "Document must have metadata attribute"
    
    # Validate field types
    assert isinstance(document.page_content, str), "page_content must be a string"
    assert isinstance(document.metadata, dict), "metadata must be a dictionary"
    
    # Validate content
    assert len(document.page_content.strip()) > 0, "page_content cannot be empty"
    
    # Validate metadata
    assert "source" in document.metadata, "metadata must have source field"
    assert isinstance(document.metadata["source"], str), "source must be a string"
    
    # Validate expected values if provided
    if expected_source:
        assert document.metadata["source"] == expected_source, f"Expected source '{expected_source}', got '{document.metadata['source']}'"
    if expected_content_length:
        assert len(document.page_content) >= expected_content_length, f"Content length must be at least {expected_content_length}"

def assert_workflow_progression_valid(initial_state: GraphState, final_state: GraphState,
                                    expected_steps: List[str] = None) -> None:
    """Validate workflow progression from initial to final state."""
    # Validate states
    assert_graph_state_valid(initial_state)
    assert_graph_state_valid(final_state)
    
    # Validate progression
    assert len(final_state["agent_messages"]) >= len(initial_state["agent_messages"]), "Final state should have more messages"
    assert len(final_state["research_steps"]) >= len(initial_state["research_steps"]), "Final state should have more research steps"
    assert len(final_state["expert_steps"]) >= len(initial_state["expert_steps"]), "Final state should have more expert steps"
    
    # Validate step progression if expected steps provided
    if expected_steps:
        for step in expected_steps:
            # Check if step appears in any message
            step_found = any(step in msg.get("content", "") for msg in final_state["agent_messages"])
            assert step_found, f"Expected step '{step}' not found in workflow progression"
    
    # Validate final state completion
    if final_state["current_step"] == "finalizer":
        assert len(final_state["final_answer"].strip()) > 0, "Final answer should not be empty when workflow is complete"
        assert len(final_state["final_reasoning_trace"].strip()) > 0, "Final reasoning trace should not be empty when workflow is complete"

def assert_error_handling_valid(operation_result: Any, expected_error_type: type = None,
                              expected_error_message: str = None) -> None:
    """Validate error handling behavior."""
    if expected_error_type:
        # Expect an exception to be raised
        with pytest.raises(expected_error_type) as exc_info:
            if callable(operation_result):
                operation_result()
            else:
                raise operation_result
        
        if expected_error_message:
            assert expected_error_message in str(exc_info.value), f"Expected error message '{expected_error_message}' not found in '{str(exc_info.value)}'"
    else:
        # Expect successful operation
        if callable(operation_result):
            result = operation_result()
            assert result is not None, "Operation should return a result"
        else:
            assert operation_result is not None, "Operation result should not be None"

def assert_performance_valid(operation_time: float, max_time: float = 5.0) -> None:
    """Validate performance requirements."""
    assert isinstance(operation_time, (int, float)), "Operation time must be a number"
    assert operation_time >= 0, "Operation time must be non-negative"
    assert operation_time <= max_time, f"Operation time {operation_time}s exceeds maximum {max_time}s"

# Test the assertion helpers
def test_assert_agent_message_valid():
    """Test agent message validation utility."""
    # Valid message
    valid_message = {
        "timestamp": "2024-01-01T00:00:00Z",
        "sender": "orchestrator",
        "receiver": "planner",
        "type": "instruction",
        "content": "Test message content",
        "step_id": 1
    }
    
    # Should not raise any exceptions
    assert_agent_message_valid(valid_message)
    assert_agent_message_valid(valid_message, "orchestrator", "planner", "instruction")
    
    # Invalid message - missing field
    invalid_message = {
        "timestamp": "2024-01-01T00:00:00Z",
        "sender": "orchestrator",
        "type": "instruction",
        "content": "Test message content"
    }
    
    with pytest.raises(AssertionError, match="Message must have receiver field"):
        assert_agent_message_valid(invalid_message)

def test_assert_graph_state_valid():
    """Test graph state validation utility."""
    # Valid state
    valid_state = {
        "agent_messages": [],
        "question": "What is AI?",
        "current_step": "input",
        "next_step": "",
        "research_steps": [],
        "expert_steps": [],
        "researcher_states": {},
        "current_research_index": 0,
        "research_results": [],
        "expert_state": None,
        "expert_answer": None,
        "expert_reasoning": "",
        "critic_planner_decision": "",
        "critic_planner_feedback": "",
        "critic_researcher_decision": "",
        "critic_researcher_feedback": "",
        "critic_expert_decision": "",
        "critic_expert_feedback": "",
        "final_answer": "",
        "final_reasoning_trace": "",
        "planner_retry_count": 0,
        "researcher_retry_count": 0,
        "expert_retry_count": 0,
        "planner_retry_limit": 3,
        "researcher_retry_limit": 5,
        "expert_retry_limit": 5
    }
    
    # Should not raise any exceptions
    assert_graph_state_valid(valid_state)
    assert_graph_state_valid(valid_state, "input", "What is AI?")
    
    # Invalid state - missing field
    invalid_state = valid_state.copy()
    del invalid_state["question"]
    
    with pytest.raises(AssertionError, match="State must have field 'question'"):
        assert_graph_state_valid(invalid_state)

def test_assert_llm_response_valid():
    """Test LLM response validation utility."""
    # Valid planner response
    valid_planner_response = {
        "plan": "Research AI basics",
        "steps": ["Step 1", "Step 2", "Step 3"]
    }
    
    # Should not raise any exceptions
    assert_llm_response_valid(valid_planner_response, response_type="planner")
    
    # Invalid response - missing required field
    invalid_response = {
        "plan": "Research AI basics"
    }
    
    with pytest.raises(AssertionError, match="Response must have field 'steps'"):
        assert_llm_response_valid(invalid_response, response_type="planner")

def test_assert_agent_config_valid():
    """Test agent config validation utility."""
    # Valid config
    valid_config = AgentConfig(
        name="test_agent",
        provider="openai",
        model="gpt-4",
        temperature=0.7,
        output_schema={"result": "string"},
        system_prompt="You are a test agent.",
        retry_limit=3
    )
    
    # Should not raise any exceptions
    assert_agent_config_valid(valid_config)
    assert_agent_config_valid(valid_config, "test_agent", "openai", "gpt-4")
    
    # Invalid config - invalid temperature
    invalid_config = AgentConfig(
        name="test_agent",
        provider="openai",
        model="gpt-4",
        temperature=3.0,  # Invalid temperature
        output_schema={"result": "string"},
        system_prompt="You are a test agent.",
        retry_limit=3
    )
    
    with pytest.raises(AssertionError, match="temperature must be between 0.0 and 2.0"):
        assert_agent_config_valid(invalid_config)

def test_assert_workflow_progression_valid():
    """Test workflow progression validation utility."""
    # Initial state
    initial_state = {
        "agent_messages": [],
        "question": "What is AI?",
        "current_step": "input",
        "next_step": "",
        "research_steps": [],
        "expert_steps": [],
        "researcher_states": {},
        "current_research_index": 0,
        "research_results": [],
        "expert_state": None,
        "expert_answer": None,
        "expert_reasoning": "",
        "critic_planner_decision": "",
        "critic_planner_feedback": "",
        "critic_researcher_decision": "",
        "critic_researcher_feedback": "",
        "critic_expert_decision": "",
        "critic_expert_feedback": "",
        "final_answer": "",
        "final_reasoning_trace": "",
        "planner_retry_count": 0,
        "researcher_retry_count": 0,
        "expert_retry_count": 0,
        "planner_retry_limit": 3,
        "researcher_retry_limit": 5,
        "expert_retry_limit": 5
    }
    
    # Final state with progression
    final_state = {
        "agent_messages": [{"content": "Research AI basics"}],
        "question": "What is AI?",
        "current_step": "researcher",
        "next_step": "expert",
        "research_steps": ["Research AI basics"],
        "expert_steps": [],
        "researcher_states": {},
        "current_research_index": 1,
        "research_results": ["AI is artificial intelligence"],
        "expert_state": None,
        "expert_answer": None,
        "expert_reasoning": "",
        "critic_planner_decision": "",
        "critic_planner_feedback": "",
        "critic_researcher_decision": "",
        "critic_researcher_feedback": "",
        "critic_expert_decision": "",
        "critic_expert_feedback": "",
        "final_answer": "",
        "final_reasoning_trace": "",
        "planner_retry_count": 0,
        "researcher_retry_count": 0,
        "expert_retry_count": 0,
        "planner_retry_limit": 3,
        "researcher_retry_limit": 5,
        "expert_retry_limit": 5
    }
    
    # Should not raise any exceptions
    assert_workflow_progression_valid(initial_state, final_state)
    assert_workflow_progression_valid(initial_state, final_state, ["Research AI basics"])

def test_assertion_helper_integration_with_tests():
    """Test assertion helper integration with actual test scenarios."""
    # Generate test data
    messages = generate_agent_messages("orchestrator", "planner", "instruction", 1)
    responses = generate_test_responses("planner", 1)
    configs = generate_test_config_data("standard", 1)
    
    # Validate generated data using assertion helpers
    assert_agent_message_valid(messages[0], "orchestrator", "planner", "instruction")
    assert_llm_response_valid(responses[0], response_type="planner")
    assert_agent_config_valid(configs[0], "test_agent_1", "openai", "gpt-4")
    
    # Test error handling
    def failing_operation():
        raise ValueError("Test error")
    
    assert_error_handling_valid(failing_operation, ValueError, "Test error")
    
    # Test performance validation
    import time
    start_time = time.time()
    time.sleep(0.1)  # Simulate operation
    operation_time = time.time() - start_time
    assert_performance_valid(operation_time, 1.0)
```
#### 6.3.3 Mock Setup Utilities

**Function Information:**
- Purpose: Define standardized test utility functions and helper components for mock setup and configuration management
- Scope: Reusable utility functions, helper methods, and support tools for setting up and managing mocks across multiple test suites
- Dependencies: pytest utilities, unittest.mock, test data generators, assertion helpers

**Signature and Parameters:**
```python
def setup_complete_mock_environment(mock_config: dict = None) -> dict[str, Any]:
    """Set up complete mock environment for system testing."""
    
def setup_agent_mocks(agent_types: list[str] = None) -> dict[str, Mock]:
    """Set up mocks for all agent types."""
    
def setup_external_service_mocks(services: list[str] = None) -> dict[str, Mock]:
    """Set up mocks for all external services."""
    
def setup_file_system_mocks(file_operations: list[str] = None) -> dict[str, Mock]:
    """Set up mocks for file system operations."""
```

**Dependencies:**
- `pytest` fixtures and utilities
- `unittest.mock` for mock setup
- Test data generation libraries
- Assertion and validation helpers

**Test Cases:**
- **Happy Path**: Standard mock setup with typical behavior, normal configuration patterns, successful mock creation
- **Edge Cases**: Complex mock chains, conditional setup, stateful mocks, large mock configurations, performance optimization
- **Error Conditions**: Setup failures, invalid configurations, missing dependencies, corrupted mock states, configuration errors

**Utility Configurations:**
- Mock setup function configuration patterns
- Helper method strategies for different mock types
- Mock response templates and data structures
- Setup behavior customization and parameterization

**Direct Usage Examples:**
- How to apply the mock setup utilities in test functions
- Parameter customization and override examples
- Integration with existing test patterns and fixtures

**Assertion Specifications:**
- Validation of mock setup correctness and completeness
- Mock object behavior verification
- Setup strategy effectiveness checks
- Configuration completeness and reusability verification

**Code Examples:**
```python
import pytest
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, List, Any, Optional
from contextlib import contextmanager

def setup_complete_mock_environment(mock_config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Set up complete mock environment for system testing."""
    if mock_config is None:
        mock_config = {
            "llm_responses": "standard",
            "external_services": ["youtube", "wikipedia", "web_search"],
            "file_operations": ["read", "write"],
            "agent_types": ["planner", "researcher", "expert", "critic", "finalizer"]
        }
    
    mock_environment = {}
    
    # Setup LLM mocks
    with patch('multi_agent_system.ChatOpenAI') as mock_chat_openai:
        mock_llm = Mock()
        mock_chat_openai.return_value = mock_llm
        mock_llm.with_structured_output.return_value = mock_llm
        mock_llm.invoke.return_value = Mock(content='{"result": "test_response"}')
        mock_environment['llm'] = mock_llm
        mock_environment['chat_openai'] = mock_chat_openai
    
    # Setup external service mocks
    external_services = mock_config.get("external_services", [])
    if "youtube" in external_services:
        with patch('multi_agent_system.YouTubeTranscriptApi') as mock_youtube:
            mock_youtube.return_value.get_transcript.return_value = [{"text": "Test transcript"}]
            mock_environment['youtube'] = mock_youtube
    
    if "wikipedia" in external_services:
        with patch('multi_agent_system.WikipediaQueryRun') as mock_wikipedia:
            mock_wikipedia.return_value.run.return_value = "Test Wikipedia content"
            mock_environment['wikipedia'] = mock_wikipedia
    
    if "web_search" in external_services:
        with patch('multi_agent_system.TavilySearchResults') as mock_tavily:
            mock_tavily.return_value.invoke.return_value = [{"title": "Test result"}]
            mock_environment['web_search'] = mock_tavily
    
    # Setup file system mocks
    file_operations = mock_config.get("file_operations", [])
    if "read" in file_operations or "write" in file_operations:
        with patch('builtins.open') as mock_open:
            mock_file = Mock()
            mock_open.return_value.__enter__.return_value = mock_file
            mock_file.read.return_value = "Test file content"
            mock_file.write = Mock()
            mock_environment['file_io'] = mock_open
    
    return mock_environment

def setup_agent_mocks(agent_types: List[str] = None) -> Dict[str, Mock]:
    """Set up mocks for all agent types."""
    if agent_types is None:
        agent_types = ["planner", "researcher", "expert", "critic", "finalizer"]
    
    agent_mocks = {}
    
    # Setup LLM mock for agents
    with patch('multi_agent_system.ChatOpenAI') as mock_chat_openai:
        mock_llm = Mock()
        mock_chat_openai.return_value = mock_llm
        mock_llm.with_structured_output.return_value = mock_llm
        
        # Configure different responses for different agent types
        response_templates = {
            "planner": '{"plan": "Test plan", "steps": ["Step 1", "Step 2"]}',
            "researcher": '{"research_findings": "Test findings", "sources": ["source1"]}',
            "expert": '{"expert_analysis": "Test analysis", "reasoning": "Test reasoning"}',
            "critic": '{"decision": "approve", "feedback": "Test feedback"}',
            "finalizer": '{"final_answer": "Test answer", "reasoning_trace": "Test trace"}'
        }
        
        for agent_type in agent_types:
            mock_agent_llm = Mock()
            mock_agent_llm.invoke.return_value = Mock(
                content=response_templates.get(agent_type, '{"result": "test"}')
            )
            agent_mocks[f"{agent_type}_llm"] = mock_agent_llm
        
        agent_mocks['chat_openai'] = mock_chat_openai
    
    return agent_mocks

def setup_external_service_mocks(services: List[str] = None) -> Dict[str, Mock]:
    """Set up mocks for all external services."""
    if services is None:
        services = ["youtube", "wikipedia", "web_search", "browser_mcp"]
    
    service_mocks = {}
    
    if "youtube" in services:
        with patch('multi_agent_system.YouTubeTranscriptApi') as mock_youtube:
            mock_api = Mock()
            mock_youtube.return_value = mock_api
            mock_api.get_transcript.return_value = [
                {"text": "This is a YouTube video about AI", "start": 0.0, "duration": 5.0},
                {"text": "It covers machine learning concepts", "start": 5.0, "duration": 5.0}
            ]
            service_mocks['youtube'] = mock_youtube
    
    if "wikipedia" in services:
        with patch('multi_agent_system.WikipediaQueryRun') as mock_wikipedia:
            mock_wiki = Mock()
            mock_wikipedia.return_value = mock_wiki
            mock_wiki.run.return_value = "Artificial Intelligence (AI) is intelligence demonstrated by machines."
            service_mocks['wikipedia'] = mock_wikipedia
    
    if "web_search" in services:
        with patch('multi_agent_system.TavilySearchResults') as mock_tavily:
            mock_search = Mock()
            mock_tavily.return_value = mock_search
            mock_search.invoke.return_value = [
                {"title": "AI Research Paper", "snippet": "Recent advances in AI...", "url": "https://example.com"},
                {"title": "AI Applications", "snippet": "AI is used in various industries...", "url": "https://example.com"}
            ]
            service_mocks['web_search'] = mock_tavily
    
    if "browser_mcp" in services:
        with patch('multi_agent_system.get_browser_mcp_tools') as mock_browser:
            mock_browser_tool = Mock()
            mock_browser_tool.name = "browser_tool"
            mock_browser_tool.invoke.return_value = {
                "page_content": "<html><body><h1>AI Information</h1></body></html>",
                "title": "AI Information Page"
            }
            mock_browser.return_value = [mock_browser_tool]
            service_mocks['browser_mcp'] = mock_browser
    
    return service_mocks

def setup_file_system_mocks(file_operations: List[str] = None) -> Dict[str, Mock]:
    """Set up mocks for file system operations."""
    if file_operations is None:
        file_operations = ["read", "write", "path_operations"]
    
    file_mocks = {}
    
    if "read" in file_operations or "write" in file_operations:
        with patch('builtins.open') as mock_open:
            mock_file = Mock()
            mock_open.return_value.__enter__.return_value = mock_file
            mock_file.read.return_value = "Test file content with AI information"
            mock_file.write = Mock()
            mock_file.__iter__ = Mock(return_value=iter(["line1", "line2", "line3"]))
            file_mocks['file_io'] = mock_open
    
    if "path_operations" in file_operations:
        with patch('main.os.path') as mock_os_path:
            mock_os_path.exists.return_value = True
            mock_os_path.isfile.return_value = True
            mock_os_path.isdir.return_value = False
            mock_os_path.dirname.return_value = "/path/to/parent"
            mock_os_path.abspath.return_value = "/absolute/path/to/file"
            mock_os_path.join.return_value = "/path/to/joined/file"
            file_mocks['path_operations'] = mock_os_path
    
    if "json_operations" in file_operations:
        with patch('main.json') as mock_json:
            mock_json.loads.return_value = {"name": "test", "value": 123}
            mock_json.dumps.return_value = '{"name": "test", "value": 123}'
            file_mocks['json_operations'] = mock_json
    
    return file_mocks

@contextmanager
def mock_environment_context(mock_config: Dict[str, Any] = None):
    """Context manager for setting up and tearing down mock environment."""
    mock_environment = setup_complete_mock_environment(mock_config)
    try:
        yield mock_environment
    finally:
        # Clean up any mock state if needed
        for mock_name, mock_obj in mock_environment.items():
            if hasattr(mock_obj, 'reset_mock'):
                mock_obj.reset_mock()

def setup_error_scenario_mocks(error_type: str = "llm_error", error_message: str = None) -> Dict[str, Any]:
    """Set up mocks for error scenario testing."""
    error_mocks = {}
    
    if error_type == "llm_error":
        with patch('multi_agent_system.ChatOpenAI') as mock_chat_openai:
            mock_llm = Mock()
            mock_chat_openai.return_value = mock_llm
            mock_llm.with_structured_output.return_value = mock_llm
            mock_llm.invoke.side_effect = Exception(error_message or "LLM invoke failed")
            error_mocks['llm'] = mock_llm
            error_mocks['chat_openai'] = mock_chat_openai
    
    elif error_type == "external_service_error":
        with patch('multi_agent_system.YouTubeTranscriptApi') as mock_youtube:
            mock_youtube.side_effect = Exception(error_message or "YouTube API error")
            error_mocks['youtube'] = mock_youtube
    
    elif error_type == "file_system_error":
        with patch('builtins.open') as mock_open:
            mock_open.side_effect = FileNotFoundError(error_message or "File not found")
            error_mocks['file_io'] = mock_open
    
    return error_mocks

def setup_performance_test_mocks(operation_type: str = "slow_llm", delay: float = 1.0) -> Dict[str, Any]:
    """Set up mocks for performance testing."""
    import time
    
    performance_mocks = {}
    
    if operation_type == "slow_llm":
        with patch('multi_agent_system.ChatOpenAI') as mock_chat_openai:
            mock_llm = Mock()
            mock_chat_openai.return_value = mock_llm
            mock_llm.with_structured_output.return_value = mock_llm
            
            def slow_invoke(*args, **kwargs):
                time.sleep(delay)
                return Mock(content='{"result": "slow_response"}')
            
            mock_llm.invoke.side_effect = slow_invoke
            performance_mocks['llm'] = mock_llm
            performance_mocks['chat_openai'] = mock_chat_openai
    
    elif operation_type == "slow_external_service":
        with patch('multi_agent_system.YouTubeTranscriptApi') as mock_youtube:
            def slow_get_transcript(*args, **kwargs):
                time.sleep(delay)
                return [{"text": "Slow transcript"}]
            
            mock_youtube.return_value.get_transcript.side_effect = slow_get_transcript
            performance_mocks['youtube'] = mock_youtube
    
    return performance_mocks

def setup_large_data_mocks(data_size: str = "medium") -> Dict[str, Any]:
    """Set up mocks for large data testing."""
    large_data_mocks = {}
    
    # Generate large response data
    data_sizes = {
        "small": "x" * 100,
        "medium": "x" * 10000,
        "large": "x" * 100000,
        "very_large": "x" * 1000000
    }
    
    large_content = data_sizes.get(data_size, data_sizes["medium"])
    
    with patch('multi_agent_system.ChatOpenAI') as mock_chat_openai:
        mock_llm = Mock()
        mock_chat_openai.return_value = mock_llm
        mock_llm.with_structured_output.return_value = mock_llm
        mock_llm.invoke.return_value = Mock(content=f'{{"result": "{large_content}"}}')
        large_data_mocks['llm'] = mock_llm
        large_data_mocks['chat_openai'] = mock_chat_openai
    
    return large_data_mocks

# Test the mock setup utilities
def test_setup_complete_mock_environment():
    """Test complete mock environment setup utility."""
    mock_environment = setup_complete_mock_environment()
    
    # Verify all expected mocks are present
    assert 'llm' in mock_environment
    assert 'chat_openai' in mock_environment
    assert 'youtube' in mock_environment
    assert 'wikipedia' in mock_environment
    assert 'web_search' in mock_environment
    assert 'file_io' in mock_environment
    
    # Verify mock functionality
    mock_llm = mock_environment['llm']
    response = mock_llm.invoke("test input")
    assert response.content == '{"result": "test_response"}'

def test_setup_agent_mocks():
    """Test agent mocks setup utility."""
    agent_mocks = setup_agent_mocks(["planner", "researcher"])
    
    # Verify agent mocks are present
    assert 'planner_llm' in agent_mocks
    assert 'researcher_llm' in agent_mocks
    assert 'chat_openai' in agent_mocks
    
    # Verify different responses for different agents
    planner_response = agent_mocks['planner_llm'].invoke("test")
    assert "plan" in planner_response.content
    
    researcher_response = agent_mocks['researcher_llm'].invoke("test")
    assert "research_findings" in researcher_response.content

def test_setup_external_service_mocks():
    """Test external service mocks setup utility."""
    service_mocks = setup_external_service_mocks(["youtube", "wikipedia"])
    
    # Verify service mocks are present
    assert 'youtube' in service_mocks
    assert 'wikipedia' in service_mocks
    
    # Verify service functionality
    youtube_mock = service_mocks['youtube']
    transcript = youtube_mock.return_value.get_transcript("test_url")
    assert len(transcript) == 2
    assert "AI" in transcript[0]["text"]

def test_setup_file_system_mocks():
    """Test file system mocks setup utility."""
    file_mocks = setup_file_system_mocks(["read", "write", "path_operations"])
    
    # Verify file mocks are present
    assert 'file_io' in file_mocks
    assert 'path_operations' in file_mocks
    
    # Verify file operations
    mock_open = file_mocks['file_io']
    with open("test.txt", "r") as f:
        content = f.read()
        assert "AI information" in content

def test_mock_environment_context():
    """Test mock environment context manager."""
    with mock_environment_context() as mock_env:
        # Verify environment is set up
        assert 'llm' in mock_env
        assert 'youtube' in mock_env
        
        # Test mock functionality
        mock_llm = mock_env['llm']
        response = mock_llm.invoke("test")
        assert response.content == '{"result": "test_response"}'
    
    # Verify cleanup (mocks should be reset)

def test_setup_error_scenario_mocks():
    """Test error scenario mocks setup utility."""
    error_mocks = setup_error_scenario_mocks("llm_error", "Test LLM error")
    
    # Verify error mock is present
    assert 'llm' in error_mocks
    
    # Verify error behavior
    mock_llm = error_mocks['llm']
    with pytest.raises(Exception, match="Test LLM error"):
        mock_llm.invoke("test input")

def test_setup_performance_test_mocks():
    """Test performance test mocks setup utility."""
    import time
    
    performance_mocks = setup_performance_test_mocks("slow_llm", 0.1)
    
    # Verify performance mock is present
    assert 'llm' in performance_mocks
    
    # Test performance behavior
    mock_llm = performance_mocks['llm']
    start_time = time.time()
    response = mock_llm.invoke("test input")
    execution_time = time.time() - start_time
    
    assert execution_time >= 0.1  # Should take at least the specified delay
    assert "slow_response" in response.content

def test_setup_large_data_mocks():
    """Test large data mocks setup utility."""
    large_data_mocks = setup_large_data_mocks("large")
    
    # Verify large data mock is present
    assert 'llm' in large_data_mocks
    
    # Test large data behavior
    mock_llm = large_data_mocks['llm']
    response = mock_llm.invoke("test input")
    assert len(response.content) > 100000  # Should be large data

def test_mock_setup_utility_integration_with_tests():
    """Test mock setup utility integration with actual test scenarios."""
    # Setup complete mock environment
    mock_env = setup_complete_mock_environment({
        "llm_responses": "planner",
        "external_services": ["youtube"],
        "file_operations": ["read"],
        "agent_types": ["planner"]
    })
    
    # Verify all mocks are properly configured
    assert 'llm' in mock_env
    assert 'youtube' in mock_env
    assert 'file_io' in mock_env
    
    # Test integration with actual components
    mock_llm = mock_env['llm']
    response = mock_llm.invoke("Create a research plan")
    assert "test_response" in response.content
    
    # Test external service integration
    youtube_mock = mock_env['youtube']
    transcript = youtube_mock.return_value.get_transcript("https://youtube.com/watch?v=test")
    assert len(transcript) > 0
    
    # Test file system integration
    mock_open = mock_env['file_io']
    with open("test.txt", "r") as f:
        content = f.read()
        assert "AI information" in content
```

## 7. Test Execution
### 7.1 Test Execution Strategy

**Function Information:**
- Purpose: Define standardized test execution strategies and patterns for the multi-agent system test suite
- Scope: Test execution frameworks, execution patterns, test organization, and execution workflows
- Dependencies: pytest execution framework, test discovery, test categorization, execution optimization

**Strategy and Configuration:**
- Test execution patterns and workflows for unit, integration, and system tests
- Test categorization and organization strategies based on component types and complexity
- Execution optimization and parallelization for large test suites
- Test discovery and filtering mechanisms for targeted testing

**Implementation Details:**
- Execution strategy configuration patterns for different test types
- Test suite organization and categorization by agent components
- Execution workflow definitions for multi-agent system testing
- Performance optimization strategies for complex agent interactions

**Direct Usage Examples:**
- How to execute different test categories (unit, integration, system) with appropriate configurations
- How to configure test execution parameters for different agent types and scenarios
- How to organize and run test suites for specific components or workflows
- How to optimize test execution performance for large-scale agent testing

**Configuration Specifications:**
- Test execution configuration and parameters for pytest framework
- Test categorization and filtering options for agent-specific testing
- Execution workflow definitions for multi-agent system scenarios
- Performance optimization settings for parallel test execution

**Code Examples:**
```python
import pytest
import subprocess
import sys
from typing import List, Dict, Any, Optional
from pathlib import Path
import time
import multiprocessing

def execute_unit_tests(test_paths: List[str] = None, parallel: bool = True, 
                      coverage: bool = True) -> Dict[str, Any]:
    """Execute unit tests for individual components."""
    if test_paths is None:
        test_paths = [
            "tests/unit/test_agent_factories.py",
            "tests/unit/test_llm_factories.py",
            "tests/unit/test_graph_factories.py",
            "tests/unit/test_utility_functions.py"
        ]
    
    cmd = ["python", "-m", "pytest"]
    
    # Add test paths
    cmd.extend(test_paths)
    
    # Add parallel execution if enabled
    if parallel:
        cmd.extend(["-n", "auto"])
    
    # Add coverage if enabled
    if coverage:
        cmd.extend([
            "--cov=src",
            "--cov-report=html",
            "--cov-report=term-missing"
        ])
    
    # Add verbosity and output options
    cmd.extend([
        "-v",
        "--tb=short",
        "--strict-markers"
    ])
    
    # Execute tests
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    return {
        "return_code": result.returncode,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "test_paths": test_paths,
        "parallel": parallel,
        "coverage": coverage
    }

def execute_integration_tests(test_paths: List[str] = None, 
                            agent_types: List[str] = None) -> Dict[str, Any]:
    """Execute integration tests for agent interactions."""
    if test_paths is None:
        test_paths = [
            "tests/integration/test_agent_communication.py",
            "tests/integration/test_message_flow.py",
            "tests/integration/test_state_synchronization.py"
        ]
    
    if agent_types is None:
        agent_types = ["planner", "researcher", "expert", "critic", "finalizer"]
    
    cmd = ["python", "-m", "pytest"]
    
    # Add test paths
    cmd.extend(test_paths)
    
    # Add agent type markers
    for agent_type in agent_types:
        cmd.extend(["-m", f"agent_{agent_type}"])
    
    # Add integration-specific options
    cmd.extend([
        "-v",
        "--tb=long",
        "--strict-markers",
        "--durations=10"
    ])
    
    # Execute tests
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    return {
        "return_code": result.returncode,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "test_paths": test_paths,
        "agent_types": agent_types
    }

def execute_system_tests(test_paths: List[str] = None, 
                        scenarios: List[str] = None) -> Dict[str, Any]:
    """Execute system tests for end-to-end workflows."""
    if test_paths is None:
        test_paths = [
            "tests/system/test_end_to_end_workflows.py",
            "tests/system/test_research_expert_workflow.py",
            "tests/system/test_critic_review_workflow.py"
        ]
    
    if scenarios is None:
        scenarios = ["standard", "complex", "error_handling"]
    
    cmd = ["python", "-m", "pytest"]
    
    # Add test paths
    cmd.extend(test_paths)
    
    # Add scenario markers
    for scenario in scenarios:
        cmd.extend(["-m", f"scenario_{scenario}"])
    
    # Add system-specific options
    cmd.extend([
        "-v",
        "--tb=long",
        "--strict-markers",
        "--durations=20",
        "--timeout=300"
    ])
    
    # Execute tests
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    return {
        "return_code": result.returncode,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "test_paths": test_paths,
        "scenarios": scenarios
    }

def execute_test_suite_by_category(category: str, 
                                  config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Execute test suite based on category with specific configuration."""
    if config is None:
        config = {
            "parallel": True,
            "coverage": True,
            "verbose": True,
            "timeout": 300
        }
    
    execution_strategies = {
        "unit": {
            "function": execute_unit_tests,
            "default_paths": [
                "tests/unit/test_agent_factories.py",
                "tests/unit/test_llm_factories.py",
                "tests/unit/test_graph_factories.py"
            ]
        },
        "integration": {
            "function": execute_integration_tests,
            "default_paths": [
                "tests/integration/test_agent_communication.py",
                "tests/integration/test_message_flow.py"
            ]
        },
        "system": {
            "function": execute_system_tests,
            "default_paths": [
                "tests/system/test_end_to_end_workflows.py",
                "tests/system/test_research_expert_workflow.py"
            ]
        }
    }
    
    if category not in execution_strategies:
        raise ValueError(f"Unknown test category: {category}")
    
    strategy = execution_strategies[category]
    execution_function = strategy["function"]
    
    # Execute with configuration
    result = execution_function(
        test_paths=config.get("test_paths", strategy["default_paths"]),
        **{k: v for k, v in config.items() if k != "test_paths"}
    )
    
    return {
        "category": category,
        "config": config,
        "result": result
    }

def execute_parallel_test_suites(test_suites: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Execute multiple test suites in parallel."""
    def execute_suite(suite_config: Dict[str, Any]) -> Dict[str, Any]:
        category = suite_config["category"]
        config = suite_config.get("config", {})
        return execute_test_suite_by_category(category, config)
    
    # Execute test suites in parallel
    with multiprocessing.Pool() as pool:
        results = pool.map(execute_suite, test_suites)
    
    # Aggregate results
    aggregated_result = {
        "total_suites": len(test_suites),
        "successful_suites": sum(1 for r in results if r["result"]["return_code"] == 0),
        "failed_suites": sum(1 for r in results if r["result"]["return_code"] != 0),
        "suite_results": results
    }
    
    return aggregated_result

def execute_agent_specific_tests(agent_type: str, 
                                test_types: List[str] = None) -> Dict[str, Any]:
    """Execute tests specific to a particular agent type."""
    if test_types is None:
        test_types = ["unit", "integration"]
    
    agent_test_mapping = {
        "planner": {
            "unit": ["tests/unit/test_planner_agent.py"],
            "integration": ["tests/integration/test_planner_integration.py"]
        },
        "researcher": {
            "unit": ["tests/unit/test_researcher_agent.py"],
            "integration": ["tests/integration/test_researcher_integration.py"]
        },
        "expert": {
            "unit": ["tests/unit/test_expert_agent.py"],
            "integration": ["tests/integration/test_expert_integration.py"]
        },
        "critic": {
            "unit": ["tests/unit/test_critic_agent.py"],
            "integration": ["tests/integration/test_critic_integration.py"]
        },
        "finalizer": {
            "unit": ["tests/unit/test_finalizer_agent.py"],
            "integration": ["tests/integration/test_finalizer_integration.py"]
        }
    }
    
    if agent_type not in agent_test_mapping:
        raise ValueError(f"Unknown agent type: {agent_type}")
    
    results = {}
    
    for test_type in test_types:
        if test_type in agent_test_mapping[agent_type]:
            test_paths = agent_test_mapping[agent_type][test_type]
            result = execute_test_suite_by_category(test_type, {"test_paths": test_paths})
            results[test_type] = result
    
    return {
        "agent_type": agent_type,
        "test_types": test_types,
        "results": results
    }

def execute_performance_tests(performance_config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Execute performance tests with specific configuration."""
    if performance_config is None:
        performance_config = {
            "test_paths": ["tests/performance/test_system_performance.py"],
            "iterations": 5,
            "timeout": 600,
            "memory_monitoring": True
        }
    
    cmd = ["python", "-m", "pytest"]
    
    # Add test paths
    cmd.extend(performance_config["test_paths"])
    
    # Add performance-specific options
    cmd.extend([
        "-v",
        "--tb=short",
        "--strict-markers",
        f"--timeout={performance_config['timeout']}",
        "-m", "performance"
    ])
    
    # Execute performance tests
    start_time = time.time()
    result = subprocess.run(cmd, capture_output=True, text=True)
    execution_time = time.time() - start_time
    
    return {
        "return_code": result.returncode,
        "stdout": result.stdout,
        "stderr": result.stderr,
        "execution_time": execution_time,
        "config": performance_config
    }

# Test the execution strategies
def test_execute_unit_tests():
    """Test unit test execution strategy."""
    result = execute_unit_tests(
        test_paths=["tests/unit/test_agent_factories.py"],
        parallel=False,
        coverage=False
    )
    
    # Verify result structure
    assert "return_code" in result
    assert "stdout" in result
    assert "stderr" in result
    assert "test_paths" in result
    assert "parallel" in result
    assert "coverage" in result
    
    # Verify test paths
    assert "tests/unit/test_agent_factories.py" in result["test_paths"]
    assert result["parallel"] == False
    assert result["coverage"] == False

def test_execute_integration_tests():
    """Test integration test execution strategy."""
    result = execute_integration_tests(
        test_paths=["tests/integration/test_agent_communication.py"],
        agent_types=["planner", "researcher"]
    )
    
    # Verify result structure
    assert "return_code" in result
    assert "stdout" in result
    assert "stderr" in result
    assert "test_paths" in result
    assert "agent_types" in result
    
    # Verify agent types
    assert "planner" in result["agent_types"]
    assert "researcher" in result["agent_types"]

def test_execute_system_tests():
    """Test system test execution strategy."""
    result = execute_system_tests(
        test_paths=["tests/system/test_end_to_end_workflows.py"],
        scenarios=["standard"]
    )
    
    # Verify result structure
    assert "return_code" in result
    assert "stdout" in result
    assert "stderr" in result
    assert "test_paths" in result
    assert "scenarios" in result
    
    # Verify scenarios
    assert "standard" in result["scenarios"]

def test_execute_test_suite_by_category():
    """Test test suite execution by category."""
    result = execute_test_suite_by_category(
        category="unit",
        config={"parallel": False, "coverage": True}
    )
    
    # Verify result structure
    assert "category" in result
    assert "config" in result
    assert "result" in result
    
    # Verify category
    assert result["category"] == "unit"
    assert result["config"]["parallel"] == False
    assert result["config"]["coverage"] == True

def test_execute_agent_specific_tests():
    """Test agent-specific test execution."""
    result = execute_agent_specific_tests(
        agent_type="planner",
        test_types=["unit"]
    )
    
    # Verify result structure
    assert "agent_type" in result
    assert "test_types" in result
    assert "results" in result
    
    # Verify agent type
    assert result["agent_type"] == "planner"
    assert "unit" in result["test_types"]
    assert "unit" in result["results"]

def test_execute_performance_tests():
    """Test performance test execution."""
    result = execute_performance_tests({
        "test_paths": ["tests/performance/test_system_performance.py"],
        "iterations": 3,
        "timeout": 300
    })
    
    # Verify result structure
    assert "return_code" in result
    assert "stdout" in result
    assert "stderr" in result
    assert "execution_time" in result
    assert "config" in result
    
    # Verify execution time
    assert result["execution_time"] >= 0
    assert result["config"]["iterations"] == 3
    assert result["config"]["timeout"] == 300

def test_execution_strategy_integration():
    """Test integration of execution strategies."""
    # Test multiple execution strategies
    unit_result = execute_unit_tests(parallel=False, coverage=False)
    integration_result = execute_integration_tests(agent_types=["planner"])
    system_result = execute_system_tests(scenarios=["standard"])
    
    # Verify all results have expected structure
    for result in [unit_result, integration_result, system_result]:
        assert "return_code" in result
        assert "stdout" in result
        assert "stderr" in result
    
    # Test parallel execution
    test_suites = [
        {"category": "unit", "config": {"parallel": False}},
        {"category": "integration", "config": {"agent_types": ["planner"]}}
    ]
    
    parallel_result = execute_parallel_test_suites(test_suites)
    
    # Verify parallel execution result
    assert "total_suites" in parallel_result
    assert "successful_suites" in parallel_result
    assert "failed_suites" in parallel_result
    assert "suite_results" in parallel_result
    assert parallel_result["total_suites"] == 2
```